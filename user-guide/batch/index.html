
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../development/">
      
      
        <link rel="next" href="../singularity/">
      
      
      <link rel="icon" href="../../favicon.ico">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.17">
    
    
      
        <title>Running Jobs on Cirrus - Cirrus User Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.bcfcd587.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/cirrus.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-3T9RVR5FTP"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-3T9RVR5FTP",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-3T9RVR5FTP",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-jobs-on-cirrus" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Cirrus User Documentation" class="md-header__button md-logo" aria-label="Cirrus User Documentation" data-md-component="logo">
      
  <img src="../../images/cirrus_logo_white-Transparent-Background.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Cirrus User Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Running Jobs on Cirrus
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EPCCed/cirrus-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    EPCCed/cirrus-docs
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Cirrus User Documentation" class="md-nav__button md-logo" aria-label="Cirrus User Documentation" data-md-component="logo">
      
  <img src="../../images/cirrus_logo_white-Transparent-Background.png" alt="logo">

    </a>
    Cirrus User Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EPCCed/cirrus-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    EPCCed/cirrus-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../e1000-migration" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cirrus migration to E1000 system
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    User Guide
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            User Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../connecting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Connecting to Cirrus
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Management and Transfer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../resource_management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    File and Resource Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../development/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Application Development Environment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Running Jobs on Cirrus
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Running Jobs on Cirrus
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Slurm commands
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    <span class="md-ellipsis">
      sinfo: information on resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      sbatch: submitting jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      squeue: monitoring jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      scancel: deleting jobs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    <span class="md-ellipsis">
      Resource Limits
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resources-on-standard-cpu-compute-nodes" class="md-nav__link">
    <span class="md-ellipsis">
      Primary resources on standard (CPU) compute nodes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#primary-resources-on-gpu-nodes" class="md-nav__link">
    <span class="md-ellipsis">
      Primary resources on GPU nodes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    <span class="md-ellipsis">
      Partitions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    <span class="md-ellipsis">
      Quality of Service (QoS)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quality of Service (QoS)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cirrus-qos" class="md-nav__link">
    <span class="md-ellipsis">
      Cirrus QoS
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm error handling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Slurm error handling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mpi-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      MPI jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#automatic-resubmission" class="md-nav__link">
    <span class="md-ellipsis">
      Automatic resubmission
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm error messages
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm queued reasons
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Output from Slurm jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Specifying resources in job scripts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      srun: Launching parallel jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-parallel-job-submission-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Example parallel job submission scripts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example parallel job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for MPI parallel job
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example: job submission script for MPI parallel job">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#note-on-mpt-task-placement" class="md-nav__link">
    <span class="md-ellipsis">
      Note on MPT task placement
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for MPI+OpenMP (mixed mode) parallel job
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-openmp-parallel-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for OpenMP parallel job
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    <span class="md-ellipsis">
      Job arrays
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    <span class="md-ellipsis">
      Job script for a job array
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    <span class="md-ellipsis">
      Submitting a job array
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    <span class="md-ellipsis">
      Job chaining
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive Jobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-srun" class="md-nav__link">
    <span class="md-ellipsis">
      Using srun
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-salloc-with-srun" class="md-nav__link">
    <span class="md-ellipsis">
      Using salloc with srun
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    <span class="md-ellipsis">
      Reservations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serial-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Serial jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#temporary-files-and-tmp-in-batch-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Temporary files and /tmp in batch jobs
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../singularity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Singularity Containers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Python
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the Cirrus GPU Nodes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../solidstate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Solid state storage
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Applications
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Software Applications
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/castep/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Castep
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/cp2k/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CP2K
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/elements/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ELEMENTS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/flacs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FLACS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/gaussian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gaussian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/gromacs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GROMACS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/helyx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HELYX&reg;
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/lammps/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LAMMPS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/MATLAB/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MATLAB
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/namd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NAMD
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/openfoam/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenFOAM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/orca/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ORCA
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/qe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quantum Espresso (QE)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/starccm%2B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    STAR-CCM+
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-packages/vasp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VASP
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Libraries
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Software Libraries
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/intel_mkl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intel MKL: BLAS, LAPACK, ScaLAPACK
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-libraries/hdf5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HDF5
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Tools
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Software Tools
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-tools/ddt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Debugging using Linaro DDT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-tools/scalasca/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Profiling using Scalasca
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../software-tools/intel-vtune/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intel VTune
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../reading/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    References and further reading
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Slurm commands
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    <span class="md-ellipsis">
      sinfo: information on resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      sbatch: submitting jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      squeue: monitoring jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      scancel: deleting jobs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    <span class="md-ellipsis">
      Resource Limits
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resources-on-standard-cpu-compute-nodes" class="md-nav__link">
    <span class="md-ellipsis">
      Primary resources on standard (CPU) compute nodes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#primary-resources-on-gpu-nodes" class="md-nav__link">
    <span class="md-ellipsis">
      Primary resources on GPU nodes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    <span class="md-ellipsis">
      Partitions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    <span class="md-ellipsis">
      Quality of Service (QoS)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quality of Service (QoS)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cirrus-qos" class="md-nav__link">
    <span class="md-ellipsis">
      Cirrus QoS
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm error handling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Slurm error handling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mpi-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      MPI jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#automatic-resubmission" class="md-nav__link">
    <span class="md-ellipsis">
      Automatic resubmission
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm error messages
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm queued reasons
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Output from Slurm jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Specifying resources in job scripts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      srun: Launching parallel jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-parallel-job-submission-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Example parallel job submission scripts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example parallel job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for MPI parallel job
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example: job submission script for MPI parallel job">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#note-on-mpt-task-placement" class="md-nav__link">
    <span class="md-ellipsis">
      Note on MPT task placement
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for MPI+OpenMP (mixed mode) parallel job
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-openmp-parallel-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for OpenMP parallel job
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    <span class="md-ellipsis">
      Job arrays
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    <span class="md-ellipsis">
      Job script for a job array
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    <span class="md-ellipsis">
      Submitting a job array
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    <span class="md-ellipsis">
      Job chaining
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Interactive Jobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-srun" class="md-nav__link">
    <span class="md-ellipsis">
      Using srun
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-salloc-with-srun" class="md-nav__link">
    <span class="md-ellipsis">
      Using salloc with srun
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    <span class="md-ellipsis">
      Reservations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serial-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Serial jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#temporary-files-and-tmp-in-batch-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Temporary files and /tmp in batch jobs
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="running-jobs-on-cirrus">Running Jobs on Cirrus</h1>
<p>As with most HPC services, Cirrus uses a scheduler to manage access to
resources and ensure that the thousands of different users of system are
able to share the system and all get access to the resources they
require. Cirrus uses the Slurm software to schedule jobs.</p>
<p>Writing a submission script is typically the most convenient way to
submit your job to the scheduler. Example submission scripts (with
explanations) for the most common job types are provided below.</p>
<p>Interactive jobs are also available and can be particularly useful for
developing and debugging applications. More details are available below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have any questions on how to run jobs on Cirrus do not hesitate to contact the <a href="mailto:support@cirrus.ac.uk">Cirrus Service Desk</a>.</p>
</div>
<p>You typically interact with Slurm by issuing Slurm commands from the
login nodes (to submit, check and cancel jobs), and by specifying Slurm
directives that describe the resources required for your jobs in job
submission scripts.</p>
<h2 id="basic-slurm-commands">Basic Slurm commands</h2>
<p>There are three key commands used to interact with the Slurm on the
command line:</p>
<ul>
<li><code>sinfo</code> - Get information on the partitions and resources available</li>
<li><code>sbatch jobscript.slurm</code> - Submit a job submission script (in this
  case called: <code>jobscript.slurm</code>) to the scheduler</li>
<li><code>squeue</code> - Get the current status of jobs submitted to the scheduler</li>
<li><code>scancel 12345</code> - Cancel a job (in this case with the job ID <code>12345</code>)</li>
</ul>
<p>We cover each of these commands in more detail below.</p>
<h3 id="sinfo-information-on-resources"><code>sinfo</code>: information on resources</h3>
<p><code>sinfo</code> is used to query information about available resources and
partitions. Without any options, <code>sinfo</code> lists the status of all
resources and partitions, e.g.</p>
<pre><code>[auser@cirrus-login3 ~]$ sinfo

PARTITION   AVAIL  TIMELIMIT  NODES  STATE NODELIST
standard       up   infinite    280   idle r1i0n[0-35],r1i1n[0-35],r1i2n[0-35],r1i3n[0-35],r1i4n[0-35],r1i5n[0-35],r1i6n[0-35],r1i7n[0-6,9-15,18-24,27-33]
gpu            up   infinite     36   idle r2i4n[0-8],r2i5n[0-8],r2i6n[0-8],r2i7n[0-8]
</code></pre>
<h3 id="sbatch-submitting-jobs"><code>sbatch</code>: submitting jobs</h3>
<p><code>sbatch</code> is used to submit a job script to the job submission system.
The script will typically contain one or more <code>srun</code> commands to launch
parallel tasks.</p>
<p>When you submit the job, the scheduler provides the job ID, which is
used to identify this job in other Slurm commands and when looking at
resource usage in SAFE.</p>
<pre><code>[auser@cirrus-login3 ~]$ sbatch test-job.slurm
Submitted batch job 12345
</code></pre>
<h3 id="squeue-monitoring-jobs"><code>squeue</code>: monitoring jobs</h3>
<p><code>squeue</code> without any options or arguments shows the current status of
all jobs known to the scheduler. For example:</p>
<pre><code>[auser@cirrus-login3 ~]$ squeue
          JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          1554  comp-cse CASTEP_a  auser  R       0:03      2 r2i0n[18-19]
</code></pre>
<p>will list all jobs on Cirrus.</p>
<p>The output of this is often overwhelmingly large. You can restrict the
output to just your jobs by adding the <code>-u $USER</code> option:</p>
<pre><code>[auser@cirrus-login3 ~]$ squeue -u $USER
</code></pre>
<h3 id="scancel-deleting-jobs"><code>scancel</code>: deleting jobs</h3>
<p><code>scancel</code> is used to delete a jobs from the scheduler. If the job is
waiting to run it is simply cancelled, if it is a running job then it is
stopped immediately. You need to provide the job ID of the job you wish
to cancel/stop. For example:</p>
<pre><code>[auser@cirrus-login3 ~]$ scancel 12345
</code></pre>
<p>will cancel (if waiting) or stop (if running) the job with ID <code>12345</code>.</p>
<h2 id="resource-limits">Resource Limits</h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you have requirements which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements.</p>
</div>
<p>There are different resource limits on Cirrus for different purposes.
There are three different things you need to specify for each job:</p>
<ul>
<li>The amount of <em>primary resource</em> you require (more information on this
  below)</li>
<li>The <em>partition</em> that you want to use - this specifies the nodes that
  are eligible to run your job</li>
<li>The <em>Quality of Service (QoS)</em> that you want to use - this specifies
  the job limits that apply</li>
</ul>
<p>Each of these aspects are described in more detail below.</p>
<p>The <em>primary resources</em> you request are <em>compute</em> resources: either CPU
cores on the standard compute nodes or GPU cards on the GPU compute
nodes. Other node resources: memory on the standard compute nodes;
memory and CPU cores on the GPU nodes are assigned pro rata based on the
primary resource that you request.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>On Cirrus, you cannot specify the memory for a job using the <code>--mem</code>
options to Slurm (e.g. <code>--mem</code>, <code>--mem-per-cpu</code>, <code>--mem-per-gpu</code>). The
amount of memory you are assigned is calculated from the amount of
primary resource you request.</p>
</div>
<h3 id="primary-resources-on-standard-cpu-compute-nodes">Primary resources on standard (CPU) compute nodes</h3>
<p>The <em>primary resource</em> you request on standard compute nodes are CPU
cores. The maximum amount of memory you are allocated is computed as the
number of CPU cores you requested multiplied by 1/36th of the total
memory available (as there are 36 CPU cores per node). So, if you
request the full node (36 cores), then you will be allocated a maximum
of all of the memory (256 GB) available on the node; however, if you
request 1 core, then you will be assigned a maximum of 256/36 = 7.1 GB
of the memory available on the node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the <code>--exclusive</code> option in jobs will give you access to the full
node memory even if you do not explicitly request all of the CPU cores
on the node.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Using the <code>--exclusive</code> option will charge your account for the usage of
the entire node, even if you don't request all the cores in your
scripts.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will not generally have access to the full amount of memory resource
on the the node as some is retained for running the operating system and
other system processes.</p>
</div>
<h3 id="primary-resources-on-gpu-nodes">Primary resources on GPU nodes</h3>
<p>The <em>primary resource</em> you request on standard compute nodes are GPU
cards. The maximum amount of memory and CPU cores you are allocated is
computed as the number of GPU cards you requested multiplied by 1/4 of
the total available (as there are 4 GPU cards per node). So, if you
request the full node (4 GPU cards), then you will be allocated a
maximum of all of the memory (384 GB) available on the node; however, if
you request 1 GPU card, then you will be assigned a maximum of 384/4 =
96 GB of the memory available on the node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the <code>--exclusive</code> option in jobs will give you access to all of
the CPU cores and the full node memory even if you do not explicitly
request all of the GPU cards on the node.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>In order to run jobs on the GPU nodes your budget must have positive GPU
hours <em>and</em> core hours associated with it. However, only your GPU hours
will be consumed when running these jobs.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Using the <code>--exclusive</code> option will charge your account for the usage of
the entire node, <em>i.e.</em>, 4 GPUs, even if you don't request all the GPUs
in your submission script.</p>
</div>
<h3 id="partitions">Partitions</h3>
<p>On Cirrus, compute nodes are grouped into partitions. You will have to
specify a partition using the <code>--partition</code> option in your submission
script. The following table has a list of active partitions on Cirrus.</p>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Description</th>
<th>Total nodes available</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>CPU nodes with 2x 18-core Intel Broadwell processors</td>
<td>352</td>
<td></td>
</tr>
<tr>
<td>gpu</td>
<td>GPU nodes with 4x Nvidia V100 GPU and 2x 20-core Intel Cascade Lake processors</td>
<td>36</td>
<td></td>
</tr>
</tbody>
</table>
<p>Cirrus Partitions</p>
<p>You can list the active partitions using</p>
<pre><code>sinfo
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>you may not have access to all the available partitions.</p>
</div>
<h3 id="quality-of-service-qos">Quality of Service (QoS)</h3>
<p>On Cirrus Quality of Service (QoS) is used alongside partitions to set
resource limits. The following table has a list of active QoS on Cirrus.</p>
<table>
<thead>
<tr>
<th>QoS Name</th>
<th>Jobs Running Per User</th>
<th>Jobs Queued Per User</th>
<th>Max Walltime</th>
<th>Max Size</th>
<th>Applies to Partitions</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>No limit</td>
<td>500 jobs</td>
<td>4 days</td>
<td>88 nodes (3168 cores/25%)</td>
<td>standard</td>
<td></td>
</tr>
<tr>
<td>largescale</td>
<td>1 job</td>
<td>4 jobs</td>
<td>24 hours</td>
<td>228 nodes (8192+ cores/65%) or 144 GPUs</td>
<td>standard, gpu</td>
<td></td>
</tr>
<tr>
<td>long</td>
<td>5 jobs</td>
<td>20 jobs</td>
<td>14 days</td>
<td>16 nodes or 8 GPUs</td>
<td>standard, gpu</td>
<td></td>
</tr>
<tr>
<td>highpriority</td>
<td>10 jobs</td>
<td>20 jobs</td>
<td>4 days</td>
<td>140 nodes</td>
<td>standard</td>
<td>charged at 1.5 x normal rate</td>
</tr>
<tr>
<td>gpu</td>
<td>No limit</td>
<td>128 jobs</td>
<td>4 days</td>
<td>64 GPUs (16 nodes/40%)</td>
<td>gpu</td>
<td></td>
</tr>
<tr>
<td>short</td>
<td>1 job</td>
<td>2 jobs</td>
<td>20 minutes</td>
<td>2 nodes or 4 GPUs</td>
<td>standard, gpu</td>
<td></td>
</tr>
<tr>
<td>lowpriority</td>
<td>No limit</td>
<td>100 jobs</td>
<td>2 days</td>
<td>36 nodes (1296 cores/10%) or 16 GPUs</td>
<td>standard, gpu</td>
<td>usage is not charged</td>
</tr>
</tbody>
</table>
<h4 id="cirrus-qos">Cirrus QoS</h4>
<p>You can find out the QoS that you can use by running the following
command:</p>
<pre><code>sacctmgr show assoc user=$USER cluster=cirrus format=cluster,account,user,qos%50
</code></pre>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="slurm-error-handling">Slurm error handling</h3>
<h4 id="mpi-jobs">MPI jobs</h4>
<p>Users of MPI codes may wish to ensure termination of all tasks on the
failure of one individual task by specifying the <code>--kill-on-bad-exit</code>
argument to <code>srun</code>. E.g.,</p>
<div class="highlight"><pre><span></span><code>srun<span class="w"> </span>-n<span class="w"> </span><span class="m">36</span><span class="w"> </span>--kill-on-bad-exit<span class="w"> </span>./my-mpi-program
</code></pre></div>
<p>This can prevent effective "hanging" of the job until the wall time
limit is reached.</p>
<h4 id="automatic-resubmission">Automatic resubmission</h4>
<p>Jobs that fail are not automatically resubmitted by Slurm on Cirrus.
Automatic resubmission can be enabled for a job by specifying the
<code>--requeue</code> option to <code>sbatch</code>.</p>
<h3 id="slurm-error-messages">Slurm error messages</h3>
<p>An incorrect submission will cause Slurm to return an error. Some common
problems are listed below, with a suggestion about the likely cause:</p>
<ul>
<li>
<p><code>sbatch: unrecognized option &lt;text&gt;</code></p>
</li>
<li>
<p>One of your options is invalid or has a typo. <code>man sbatch</code> to help.</p>
</li>
<li>
<p><code>error: Batch job submission failed: No partition specified or system default partition</code></p>
</li>
</ul>
<blockquote>
<p>A <code>--partition=</code> option is missing. You must specify the partition
(see the list above). This is most often <code>--partition=standard</code>.</p>
</blockquote>
<ul>
<li><code>error: invalid partition specified: &lt;partition&gt;</code></li>
</ul>
<blockquote>
<p><code>error: Batch job submission failed: Invalid partition name specified</code></p>
<p>Check the partition exists and check the spelling is correct.</p>
</blockquote>
<ul>
<li><code>error: Batch job submission failed: Invalid account or account/partition combination specified</code></li>
</ul>
<blockquote>
<p>This probably means an invalid account has been given. Check the
<code>--account=</code> options against valid accounts in SAFE.</p>
</blockquote>
<ul>
<li><code>error: Batch job submission failed: Invalid qos specification</code></li>
</ul>
<blockquote>
<p>A QoS option is either missing or invalid. Check the script has a
<code>--qos=</code> option and that the option is a valid one from the table
above. (Check the spelling of the QoS is correct.)</p>
</blockquote>
<ul>
<li><code>error: Your job has no time specification (--time=)...</code></li>
</ul>
<blockquote>
<p>Add an option of the form <code>--time=hours:minutes:seconds</code> to the
submission script. E.g., <code>--time=01:30:00</code> gives a time limit of 90
minutes.</p>
</blockquote>
<ul>
<li><code>error: QOSMaxWallDurationPerJobLimit</code>
  <code>error: Batch job submission failed: Job violates accounting/QOS policy</code>
  <code>(job submit limit, user's size and/or time limits)</code></li>
</ul>
<p>The script has probably specified a time limit which is too long for
  the corresponding QoS. E.g., the time limit for the short QoS is 20
  minutes.</p>
<h3 id="slurm-queued-reasons">Slurm queued reasons</h3>
<p>The <code>squeue</code> command allows users to view information for jobs managed
by Slurm. Jobs typically go through the following states: PENDING,
RUNNING, COMPLETING, and COMPLETED. The first table provides a
description of some job state codes. The second table provides a
description of the reasons that cause a job to be in a state.</p>
<table>
<thead>
<tr>
<th>Status</th>
<th>Code</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PENDING</td>
<td>PD</td>
<td>Job is awaiting resource allocation.</td>
</tr>
<tr>
<td>RUNNING</td>
<td>R</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>SUSPENDED</td>
<td>S</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>COMPLETING</td>
<td>CG</td>
<td>Job is in the process of completing. Some processes on some nodes may still be active.</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>CD</td>
<td>Job has terminated all processes on all nodes with an exit code of zero.</td>
</tr>
<tr>
<td>TIMEOUT</td>
<td>TO</td>
<td>Job terminated upon reaching its time limit.</td>
</tr>
<tr>
<td>STOPPED</td>
<td>ST</td>
<td>Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job.</td>
</tr>
<tr>
<td>OUT_OF_MEMORY</td>
<td>OOM</td>
<td>Job experienced out of memory error.</td>
</tr>
<tr>
<td>FAILED</td>
<td>F</td>
<td>Job terminated with non-zero exit code or other failure condition.</td>
</tr>
<tr>
<td>NODE_FAIL</td>
<td>NF</td>
<td>Job terminated due to failure of one or more allocated nodes.</td>
</tr>
<tr>
<td>CANCELLED</td>
<td>CA</td>
<td>Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated.</td>
</tr>
</tbody>
</table>
<p>Slurm Job State codes</p>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAG">Job State
Codes</a></p>
<table>
<thead>
<tr>
<th>Reason</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Priority</td>
<td>One or more higher priority jobs exist for this partition or advanced reservation.</td>
</tr>
<tr>
<td>Resources</td>
<td>The job is waiting for resources to become available.</td>
</tr>
<tr>
<td>BadConstraints</td>
<td>The job's constraints can not be satisfied.</td>
</tr>
<tr>
<td>BeginTime</td>
<td>The job's earliest start time has not yet been reached.</td>
</tr>
<tr>
<td>Dependency</td>
<td>This job is waiting for a dependent job to complete.</td>
</tr>
<tr>
<td>Licenses</td>
<td>The job is waiting for a license.</td>
</tr>
<tr>
<td>WaitingForScheduling</td>
<td>No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.</td>
</tr>
<tr>
<td>Prolog</td>
<td>Its PrologSlurmctld program is still running.</td>
</tr>
<tr>
<td>JobHeldAdmin</td>
<td>The job is held by a system administrator.</td>
</tr>
<tr>
<td>JobHeldUser</td>
<td>The job is held by the user.</td>
</tr>
<tr>
<td>JobLaunchFailure</td>
<td>The job could not be launched. This may be due to a file system problem, invalid program name, etc.</td>
</tr>
<tr>
<td>NonZeroExitCode</td>
<td>The job terminated with a non-zero exit code.</td>
</tr>
<tr>
<td>InvalidAccount</td>
<td>The job's account is invalid.</td>
</tr>
<tr>
<td>InvalidQOS</td>
<td>The job's QOS is invalid.</td>
</tr>
<tr>
<td>QOSUsageThreshold</td>
<td>Required QOS threshold has been breached.</td>
</tr>
<tr>
<td>QOSJobLimit</td>
<td>The job's QOS has reached its maximum job count.</td>
</tr>
<tr>
<td>QOSResourceLimit</td>
<td>The job's QOS has reached some resource limit.</td>
</tr>
<tr>
<td>QOSTimeLimit</td>
<td>The job's QOS has reached its time limit.</td>
</tr>
<tr>
<td>NodeDown</td>
<td>A node required by the job is down.</td>
</tr>
<tr>
<td>TimeLimit</td>
<td>The job exhausted its time limit.</td>
</tr>
<tr>
<td>ReqNodeNotAvail</td>
<td>Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's "reason" field as "UnavailableNodes". Such nodes will typically require the intervention of a system administrator to make available.</td>
</tr>
</tbody>
</table>
<p>Slurm Job Reasons</p>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAF">Job
Reasons</a></p>
<h2 id="output-from-slurm-jobs">Output from Slurm jobs</h2>
<p>Slurm places standard output (STDOUT) and standard error (STDERR) for
each job in the file <code>slurm_&lt;JobID&gt;.out</code>. This file appears in the job's
working directory once your job starts running.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This file is plain text and can contain useful information to help
debugging if a job is not working as expected. The Cirrus Service Desk
team will often ask you to provide the contents of this file if you
contact them for help with issues.</p>
</div>
<h2 id="specifying-resources-in-job-scripts">Specifying resources in job scripts</h2>
<p>You specify the resources you require for your job using directives at
the top of your job submission script using lines that start with the
directive <code>#SBATCH</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Options provided using <code>#SBATCH</code> directives can also be specified as
command line options to <code>srun</code>.</p>
</div>
<p>If you do not specify any options, then the default for each option will
be applied. As a minimum, all job submissions must specify the budget
that they wish to charge the job too, the partition they wish to use and
the QoS they want to use with the options:</p>
<ul>
<li><code>--account=&lt;budgetID&gt;</code> your budget ID is usually something like
   <code>t01</code> or <code>t01-test</code>. You can see which budget codes you can charge
   to in SAFE.</li>
<li><code>--partition=&lt;partition&gt;</code> The partition specifies the set of nodes
   you want to run on. More information on available partitions is
   given above.</li>
<li><code>--qos="QoS"</code> The QoS specifies the limits to apply to your job.
   More information on available QoS are given above.</li>
</ul>
<p>Other common options that are used are:</p>
<ul>
<li><code>--time=&lt;hh:mm:ss&gt;</code> the maximum walltime for your job. <em>e.g.</em> For a
   6.5 hour walltime, you would use <code>--time=6:30:0</code>.</li>
<li><code>--job-name=&lt;jobname&gt;</code> set a name for the job to help identify it in
   Slurm command output.</li>
</ul>
<p>Other not so common options that are used are:</p>
<ul>
<li><code>--switches=max-switches{@max-time-to-wait}</code> optimum switches and
   max time to wait for them. The scheduler will wait indefinitely when
   attempting to place these jobs. Users can override this indefinite
   wait. The scheduler will deliberately place work to clear space for
   these jobs, so we don't foresee the indefinite wait nature to be an
   issue.</li>
</ul>
<p>In addition, parallel jobs will also need to specify how many nodes,
parallel processes and threads they require.</p>
<ul>
<li><code>--exclusive</code> to ensure that you have exclusive access to a compute
   node</li>
<li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li>
<li><code>--tasks-per-node=&lt;processes per node&gt;</code> the number of parallel
   processes (e.g. MPI ranks) per node.</li>
<li><code>--cpus-per-task=&lt;threads per task&gt;</code> the number of threads per
   parallel process (e.g. number of OpenMP threads per MPI task for
   hybrid MPI/OpenMP jobs). <strong>Note:</strong> you must also set the
   <code>OMP_NUM_THREADS</code> environment variable if using OpenMP in your job
   and usually add the <code>--cpu-bind=cores</code> option to <code>srun</code>. See the
   example below for an MPI+OpenMP job for further notes on
   <code>--cpus-per-task</code>.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For parallel jobs, you should request exclusive node access with the
<code>--exclusive</code> option to ensure you get the expected resources and
performance.</p>
</div>
<h2 id="srun-launching-parallel-jobs"><code>srun</code>: Launching parallel jobs</h2>
<p>If you are running parallel jobs, your job submission script should
contain one or more <code>srun</code> commands to launch the parallel executable
across the compute nodes. As well as launching the executable, <code>srun</code>
also allows you to specify the distribution and placement (or <em>pinning</em>)
of the parallel processes and threads.</p>
<p>If you are running MPI jobs that do not also use OpenMP threading, then
you should use <code>srun</code> with no additional options. <code>srun</code> will use the
specification of nodes and tasks from your job script, <code>sbatch</code> or
<code>salloc</code> command to launch the correct number of parallel tasks.</p>
<p>If you are using OpenMP threads then you will generally add the
<code>--cpu-bind=cores</code> option to <code>srun</code> to bind threads to cores to obtain
the best performance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See the example job submission scripts below for examples of using
<code>srun</code> for pure MPI jobs and for jobs that use OpenMP threading.</p>
</div>
<h2 id="example-parallel-job-submission-scripts">Example parallel job submission scripts</h2>
<p>A subset of example job submission scripts are included in full below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Do not replace <code>srun</code> with <code>mpirun</code> in the following examples. Although
this might work under special circumstances, it is not guaranteed and
therefore not supported.</p>
</div>
<h3 id="example-job-submission-script-for-mpi-parallel-job">Example: job submission script for MPI parallel job</h3>
<p>A simple MPI job submission script to submit a job using 4 compute nodes
and 36 MPI ranks per node for 20 minutes would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (name, compute nodes, job time)</span>
<span class="c1">#SBATCH --job-name=Example_MPI_Job</span>
<span class="c1">#SBATCH --time=0:20:0</span>
<span class="c1">#SBATCH --exclusive</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --tasks-per-node=36</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="c1">#SBATCH --account=[budget code]</span>
<span class="c1"># We use the &quot;standard&quot; partition as we are running on CPU nodes</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1"># We use the &quot;standard&quot; QoS as our runtime is less than 4 days</span>
<span class="c1">#SBATCH --qos=standard</span>

<span class="c1"># Load the default HPE MPI environment</span>
module<span class="w"> </span>load<span class="w"> </span>mpt

<span class="c1"># Change to the submission directory</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically</span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Launch the parallel job</span>
<span class="c1">#   Using 144 MPI processes and 36 MPI processes per node</span>
<span class="c1">#   srun picks up the distribution from the sbatch options</span>
srun<span class="w"> </span>./my_mpi_executable.x
</code></pre></div>
<p>This will run your executable "my_mpi_executable.x" in parallel on 144
MPI processes using 4 nodes (36 cores per node, i.e. not using
hyper-threading). Slurm will allocate 4 nodes to your job and srun will
place 36 MPI processes on each node (one per physical core).</p>
<p>By default, srun will launch an MPI job that uses all of the cores you
have requested via the "nodes" and "tasks-per-node" options. If you want
to run fewer MPI processes than cores you will need to change the
script.</p>
<p>For example, to run this program on 128 MPI processes you have two
options:</p>
<ul>
<li>set <code>--tasks-per-node=32</code> for an even distribution across nodes
   (this may not always be possible depending on the exact combination
   of nodes requested and MPI tasks required)</li>
<li>set the number of MPI tasks explicitly using <code>#SBATCH --ntasks=128</code></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you specify <code>--ntasks</code> explicitly and it is not compatible with the
value of <code>tasks-per-node</code> then you will get a warning message from
srun such as <code>srun: Warning: can't honor --ntasks-per-node set to 36</code>.</p>
<p>In this case, srun does the sensible thing and allocates MPI processes
as evenly as it can across nodes. For example, the second option above
would result in 32 MPI processes on each of the 4 nodes.</p>
</div>
<p>See above for a more detailed discussion of the different <code>sbatch</code>
options.</p>
<h4 id="note-on-mpt-task-placement">Note on MPT task placement</h4>
<p>By default, <code>mpt</code> will distribute processss to physical cores (cores
0-17 on socket 0, and cores 18-35 on socket 1) in a cyclic fashion. That
is, rank 0 would be placed on core 0, task 1 on core 18, rank 2 on core
1, and so on (in a single-node job). This may be undesirable. Block,
rather than cyclic, distribution can be obtained with</p>
<div class="highlight"><pre><span></span><code><span class="c1">#SBATCH --distribution=block:block</span>
</code></pre></div>
<p>The <code>block:block</code> here refers to the distribution on both nodes and
sockets. This will distribute rank 0 for core 0, rank 1 to core 1, rank
2 to core 2, and so on.</p>
<h3 id="example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job">Example: job submission script for MPI+OpenMP (mixed mode) parallel job</h3>
<p>Mixed mode codes that use both MPI (or another distributed memory
parallel model) and OpenMP should take care to ensure that the shared
memory portion of the process/thread placement does not span more than
one node. This means that the number of shared memory threads should be
a factor of 36.</p>
<p>In the example below, we are using 4 nodes for 6 hours. There are 8 MPI
processes in total (2 MPI processes per node) and 18 OpenMP threads per
MPI process. This results in all 36 physical cores per node being used.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>the use of the <code>--cpu-bind=cores</code> option to generate the correct
affinity settings.</p>
</div>
<p><div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (name, compute nodes, job time)</span>
<span class="c1">#SBATCH --job-name=Example_MPI_Job</span>
<span class="c1">#SBATCH --time=0:20:0</span>
<span class="c1">#SBATCH --exclusive</span>
<span class="c1">#SBATCH --nodes=4</span>
<span class="c1">#SBATCH --ntasks=8</span>
<span class="c1">#SBATCH --tasks-per-node=2</span>
<span class="c1">#SBATCH --cpus-per-task=18</span>

<span class="c1"># Replace [budget code] below with your project code (e.g. t01)</span>
<span class="c1">#SBATCH --account=[budget code]</span>
<span class="c1"># We use the &quot;standard&quot; partition as we are running on CPU nodes</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1"># We use the &quot;standard&quot; QoS as our runtime is less than 4 days</span>
<span class="c1">#SBATCH --qos=standard</span>

<span class="c1"># Load the default HPE MPI environment</span>
module<span class="w"> </span>load<span class="w"> </span>mpt

<span class="c1"># Change to the submission directory</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c1"># Set the number of threads to 18</span>
<span class="c1">#   There are 18 OpenMP threads per MPI process</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">18</span>

<span class="c1"># Launch the parallel job</span>
<span class="c1">#   Using 8 MPI processes</span>
<span class="c1">#   2 MPI processes per node</span>
<span class="c1">#   18 OpenMP threads per MPI process</span>

srun<span class="w"> </span>--cpu-bind<span class="o">=</span>cores<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">18</span><span class="w"> </span>./my_mixed_executable.x<span class="w"> </span>arg1<span class="w"> </span>arg2
</code></pre></div>
In the above we add <code>--cpus-per-task=18</code> to the <code>srun</code> command to be
consistent with that specified to <code>#SBATCH</code>. This is required to ensure
that the correct assignment of threads to physical cores takes place.
The reason for this duplication is that the value specified to <code>SBATCH</code>
does not propagate automatically to <code>srun</code>. The alternative is to
specify:
<div class="highlight"><pre><span></span><code>export SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}
</code></pre></div>
in the script before the <code>srun</code> command. This will allow the <code>--cpus-per-task</code> value specified at submission (<code>SBATCH</code>) to propagate to <code>srun</code>
(the default value would be <code>--cpus-per-task=1</code> at the <code>srun</code> stage).
Failure to use either of these
approaches may result in threads using the same physical core, which
will cause a significant degradation in performance compared with
that expected.</p>
<h3 id="example-job-submission-script-for-openmp-parallel-job">Example: job submission script for OpenMP parallel job</h3>
<p>A simple OpenMP job submission script to submit a job using 1 compute
nodes and 36 threads for 20 minutes would look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (name, compute nodes, job time)</span>
<span class="c1">#SBATCH --job-name=Example_OpenMP_Job</span>
<span class="c1">#SBATCH --time=0:20:0</span>
<span class="c1">#SBATCH --exclusive</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --tasks-per-node=1</span>
<span class="c1">#SBATCH --cpus-per-task=36</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="c1">#SBATCH --account=[budget code]</span>
<span class="c1"># We use the &quot;standard&quot; partition as we are running on CPU nodes</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1"># We use the &quot;standard&quot; QoS as our runtime is less than 4 days</span>
<span class="c1">#SBATCH --qos=standard</span>

<span class="c1"># Load any required modules</span>
module<span class="w"> </span>load<span class="w"> </span>mpt

<span class="c1"># Change to the submission directory</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c1"># Set the number of threads to the CPUs per task</span>
<span class="c1"># and propagate `--cpus-per-task` value to srun</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SRUN_CPUS_PER_TASK</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>

<span class="c1"># Launch the parallel job</span>
<span class="c1">#   Using 36 threads per node</span>
<span class="c1">#   srun picks up the distribution from the sbatch options</span>
srun<span class="w"> </span>--cpu-bind<span class="o">=</span>cores<span class="w"> </span>./my_openmp_executable.x
</code></pre></div>
<p>This will run your executable <code>my_openmp_executable.x</code> in parallel on 36
threads. Slurm will allocate 1 node to your job and srun will place 36
threads (one per physical core).</p>
<p>See above for a more detailed discussion of the different <code>sbatch</code>
options</p>
<h2 id="job-arrays">Job arrays</h2>
<p>The Slurm job scheduling system offers the <em>job array</em> concept, for
running collections of almost-identical jobs. For example, running the
same program several times with different arguments or input data.</p>
<p>Each job in a job array is called a <em>subjob</em>. The subjobs of a job array
can be submitted and queried as a unit, making it easier and cleaner to
handle the full set, compared to individual jobs.</p>
<p>All subjobs in a job array are started by running the same job script.
The job script also contains information on the number of jobs to be
started, and Slurm provides a subjob index which can be passed to the
individual subjobs or used to select the input data per subjob.</p>
<h3 id="job-script-for-a-job-array">Job script for a job array</h3>
<p>As an example, the following script runs 56 subjobs, with the subjob
index as the only argument to the executable. Each subjob requests a
single node and uses all 36 cores on the node by placing 1 MPI process
per core and specifies 4 hours maximum runtime per subjob:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>
<span class="c1"># Slurm job options (name, compute nodes, job time)</span>

<span class="c1">#SBATCH --name=Example_Array_Job</span>
<span class="c1">#SBATCH --time=04:00:00</span>
<span class="c1">#SBATCH --exclusive</span>
<span class="c1">#SBATCH --nodes=1</span>
<span class="c1">#SBATCH --tasks-per-node=36</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>
<span class="c1">#SBATCH --array=0-55</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="c1">#SBATCH --account=[budget code]</span>
<span class="c1"># We use the &quot;standard&quot; partition as we are running on CPU nodes</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1"># We use the &quot;standard&quot; QoS as our runtime is less than 4 days</span>
<span class="c1">#SBATCH --qos=standard</span>

<span class="c1"># Load the default HPE MPI environment</span>
module<span class="w"> </span>load<span class="w"> </span>mpt

<span class="c1"># Change to the submission directory</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically</span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

srun<span class="w"> </span>/path/to/exe<span class="w"> </span><span class="nv">$SLURM_ARRAY_TASK_ID</span>
</code></pre></div>
<h3 id="submitting-a-job-array">Submitting a job array</h3>
<p>Job arrays are submitted using <code>sbatch</code> in the same way as for standard
jobs:</p>
<pre><code>sbatch job_script.pbs
</code></pre>
<h2 id="job-chaining">Job chaining</h2>
<p>Job dependencies can be used to construct complex pipelines or chain
together long simulations requiring multiple steps.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code>--parsable</code> option to <code>sbatch</code> can simplify working with job
dependencies. It returns the job ID in a format that can be used as the
input to other commands.</p>
</div>
<p>For example:</p>
<pre><code>jobid=$(sbatch --parsable first_job.sh)
sbatch --dependency=afterok:$jobid second_job.sh
</code></pre>
<p>or for a longer chain:</p>
<pre><code>jobid1=$(sbatch --parsable first_job.sh)
jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh)
jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh)
sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh
</code></pre>
<h2 id="interactive-jobs">Interactive Jobs</h2>
<p>When you are developing or debugging code you often want to run many
short jobs with a small amount of editing the code between runs. This
can be achieved by using the login nodes to run small/short MPI jobs.
However, you may want to test on the compute nodes (e.g. you may want to
test running on multiple nodes across the high performance
interconnect). One way to achieve this on Cirrus is to use an
interactive jobs.</p>
<p>Interactive jobs via SLURM take two slightly different forms. The first
uses <code>srun</code> directly to allocate resource to be used interactively; the
second uses both <code>salloc</code> and <code>srun</code>.</p>
<h3 id="using-srun">Using srun</h3>
<p>An interactive job via <code>srun</code> allows you to execute commands directly
from the command line without using a job submission script, and to see
the output from your program directly in the terminal.</p>
<p>A convenient way to do this is as follows.</p>
<pre><code>[user@cirrus-login1]$ srun --exclusive --nodes=1 --time=00:20:00 --partition=standard --qos=standard --account=z04 --pty /usr/bin/bash --login
[user@r1i0n14]$
</code></pre>
<p>This requests the exclusive use of one node for the given time (here, 20
minutes). The <code>--pty /usr/bin/bash --login</code> requests an interactive
login shell be started. (Note the prompt has changed.) Interactive
commands can then be used as normal and will execute on the compute
node. When no longer required, you can type <code>exit</code> or CTRL-D to release
the resources and return control to the front end shell.</p>
<pre><code>[user@r1i0n14]$ exit
logout
[user@cirrus-login1]$
</code></pre>
<p>Note that the new interactive shell will reflect the environment of the
original login shell. If you do not wish this, add the <code>--export=none</code>
argument to <code>srun</code> to provide a clean login environment.</p>
<p>Within an interactive job, one can use <code>srun</code> to launch parallel jobs in
the normal way, e.g.,</p>
<pre><code>[user@r1i0n14]$ srun -n 2 ./a.out
</code></pre>
<p>In this context, one could also use <code>mpirun</code> directly. Note we are
limited to the 36 cores of our original <code>--nodes=1</code> <code>srun</code> request.</p>
<h3 id="using-salloc-with-srun">Using <code>salloc</code> with <code>srun</code></h3>
<p>This approach uses the<code>salloc</code> command to reserve compute nodes and then
<code>srun</code> to launch relevant work.</p>
<p>To submit a request for a job reserving 2 nodes (72 physical cores) for
1 hour you would issue the command:</p>
<div class="highlight"><pre><span></span><code><span class="o">[</span>user@cirrus-login1<span class="o">]</span>$<span class="w"> </span>salloc<span class="w"> </span>--exclusive<span class="w"> </span>--nodes<span class="o">=</span><span class="m">2</span><span class="w"> </span>--tasks-per-node<span class="o">=</span><span class="m">36</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>--time<span class="o">=</span><span class="m">01</span>:00:00<span class="w">  </span>--partition<span class="o">=</span>standard<span class="w"> </span>--qos<span class="o">=</span>standard<span class="w"> </span>--account<span class="o">=</span>t01
salloc:<span class="w"> </span>Granted<span class="w"> </span>job<span class="w"> </span>allocation<span class="w"> </span><span class="m">8699</span>
salloc:<span class="w"> </span>Waiting<span class="w"> </span><span class="k">for</span><span class="w"> </span>resource<span class="w"> </span>configuration
salloc:<span class="w"> </span>Nodes<span class="w"> </span>r1i7n<span class="o">[</span><span class="m">13</span>-14<span class="o">]</span><span class="w"> </span>are<span class="w"> </span>ready<span class="w"> </span><span class="k">for</span><span class="w"> </span>job
<span class="o">[</span>user@cirrus-login1<span class="o">]</span>$
</code></pre></div>
<p>Note that this starts a new shell on the login node associated with the
allocation (the prompt has not changed). The allocation may be released
by exiting this new shell.</p>
<pre><code>[user@cirrus-login1]$ exit
salloc: Relinquishing job allocation 8699
[user@cirrus-login1]$
</code></pre>
<p>While the allocation lasts you will be able to run parallel jobs on the
compute nodes by issuing the <code>srun</code> command in the normal way. The
resources available are those specified in the original <code>salloc</code>
command. For example, with the above allocation,</p>
<pre><code>$ srun ./mpi-code.out
</code></pre>
<p>will run 36 MPI tasks per node on two nodes.</p>
<p>If your allocation reaches its time limit, it will automatically be
termintated and the associated shell will exit. To check that the
allocation is still running, use <code>squeue</code>:</p>
<pre><code>[user@cirrus-login1]$ squeue -u user
           JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            8718  standard     bash    user   R       0:07      2 r1i7n[18-19]
</code></pre>
<p>Choose a time limit long enough to allow the relevant work to be
completed.</p>
<p>The <code>salloc</code> method may be useful if one wishes to associate operations
on the login node (e.g., via a GUI) with work in the allocation itself.</p>
<h2 id="reservations">Reservations</h2>
<p>Reservations are available on Cirrus. These allow users to reserve a
number of nodes for a specified length of time starting at a particular
time on the system.</p>
<p>Reservations require justification. They will only be approved if the
request could not be fulfilled with the standard queues. For example,
you require a job/jobs to run at a particular time e.g. for a
demonstration or course.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reservation requests must be submitted at least 120 hours in advance of
the reservation start time. We cannot guarantee to meet all reservation
requests due to potential conflicts with other demands on the service
but will do our best to meet all requests.</p>
</div>
<p>Reservations will be charged at 1.5 times the usual rate and our policy
is that they will be charged the full rate for the entire reservation at
the time of booking, whether or not you use the nodes for the full time.
In addition, you will not be refunded the compute time if you fail to
use them due to a job crash unless this crash is due to a system
failure.</p>
<p>To request a reservation you complete a form on SAFE:</p>
<ol>
<li>[Log into SAFE](<a href="https://safe.epcc.ed.ac.uk">https://safe.epcc.ed.ac.uk</a>)</li>
<li>Under the "Login accounts" menu, choose the "Request reservation"
     option</li>
</ol>
<p>On the first page, you need to provide the following:</p>
<ul>
<li>The start time and date of the reservation.</li>
<li>The end time and date of the reservation.</li>
<li>Your justification for the reservation -- this must be provided or
   the request will be rejected.</li>
<li>The number of nodes required.</li>
</ul>
<p>On the second page, you will need to specify which username you wish the
reservation to be charged against and, once the username has been
selected, the budget you want to charge the reservation to. (The
selected username will be charged for the reservation but the
reservation can be used by all members of the selected budget.)</p>
<p>Your request will be checked by the Cirrus User Administration team and,
if approved, you will be provided a reservation ID which can be used on
the system. To submit jobs to a reservation, you need to add
<code>--reservation=&lt;reservation ID&gt;</code> and <code>--qos=reservation</code> options to your
job submission script or Slurm job submission command.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You must have at least 1 CPUh - and 1 GPUh for reservations on GPU
nodes - to be able to submit jobs to reservations.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can submit jobs to a reservation as soon as the reservation has been
set up; jobs will remain queued until the reservation starts.</p>
</div>
<h2 id="serial-jobs">Serial jobs</h2>
<p>Unlike parallel jobs, serial jobs will generally not need to specify the
number of nodes and exclusive access (unless they want access to all of
the memory on a node. You usually only need the <code>--ntasks=1</code> specifier.
For example, a serial job submission script could look like:</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (name, compute nodes, job time)</span>
<span class="c1">#SBATCH --job-name=Example_Serial_Job</span>
<span class="c1">#SBATCH --time=0:20:0</span>
<span class="c1">#SBATCH --ntasks=1</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="c1">#SBATCH --account=[budget code]</span>
<span class="c1"># We use the &quot;standard&quot; partition as we are running on CPU nodes</span>
<span class="c1">#SBATCH --partition=standard</span>
<span class="c1"># We use the &quot;standard&quot; QoS as our runtime is less than 4 days</span>
<span class="c1">#SBATCH --qos=standard</span>

<span class="c1"># Change to the submission directory</span>
<span class="nb">cd</span><span class="w"> </span><span class="nv">$SLURM_SUBMIT_DIR</span>

<span class="c1"># Enforce threading to 1 in case underlying libraries are threaded</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Launch the serial job</span>
<span class="c1">#   Using 1 thread</span>
srun<span class="w"> </span>--cpu-bind<span class="o">=</span>cores<span class="w"> </span>./my_serial_executable.x
</code></pre></div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember that you will be allocated memory based on the number of tasks
(i.e. CPU cores) that you request. You will get ~7.1 GB per task/core.
If you need more than this for your serial job then you should ask for
the number of tasks you need for the required memory (or use the
<code>--exclusive</code> option to get access to all the memory on a node) and
launch specifying a single task using
<code>srun --ntasks=1 --cpu-bind=cores</code>.</p>
</div>
<h2 id="temporary-files-and-tmp-in-batch-jobs">Temporary files and <code>/tmp</code> in batch jobs</h2>
<p>Applications which normally read and write temporary files from <code>/tmp</code>
may require some care in batch jobs on Cirrus. As the size of <code>/tmp</code> on
backend nodes is relatively small (\&lt; 150 MB), applications should use a
different location to prevent possible failures. This is relevant for
both CPU and GPU nodes.</p>
<p>Note also that the default value of the variable <code>TMPDIR</code> in batch jobs
is a memory-resident file system location specific to the current job
(typically in the <code>/dev/shm</code> directory). Files here reduce the available
capacity of main memory on the node.</p>
<p>It is recommended that applications with significant temporary file
space requirement should use the <code>/user-guide/solidstate</code>. E.g., a
submission script might contain:</p>
<pre><code>export TMPDIR="/scratch/space1/x01/auser/$SLURM_JOBID.tmp"
mkdir -p $TMPDIR
</code></pre>
<p>to set the standard temporary directory to a unique location in the
solid state storage. You will also probably want to add a line to clean
up the temporary directory at the end of your job script, e.g.</p>
<pre><code>rm -r $TMPDIR
</code></pre>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Applications should not hard-code specific locations such as <code>/tmp</code>.
Parallel applications should further ensure that there are no collisions
in temporary file names on separate processes/nodes.</p>
</div>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/EPCCed" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["tabs"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.1e8ae164.min.js"></script>
      
    
  </body>
</html>