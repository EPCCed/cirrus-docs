{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":""},{"location":"#cirrus","title":"Cirrus","text":"<p>Cirrus is a HPC and data science service hosted and run by EPCC at The University of Edinburgh. It is supported by the Edinburgh and South East Scotland City Region Deal and by  EPSRC. It is one of the UKRI Tier-2 National HPC Services.</p> <p>Cirrus is available to industry and academic researchers. For information on how to get access to the system please see the Cirrus website.</p> <p>The Cirrus service is an HPE Cray EX4000 CPU-based system. There are 192 standard compute nodes with 768 GB per node DDR5 memory and 64 high memory nodes with 1,536 GB per node. All compute nodes have dual AMD EPYC 9825 processors with 144 physical cores each (288 physical cores per node). In total the system has 256 compute nodes with 72,728 cores. Nodes are connected by a high-performance Slingshot 11 network.</p> <p>Important</p> <p>This documentation is for the new Cirrus EX4000 system that started service on 24 November 2025. The old Cirrus SIG ICE will continue running in parallel for a short time (until 8 December 2025). Documentation for the older Cirrus system is still available.</p> <p>The menu includes links to:</p> <ul> <li>General User Guide: information on how to use Cirrus.</li> <li>Software Applications: using specific software applications   on Cirrus.</li> <li>Software Libraries: compiling against specific libraries on   Cirrus.</li> <li>Software Tools: Information on using tools such as debuggers and   profilers on Cirrus.</li> </ul> <p>Information on using the SAFE web interface for managing and reporting on your usage on Cirrus, applying for user accounts, and more, can be found in the EPCC SAFE Documentation.</p> <p>If you have questions about Cirrus, please contact the service desk: <code>support@cirrus.ac.uk</code>.</p>"},{"location":"migration-2025/","title":"Cirrus migration 2025","text":"<p>Important</p> <p>This information was last updated on 20 Nov 2025</p> <p>This section of the documentation is a guide on the migration process from the  old Cirrus system (SGI ICE XA) to the new Cirrus system (HPE Cray EX4000) in Q4 2025.</p> <p>Important</p> <p>The date for the switch from old Cirrus system to the new Cirrus system is Monday 8 December 2025.</p> <p>After the switch you will no longer be able to access the old Cirrus login and compute nodes or the RPOOL (/scratch) solid state storage.</p> <p>More information is available on the following pages</p> <ul> <li>Information for users and projects not continuing on Cirrus EX4000, includes:<ul> <li>Requirement to transfer data off Cirrus before switch date</li> <li>Information on how to apply for access to Cirrus EX4000</li> </ul> </li> <li>Migrating your account to Cirrus EX4000, includes:<ul> <li>Checking if your project/account will migrate to Cirrus EX4000</li> <li>Checking how much resource you will have on Cirrus EX4000</li> <li>How to setup your account on Cirrus EX4000</li> </ul> </li> <li>Migrating data to Cirrus EX4000, includes:<ul> <li>How existing project/user data will be migrated to Cirrus EX4000</li> <li>Requirement to transfer data off /scratch (RPOOL) before switch date</li> </ul> </li> <li>Main differences between old Cirrus and Cirrus EX4000, includes:<ul> <li>Summary of hardware differences</li> <li>Summary of software differences</li> </ul> </li> </ul> <p>Tip</p> <p>If you need help or have questions on this migration process,  please contact the Cirrus service desk</p>"},{"location":"migration-2025/account-migration/","title":"Migrating your account to Cirrus EX4000","text":"<p>Important</p> <p>This information was last updated on 20 Nov 2025</p> <p>This section covers the following questions:</p> <ul> <li>When will I be able to access Cirrus EX4000?</li> <li>Has my project has been migrated to Cirrus EX4000?</li> <li>How much resource will my project have on Cirrus EX4000?</li> <li>How do I set up an account on Cirrus EX4000?</li> <li>How do I log into Cirrus EX4000 for the first time?</li> </ul> <p>Tip</p> <p>If you need help or have questions on this migration process,  please contact the Cirrus service desk</p>"},{"location":"migration-2025/account-migration/#when-will-i-be-able-to-access-cirrus-ex4000","title":"When will I be able to access Cirrus EX4000?","text":"<p>We are planning a preview period in week beginning 24 Nov 2025 when all users and projects migrating to Cirrus EX4000 will have access to test use of the system in an uncharged mode.</p> <p>Full service will start on Monday 8 December 2025. At this point jobs will be  charged against budgets.</p>"},{"location":"migration-2025/account-migration/#has-my-project-been-migrated-to-cirrus-ex4000","title":"Has my project been migrated to Cirrus EX4000?","text":"<p>If you have an active Cirrus CPU allocation beyond 31 Dec 2025 then your project will very likely be migrated to Cirrus EX4000. If your project is migrated to Cirrus EX4000 then it will have the same project code as it had on old Cirrus.</p>"},{"location":"migration-2025/account-migration/#checking-if-your-project-has-been-migrated","title":"Checking if your project has been migrated","text":"<p>How you check if your project has been migrated depends on whether you are a project PI/manager or user:</p> <ul> <li>If you are a PI/manager: If your project has been migrated to Cirrus EX4000 then   you will be able to see this on the Project page in SAFE. In particular, you   will have a button labelled   \"Manage Group Time Allocations for CirrusEX (Allocations on Cirrus (2025 refresh))\"   in the \"Time Budgets\" section on the project page.</li> <li>If you are a user: If your project has been migrated to Cirrus EX4000 then you will   see an project \"CirrusEX\" coreh allocation on your login account page in SAFE.</li> </ul>"},{"location":"migration-2025/account-migration/#how-much-resource-will-my-project-have-on-cirrus-ex4000","title":"How much resource will my project have on Cirrus EX4000?","text":"<p>As on the old Cirrus CPU partition, the unit of allocation on Cirrus EX4000 is core hours (coreh). Your initial allocations on Cirrus EX4000 will have the  same number of coreh as old Cirrus allocations.</p>"},{"location":"migration-2025/account-migration/#how-do-i-set-up-an-cirrus-ex4000-account","title":"How do I set up an Cirrus EX4000 account?","text":"<p>How you setup your account depends on whether you have an existing Cirrus account in the project you are using for access or if you need a new account.</p>"},{"location":"migration-2025/account-migration/#migrate-an-existing-cirrus-account-to-cirrus-ex4000","title":"Migrate an existing Cirrus account to Cirrus EX4000","text":"<ol> <li>Login to EPCC SAFE</li> <li>Use the \"Login accounts\" menu to select your existing Cirrus account (this will be in the format <code>username@eidf</code>)</li> <li>At the bottom of the account page, click the \"Add Machine\" button</li> <li>Select \"Cirrus: Cirrus 2025 refresh\" from the dropdown list</li> <li>Click \"Join\"</li> </ol> <p>You should now be able to login to Cirrus EX4000 using the existing credentials  associated with this account.</p>"},{"location":"migration-2025/account-migration/#create-a-new-account-on-cirrus-ex4000","title":"Create a new account on Cirrus EX4000","text":"<p>Use the standard account creation process in SAFE and select \"Cirrus: Cirrus 2025 refresh\" for the machine to request an account on.</p> <p>You will need to setup credentials (MFA/TOTP token and SSH key) in the usual way to be able to login to Cirrus EX4000.</p>"},{"location":"migration-2025/account-migration/#how-do-i-log-into-cirrus-ex4000-for-the-first-time","title":"How do I log into Cirrus EX4000 for the first time?","text":"<p>You will log into Cirrus EX4000 in the same way as you logged in to old Cirrus using SSH to access <code>login.cirrus.ac.uk</code> using an SSH key pair and TOTP. Your SSH key and TOTP will be the same as it was on old Cirrus. If you have issues, the Cirrus documentation covers logging from a variety of operating systems along with troubleshooting tips:</p> <ul> <li>Logging in to Cirrus</li> </ul> <p>If you are still having issues after following the documentation and troubleshooting tips, please contact the Cirrus service desk </p>"},{"location":"migration-2025/cirrus-differences/","title":"Main differences on Cirrus EX4000","text":"<p>Important</p> <p>This information was last updated on 20 Nov 2025</p> <p>This section provides an overview of the main differences between the current Cirrus system and Cirrus EX4000 along with links to more information where appropriate. It will be udated with more information as it becomes available.</p>"},{"location":"migration-2025/cirrus-differences/#hardware","title":"Hardware","text":"<ul> <li>There are no GPU nodes on Cirrus EX4000</li> <li>There are 288 cores on a Cirrus EX4000 compute node rather than 36 on current Cirrus</li> <li>Cirrus EX4000 will use the HPE Cray Slingshot 11 interconnect rather than the    Infiniband interconnect used on current Cirrus</li> </ul>"},{"location":"migration-2025/cirrus-differences/#software","title":"Software","text":"<ul> <li>The software environment will completely change to be based on the   HPE Cray Programming Environment - no modules that were available   on old Cirrus system will be available in the same way on the new Cirrus   system. We will update the documentation to cover these changes in    detail ahead of the switch between systems.</li> </ul>"},{"location":"migration-2025/cirrus-differences/#slurm-scheduler-configuration","title":"Slurm scheduler configuration","text":"<ul> <li>Many of the partitions and QoS will be similar between the two systems   but limits will change. These will be documented in more detail ahead of    the switch.</li> </ul>"},{"location":"migration-2025/data-migration/","title":"Data migration to Cirrus EX4000","text":"<p>Important</p> <p>This information was last updated on 20 Nov 2025</p>"},{"location":"migration-2025/data-migration/#home-and-work-file-systems","title":"/home and /work file systems","text":"<p>The same file systems that provided /home and /work on the old Cirrus system will be mounted on Cirrus EX4000 in the same locations. This means that when you log onto Cirrus EX4000, you will find all the data that you had on Cirrus available in the same locations.</p> <p>Important</p> <p>The data available will include any compiled applications and libraries that you built or installed on the old Cirrus system. These binaries will almost certainly not work on Cirrus EX4000 and you should expect and plan to rebuild, re-install and test your applications on Cirrus EX4000.</p>"},{"location":"migration-2025/data-migration/#solid-state-scratch-rpool-file-system","title":"Solid state /scratch (RPOOL) file system","text":"<p>The solid state /scratch file system on old Cirrus will not be available on  Cirrus EX4000 and there will be no way to retrieve data from it once old Cirrus  has ended. You should move any data you wish to keep on this file system to an alternative location before the end of the current Cirrus system at 08:00 GMT on Monday 8 December 2025.</p>"},{"location":"migration-2025/not-migrating/","title":"Impacts for projects not migrating to Cirrus EX4000","text":"<p>Important</p> <p>This information was last updated on 20 Nov 2025</p> <p>This page summarises the impacts on users and projects who are not migrating to Cirrus EX4000 on 8 December 2025. It also highlights any actions you need to take.</p> <p>Tip</p> <p>If you are unsure whether or not your project is migrating to  Cirrus EX4000, see the information on the account migration page.</p>"},{"location":"migration-2025/not-migrating/#data-access","title":"Data access","text":"<p>Projects and user accounts that are not migrating to Cirrus EX4000 will lose access to any data on Cirrus from 08:00 GMT on Monday 8 December 2025.</p> <p>Users and projects should copy any data they wish to keep off Cirrus before  08:00 on Monday 8 December 2025.</p>"},{"location":"migration-2025/not-migrating/#getting-access-to-cirrus-ex4000","title":"Getting access to Cirrus EX4000","text":"<p>If projects want to access Cirrus EX4000 for their research, there are a number of different access routes. Please consult the Access page on the Cirrus website for details on how to do this.</p>"},{"location":"software-libraries/fftw/","title":"FFTW","text":"<ul> <li>Provides: FFTW v3</li> <li>Access: <code>module load cray-fftw</code></li> </ul> <p>FFTW is a C subroutine library (which includes a Fortran interface) for computing the discrete Fourier transform (DFT) in one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/odd data, i.e. the discrete cosine/sine transforms or DCT/DST).</p> <p>Only the version 3 interface is available on Cirrus.</p>"},{"location":"software-libraries/hdf5/","title":"HDF5","text":"<p>The Hierarchical Data Format HDF5 (and its parallel manifestation HDF5 parallel) is a standard library and data format developed and supported by The HDF Group, and is released under a BSD-like license.</p> <p>Both serial and parallel versions are available on ACirrus as standard modules:</p> <ul> <li><code>module load cray-hdf5</code> (serial version)</li> <li><code>module load cray-hdf5-parallel</code> (MPI parallel version)</li> </ul> <p>Use <code>module help</code> to locate <code>cray-</code>specific release notes on a particular version.</p>"},{"location":"software-libraries/hdf5/#compiling-applications-against-hdf5","title":"Compiling applications against HDF5","text":"<p>If the appropriate programming environment and HDF5 modules are loaded, compiling applications against the HDF5 libraries should straightforward. You should use the compiler wrappers <code>cc</code>, <code>CC</code>, and/or <code>ftn</code>. See, e.g., <code>cc --cray-print-opts</code> for the full list of include paths and library paths and options added by the compiler wrapper.</p>"},{"location":"software-libraries/hdf5/#resources","title":"Resources","text":"<p>The HDF5 support website includes general documentation.</p> <p>For parallel HDF5, some tutorials and presentations are available.</p>"},{"location":"software-libraries/libsci/","title":"HPE Cray LibSci","text":"<ul> <li>Provides: BLAS, LAPACK, CBLAS, LAPACKE, BLACS, ScaLAPACK</li> <li>Access: <code>module load cray-libsci</code> (note: loaded by default for all users)</li> </ul> <p>Cray scientific libraries, available for all compiler choices provides access to the Fortran BLAS and LAPACK interface for basic linear algebra, the corresponding C interfaces CBLAS and LAPACKE, and BLACS and ScaLAPACK for parallel linear algebra. Type <code>man intro_libsci</code> for further details.</p>"},{"location":"software-libraries/netcdf/","title":"NetCDF","text":"<p>The Network Common Data Form NetCDF (and its parallel manifestation NetCDF parallel) is a standard library and data format developed and supported by UCAR is released under a BSD-like license.</p> <p>Both serial and parallel versions are available on Cirrus as standard modules:</p> <ul> <li><code>module load cray-netcdf</code> (serial version)</li> <li><code>module load cray-netcdf-hdf5parallel</code> (MPI parallel version)</li> </ul> <p>Note that one should first load the relevant HDF module file, e.g., <pre><code>$ module load cray-hdf5\n$ module load cray-netcdf\n</code></pre> for the serial version.</p> <p>Use <code>module spider</code> to locate available versions, and use <code>module help</code> to locate <code>cray-</code>specific release notes on a particular version.</p>"},{"location":"software-libraries/netcdf/#resources","title":"Resources","text":"<p>The NetCDF home page.</p>"},{"location":"software-packages/MATLAB/","title":"MATLAB","text":"<p>MATLAB combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly.</p>"},{"location":"software-packages/MATLAB/#useful-links","title":"Useful Links","text":"<ul> <li>MATLAB Documentation</li> </ul>"},{"location":"software-packages/MATLAB/#using-matlab-on-cirrus","title":"Using MATLAB on Cirrus","text":"<p>MATLAB R2020b and R2021b are available on Cirrus. R2020b is the  current default.</p> <p>This installation of MATLAB on Cirrus is covered by an Academic License - for use in teaching, academic research, and meeting course requirements at degree granting institutions only. Not for government, commercial, or other organizational use.</p> <p>If your use of MATLAB is not covered by this license then please do not use this installation. Please contact the Cirrus Helpdesk to arrange use of your own MATLAB license on Cirrus.</p> <p>Detailed version information:</p> <pre><code>-----------------------------------------------------------------------------------------------------\nMATLAB Version: 9.9.0.2037887 (R2020b) Update 8\nMATLAB License Number: 904098\nOperating System: Linux 4.18.0-305.25.1.el8_4.x86_64 #1 SMP Mon Oct 18 14:34:11 EDT 2021 x86_64\nJava Version: Java 1.8.0_202-b08 with Oracle Corporation Java HotSpot(TM) 64-Bit Server VM mixed mode\n-----------------------------------------------------------------------------------------------------\nMATLAB                                                Version 9.9         (R2020b)\nSimulink                                              Version 10.2        (R2020b)\nDSP System Toolbox                                    Version 9.11        (R2020b)\nDeep Learning HDL Toolbox                             Version 1.0         (R2020b)\nDeep Learning Toolbox                                 Version 14.1        (R2020b)\nImage Processing Toolbox                              Version 11.2        (R2020b)\nParallel Computing Toolbox                            Version 7.3         (R2020b)\nSignal Processing Toolbox                             Version 8.5         (R2020b)\nStatistics and Machine Learning Toolbox               Version 12.0        (R2020b)\nSymbolic Math Toolbox                                 Version 8.6         (R2020b)\nWavelet Toolbox                                       Version 5.5         (R2020b)\n</code></pre>"},{"location":"software-packages/MATLAB/#running-matlab-jobs","title":"Running MATLAB jobs","text":"<p>On Cirrus, MATLAB is intended to be used on the compute nodes within Slurm job scripts. Use on the login nodes should be restricted to setting preferences, accessing help, and launching MDCS jobs. It is recommended that MATLAB is used without a GUI on the compute nodes, as the interactive response is slow.</p>"},{"location":"software-packages/MATLAB/#running-parallel-matlab-jobs-using-the-local-cluster","title":"Running parallel MATLAB jobs using the local cluster","text":"<p>The license for this installation of MATLAB provides only 32 workers via MDCS but provides 36 workers via the local cluster profile (there are 36 cores on a Cirrus compute node), so we only recommend the use of MDCS to test the configuration of distributed memory parallel computations for eventual use of your own MDCS license.</p> <p>The local cluster should be used within a Slurm job script - you submit a job that runs MATLAB and uses the local cluster, which is the compute node that the job is running on.</p> <p>MATLAB will normally use up to the total number of cores on a node for multi-threaded operations (e.g. matrix inversions) and for parallel computations. It also make no restriction on its memory use. These features are incompatible with the shared use of nodes on Cirrus. For the local cluster, a wrapper script is provided to limit the number of cores and amount of memory used, in proportion to the number of CPUs selected in the Slurm job script. Please use this wrapper instead of using MATLAB directly.</p> <p>Say you have a job that requires 3 workers, each running 2 threads. As such, you should employ 3x2=6 cores. An example job script for this particular case would be :</p> <pre><code>#SBATCH --job-name=Example_MATLAB_Job\n#SBATCH --time=0:20:0\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=6\n#SBATCH --cpus-per-task=1\n\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\nmodule load matlab\n\nmatlab_wrapper -nodisplay &lt; /mnt/lustre/indy2lfs/sw/cse-matlab/examples/testp.m &gt; testp.log\n</code></pre> <p>Note, for MATLAB versions R2019 and later, the matlab_wrapper_2019 script may be required (see 2019 section below).</p> <p>This would run the testp.m script, without a display, and exit when testp.m has finished. 6 CPUs are selected, which correspond to 6 cores, and the following limits would be set initially :</p> <pre><code>ncores = 6\nmemory = 42GB\n\nMaximum number of computational threads (maxNumCompThreads)          = 6\nPreferred number of workers in a parallel pool (PreferredNumWorkers) = 6\nNumber of workers to start on your local machine (NumWorkers)        = 6\nNumber of computational threads to use on each worker (NumThreads)   = 1\n</code></pre> <p>The testp.m program sets NumWorkers to 3 and NumThreads to 2 :</p> <pre><code>cirrus_cluster = parcluster('local');\nncores = cirrus_cluster.NumWorkers * cirrus_cluster.NumThreads;\ncirrus_cluster.NumWorkers = 3;\ncirrus_cluster.NumThreads = 2;\nfprintf(\"NumWorkers = %d NumThreads = %d ncores = %d\\n\",cirrus_cluster.NumWorkers,cirrus_cluster.NumThreads,ncores);\nif cirrus_cluster.NumWorkers * cirrus_cluster.NumThreads &gt; ncores\n    disp(\"NumWorkers * NumThreads &gt; ncores\");\n    disp(\"Exiting\");\n    exit(1);\nend\nsaveProfile(cirrus_cluster);\nclear cirrus_cluster;\n\n\nn = 3;\nA = 3000;\n\na=zeros(A,A,n);\nb=1:n;\n\nparpool;\n\ntic\nparfor i = 1:n\n    a(:,:,i) = rand(A);\nend\ntoc\ntic\nparfor i = 1:n\n    b(i) = max(abs(eig(a(:,:,i))));\nend\ntoc\n</code></pre> <p>Note that PreferredNumWorkers, NumWorkers and NumThreads persist between MATLAB sessions but will be updated correctly if you use the wrapper each time.</p> <p>NumWorkers and NumThreads can be changed (using parcluster and saveProfile) but NumWorkers * NumThreads should be less than or equal to the number of cores (ncores above). If you wish a worker to run a threaded routine in serial, you must set NumThreads to 1 (the default).</p> <p>If you specify exclusive node access, then all the cores and memory will be available. On the login nodes, a single core is used and memory is not limited.</p>"},{"location":"software-packages/MATLAB/#matlab-2019-versions","title":"MATLAB 2019 versions","text":"<p>There has been a change of configuration options for MATLAB from version R2019 and onwards that means the -r flag has been replaced with the -batch flag. To accommodate that a new job wrapper script is required to run applications. For these versions of MATLAB, if you need to use the -r or -batch flag replace this line in your Slurm script, i.e.:</p> <pre><code>matlab_wrapper -nodisplay -nodesktop -batch \"main_simulated_data_FINAL_clean(\"$ind\",\"$gamma\",\"$rw\",'\"$SLURM_JOB_ID\"')\n</code></pre> <p>with:</p> <pre><code>matlab_wrapper_2019 -nodisplay -nodesktop -batch \"main_simulated_data_FINAL_clean(\"$ind\",\"$gamma\",\"$rw\",'\"$SLURM_JOB_ID\"')\n</code></pre> <p>and this should allow scripts to run normally.</p>"},{"location":"software-packages/MATLAB/#running-parallel-matlab-jobs-using-mdcs","title":"Running parallel MATLAB jobs using MDCS","text":"<p>It is possible to use MATLAB on the login node to set up an MDCS Slurm cluster profile and then launch jobs using that profile. However, this does not give per-job control of the number of cores and walltime; these are set once in the profile.</p> <p>This MDCS profile can be used in MATLAB on the login node - the MDCS computations are done in Slurm jobs launched using the profile.</p>"},{"location":"software-packages/MATLAB/#configuration","title":"Configuration","text":"<p>Start MATLAB on the login node. Configure MATLAB to run parallel jobs on your cluster by calling configCluster. For each cluster, configCluster only needs to be called once per version of MATLAB :</p> <pre><code>configCluster\n</code></pre> <p>Jobs will now default to the cluster rather than submit to the local machine (the login node in this case).</p>"},{"location":"software-packages/MATLAB/#configuring-jobs","title":"Configuring jobs","text":"<p>Prior to submitting the job, you can specify various parameters to pass to our jobs, such as walltime, e-mail, etc. Other than ProjectCode and WallTime, none of these are required to be set.</p> <p>NOTE: Any parameters specified using this workflow will be persistent between MATLAB sessions :</p> <pre><code>% Get a handle to the cluster.\nc = parcluster('cirrus');\n\n% Assign the project code for the job.  **[REQUIRED]**\nc.AdditionalProperties.ProjectCode = 'project-code';\n\n% Specify the walltime (e.g. 5 hours).  **[REQUIRED]**\nc.AdditionalProperties.WallTime = '05:00:00';\n\n% Specify e-mail address to receive notifications about your job.\nc.AdditionalProperties.EmailAddress = 'your_name@your_address';\n\n% Request a specific reservation to run your job.  It is better to\n% use the queues rather than a reservation.\nc.AdditionalProperties.Reservation = 'your-reservation';\n\n% Set the job placement (e.g., pack, excl, scatter:excl).\n% Usually the default of free is what you want.\nc.AdditionalProperties.JobPlacement = 'pack';\n\n% Request to run in a particular queue.  Usually the default (no\n% specific queue requested) will route the job to the correct queue.\nc.AdditionalProperties.QueueName = 'queue-name';\n</code></pre> <p>Save changes after modifying AdditionalProperties fields :</p> <pre><code>c.saveProfile\n</code></pre> <p>To see the values of the current configuration options, call the specific AdditionalProperties name :</p> <pre><code>c.AdditionalProperties\n</code></pre> <p>To clear a value, assign the property an empty value ('', [], or false) :</p> <pre><code>% Turn off email notifications.\nc.AdditionalProperties.EmailAddress = '';\n</code></pre>"},{"location":"software-packages/MATLAB/#interactive-jobs","title":"Interactive jobs","text":"<p>To run an interactive pool job on the cluster, use parpool as before. configCluster sets NumWorkers to 32 in the cluster to match the number of MDCS workers available in our TAH licence. If you have your own MDCS licence, you can change this by setting c.NumWorkers and saving the profile. :</p> <pre><code>% Open a pool of 32 workers on the cluster.\np = parpool('cirrus',32);\n</code></pre> <p>Rather than running locally on one compute node machine, this pool can run across multiple nodes on the cluster :</p> <pre><code>% Run a parfor over 1000 iterations.\nparfor idx = 1:1000\n  a(idx) = ...\nend\n</code></pre> <p>Once you have finished using the pool, delete it :</p> <pre><code>% Delete the pool\np.delete\n</code></pre>"},{"location":"software-packages/MATLAB/#serial-jobs","title":"Serial jobs","text":"<p>Rather than running interactively, use the batch command to submit asynchronous jobs to the cluster. This is generally more useful on Cirrus, which usually has long queues. The batch command will return a job object which is used to access the output of the submitted job. See the MATLAB documentation for more help on batch :</p> <pre><code>% Get a handle to the cluster.\nc = parcluster('cirrus');\n\n% Submit job to query where MATLAB is running on the cluster.\nj = c.batch(@pwd, 1, {});\n\n% Query job for state.\nj.State\n\n% If state is finished, fetch results.\nj.fetchOutputs{:}\n\n% Delete the job after results are no longer needed.\nj.delete\n</code></pre> <p>To retrieve a list of currently running or completed jobs, call parcluster to retrieve the cluster object. The cluster object stores an array of jobs that were run, are running, or are queued to run. This allows you to fetch the results of completed jobs. Retrieve and view the list of jobs as shown below :</p> <pre><code>c = parcluster('cirrus');\njobs = c.Jobs\n</code></pre> <p>Once you have identified the job you want, you can retrieve the results as you have done previously.</p> <p>fetchOutputs is used to retrieve function output arguments; if using batch with a script, use load instead. Data that has been written to files on the cluster needs be retrieved directly from the file system.</p> <p>To view results of a previously completed job :</p> <pre><code>% Get a handle on job with ID 2.\nj2 = c.Jobs(2);\n</code></pre> <p>NOTE: You can view a list of your jobs, as well as their IDs, using the above c.Jobs command :</p> <pre><code>% Fetch results for job with ID 2.\nj2.fetchOutputs{:}\n\n% If the job produces an error, view the error log file.\nc.getDebugLog(j.Tasks(1))\n</code></pre> <p>NOTE: When submitting independent jobs, with multiple tasks, you will have to specify the task number.</p>"},{"location":"software-packages/MATLAB/#parallel-jobs","title":"Parallel jobs","text":"<p>Users can also submit parallel workflows with batch. You can use the following example (parallel_example.m) for a parallel job :</p> <pre><code>function t = parallel_example(iter)\n\n  if nargin==0, iter = 16; end\n\n  disp('Start sim')\n\n  t0 = tic;\n  parfor idx = 1:iter\n    A(idx) = idx;\n    pause(2);\n  end\n  t =toc(t0);\n\n  disp('Sim completed.')\n</code></pre> <p>Use the batch command again, but since you are running a parallel job, you also specify a MATLAB Pool :</p> <pre><code>% Get a handle to the cluster.\nc = parcluster('cirrus');\n\n% Submit a batch pool job using 4 workers for 16 simulations.\nj = c.batch(@parallel_example, 1, {}, 'Pool', 4);\n\n% View current job status.\nj.State\n\n% Fetch the results after a finished state is retrieved.\nj.fetchOutputs{:}\n\nans =\n\n8.8872\n</code></pre> <p>The job ran in 8.89 seconds using 4 workers. Note that these jobs will always request N+1 CPU cores, since one worker is required to manage the batch job and pool of workers. For example, a job that needs eight workers will consume nine CPU cores. With a MDCS licence for 32 workers, you will be able to have a pool of 31 workers.</p> <p>Run the same simulation but increase the Pool size. This time, to retrieve the results later, keep track of the job ID.</p> <p>NOTE: For some applications, there will be a diminishing return when allocating too many workers, as the overhead may exceed computation time. :</p> <pre><code>% Get a handle to the cluster.\nc = parcluster('cirrus');\n\n% Submit a batch pool job using 8 workers for 16 simulations.\nj = c.batch(@parallel_example, 1, {}, 'Pool', 8);\n\n% Get the job ID\nid = j.ID\n\nId =\n\n4\n</code></pre> <pre><code>% Clear workspace, as though you have quit MATLAB.\nclear j\n</code></pre> <p>Once you have a handle to the cluster, call the findJob method to search for the job with the specified job ID :</p> <pre><code>% Get a handle to the cluster.\nc = parcluster('cirrus');\n\n% Find the old job\nj = c.findJob('ID', 4);\n\n% Retrieve the state of the job.\nj.State\n\nans\n\nfinished\n\n% Fetch the results.\nj.fetchOutputs{:};\n\nans =\n\n4.7270\n\n% If necessary, retrieve an output/error log file.\nc.getDebugLog(j)\n</code></pre> <p>The job now runs 4.73 seconds using 8 workers. Run code with different number of workers to determine the ideal number to use.</p> <p>Alternatively, to retrieve job results via a graphical user interface, use the Job Monitor (Parallel &gt; Monitor Jobs).</p> <p></p>"},{"location":"software-packages/MATLAB/#debugging","title":"Debugging","text":"<p>If a serial job produces an error, you can call the getDebugLog method to view the error log file :</p> <pre><code>j.Parent.getDebugLog(j.Tasks(1))\n</code></pre> <p>When submitting independent jobs, with multiple tasks, you will have to specify the task number. For Pool jobs, do not dereference into the job object :</p> <pre><code>j.Parent.getDebugLog(j)\n</code></pre> <p>The scheduler ID can be derived by calling schedID :</p> <pre><code>schedID(j)\n\nans\n\n25539\n</code></pre>"},{"location":"software-packages/MATLAB/#to-learn-more","title":"To learn more","text":"<p>To learn more about the MATLAB Parallel Computing Toolbox, check out these resources:</p> <ul> <li>Parallel Computing Coding   Examples</li> <li>Parallel Computing   Documentation</li> <li>Parallel Computing   Overview</li> <li>Parallel Computing   Tutorials</li> <li>Parallel Computing   Videos</li> <li>Parallel Computing   Webinars</li> </ul>"},{"location":"software-packages/castep/","title":"CASTEP","text":"<p>CASTEP is a leading code for calculating the properties of materials from first principles. Using density functional theory, it can simulate a wide range of properties of materials proprieties including energetics, structure at the atomic level, vibrational properties, electronic response properties etc. In particular it has a wide range of spectroscopic features that link directly to experiment, such as infra-red and Raman spectroscopies, NMR, and core level spectra.</p>"},{"location":"software-packages/castep/#useful-links","title":"Useful Links","text":"<ul> <li>CASTEP Documentation and Tutorials</li> <li>CASTEP Licensing</li> </ul>"},{"location":"software-packages/castep/#using-castep-on-cirrus","title":"Using CASTEP on Cirrus","text":"<p>CASTEP is only available to users who have a valid CASTEP licence.</p> <p>If you have a CASTEP licence and wish to have access to CASTEP on Cirrus please submit a request through the SAFE.</p>"},{"location":"software-packages/castep/#running-parallel-castep-jobs","title":"Running parallel CASTEP jobs","text":"<p>CASTEP can exploit multiple nodes on Cirrus and can be run on a subset of cores on a node or across multiple nodes (with exclusive node access).</p>"},{"location":"software-packages/castep/#example-multi-core-castep-job","title":"Example: multi-core CASTEP job","text":"<p>For example, the following script will run a CASTEP job using 36 cores on a  single node. You may share node resources with other users.</p> <pre><code>#!/bin/bash\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=CASTEP_Example\n#SBATCH --time=1:0:0\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=36\n#SBATCH --cpus-per-task=1\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load CASTEP module\nmodule load castep\n\n# Set OMP_NUM_THREADS=1 to avoid unintentional threading\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run using input in test_calc.in\nsrun --distribution=block:block castep.mpi test_calc\n</code></pre>"},{"location":"software-packages/castep/#example-multi-node-castep-job","title":"Example: multi-node CASTEP job","text":"<p>For example, the following script will run a CASTEP job using 2 nodes (576 cores).</p> <pre><code>#!/bin/bash\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=CASTEP_Example\n#SBATCH --time=1:0:0\n#SBATCH --exclusive\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=288\n#SBATCH --cpus-per-task=1\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load CASTEP module\nmodule load castep\n\n# Set OMP_NUM_THREADS=1 to avoid unintentional threading\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run using input in test_calc.in\nsrun --distribution=block:block castep.mpi test_calc\n</code></pre>"},{"location":"software-packages/cp2k/","title":"CP2K","text":"<p>CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems. CP2K provides a general framework for different modelling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, \u2026), and classical force fields (AMBER, CHARMM, \u2026). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimisation, and transition state optimisation using NEB or dimer method.</p>"},{"location":"software-packages/cp2k/#useful-links","title":"Useful Links","text":"<ul> <li>CP2K Reference Manual</li> <li>CP2K HOWTOs</li> <li>CP2K FAQs</li> </ul>"},{"location":"software-packages/cp2k/#using-cp2k-on-cirrus","title":"Using CP2K on Cirrus","text":"<p>CP2K is available through the <code>cp2k</code> module. Loading this module provides access to the MPI/OpenMP hybrid <code>cp2k.psmp</code> executable. To see which versions are available: <pre><code>$ module avail cp2k\n...\n   cp2k/2025.2\n</code></pre></p>"},{"location":"software-packages/cp2k/#optional-packages","title":"Optional packages","text":"<p>The centrally installed module versions of CP2K are managed by Spack. To see the list of optional packages built, along with the compiler details, one can use, e.g.: <pre><code>$ module load cp2k\n$ cp2k.psmp --version\n CP2K version 2025.2\n Source code revision\n cp2kflags: omp libint fftw3 libxc parallel scalapack xsmm\n compiler: GCC version 14.2.1 20240801 (Red Hat 14.2.1-1)\n...\n</code></pre> showing that CP2K has been built with OpenMP, libint, FFTW3, and so on.</p>"},{"location":"software-packages/cp2k/#running-parallel-cp2k-jobs-mpiopenmp-hybrid","title":"Running parallel CP2K jobs (MPI/OpenMP hybrid)","text":"<p>To run CP2K using MPI and OpenMP, load the <code>cp2k</code> module and use the <code>cp2k.psmp</code> executable.</p> <p>For larger jobs requiring one or more nodes, an exclusive SLURM submission is appropriate. For example, a job using two nodes (576 cores), with two cores (OpenMP threads) for each MPI process:</p> Exclusive SLURM job submission script for CP2K <p><pre><code>#!/bin/bash\n\n#SBATCH --export=none\n#SBATCH --time=00:20:00\n\n#SBATCH --nodes=2\n#SBATCH --exclusive\n\n#SBATCH --ntasks-per-node=188\n#SBATCH --cpus-per-task=2\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n#SBATCH --distribution=block:block\n#SBATCH --hint=nomultithread\n\nmodule load cp2k\n\nexport OMP_NUM_THREADS=2\nexport OMP_PLACES=cores\n\nsrun cp2k.psmp -i H2O-0512.inp\n</code></pre> In this example, SLURM allocates two nodes based on <code>--nodes=2</code> and <code>--exclusive</code>, and the <code>srun parameters are controlled via</code>--ntasks-per-node=188<code>and</code>--cpus-per-task=2<code>inherited from the environment, together with</code>OMP_NUM_THREADS=2`.</p> <p>A valid budget may be required, e.g., <pre><code>#SBATCH --account=z00`\n</code></pre></p> <p>A relatively small CP2K job requiring less than a full nude should be run in non-exclusive mode. E.g., A job requiring 18 MPI processes each with two threads might be:</p> Non-exclusive SLURM job submission for CP2K <p><pre><code>#!/bin/bash\n\n#SBATCH --export=none\n#SBATCH --time=00:20:00\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n#SBATCH --ntasks=18\n#SBATCH --cpus-per-task=2\n\n#SBATCH --distribution=block:block\n#SBATCH --hint=nomultithread\n\nmodule load cp2k\n\nexport OMP_NUM_THREADS=2\nexport OMP_PLACES=cores\n\nsrun cp2k.psmp -i H2O-0064.inp\n</code></pre> This job will use a total of 36 cores.</p> <p>See Running Jobs on Cirrus for further general information on SLURM submissions.</p>"},{"location":"software-packages/cp2k/#compiling-and-testing-cp2k-on-cirrus","title":"Compiling and testing CP2K on Cirrus","text":""},{"location":"software-packages/cp2k/#using-spack","title":"Using Spack","text":"<p>If a specific version of CP2K is required, and this is not available as a central module, it is possible to build CP2K using Spack. <pre><code>$ module load spack\n# spack info cp2k\n</code></pre></p> <p>For further information, see</p> <ul> <li>Using Spack on Cirrus</li> <li>Building CP2K using Spack from the CP2K documentation</li> </ul>"},{"location":"software-packages/cp2k/#builds-using-the-toolchain","title":"Builds using the toolchain","text":"<p>The recommendation for developers who wish to build their own version is to use the toolchain build process.</p> <p>Use a starting point which is <code>PrgEnv-gnu</code>, and use MKL for linear algebra. This involves unloading the the default <code>cray-libsci</code> module and loading MKL, e.g., <pre><code>module load PrgEnv-gnu\nmodule load cray-python\nmodule load cray-fftw\n\nmodule unload cray-libsci\nexport MKLROOT=/opt/intel/oneapi/mkl/2025.0\n\nsource ${MKLROOT}/env/vars.sh\n</code></pre> One should then be able to run the toochain build via, e.g., <pre><code>./install_cp2k_toolchain.sh --enable-cray --with-mkl=system\n</code></pre></p>"},{"location":"software-packages/cp2k/#testing","title":"Testing","text":"<p>The entire regression test suite can be run using, schematically, <pre><code>CP2K_DIR=$(pwd)/cp2k\n\nsource ${CP2K_DIR}/tools/toolchain/install/setup\n\nexport OMP_NUM_THREADS=2\nexport OMP_PLACES=cores\n\n${CP2K_DIR}/tests/do_regtest.py \\\n        --workbasedir=$(pwd) \\\n        --maxtasks=36 \\\n        --mpiranks=2 \\\n        --ompthreads=${OMP_NUM_THREADS} \\\n        --mpiexec=\"srun --ntasks=2 --cpus-per-task=${OMP_NUM_THREADS}\" \\\n        ${CP2K_DIR}/exe/local psmp\n</code></pre> All tests should pass. One should see, e.g., <pre><code>------------------------------- Summary --------------------------------\nNumber of FAILED  tests 0\nNumber of WRONG   tests 0\nNumber of CORRECT tests 4261\nTotal number of   tests 4261\n\nSummary: correct: 4261 / 4261; 11min\nStatus: OK\n</code></pre></p>"},{"location":"software-packages/gromacs/","title":"GROMACS","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"software-packages/gromacs/#useful-links","title":"Useful Links","text":"<ul> <li>GROMACS User Guides</li> <li>GROMACS Tutorials</li> </ul>"},{"location":"software-packages/gromacs/#using-gromacs-on-cirrus","title":"Using GROMACS on Cirrus","text":"<p>GROMACS is Open Source software and is freely available to all Cirrus users. The central installation supports the single-precision version of GROMACS compiled with MPI and OpenMP support.</p> <p>The <code>gmx_mpi</code> binary is available after loading a <code>gromacs</code> module.</p>"},{"location":"software-packages/gromacs/#running-parallel-gromacs-jobs","title":"Running parallel GROMACS jobs","text":"<p>GROMACS can use full nodes in parallel (with the <code>--exclusive</code> option to <code>sbatch</code>) or run in parallel (or even serial) on a subset of the  cores on a node. GROMACS can make use of both distributed memory parallelism (via MPI) and shared memory parallelism via OpenMP.</p>"},{"location":"software-packages/gromacs/#example-pure-mpi-using-multiple-nodes","title":"Example: pure MPI using multiple nodes","text":"<p>GROMACS can exploit multiple nodes on Cirrus.</p> <p>For example, the following script will run a GROMACS MD job using 2 nodes (576 cores) with pure MPI.</p> <pre><code>#!/bin/bash --login\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=gmx_test\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=288\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0:25:0\n# Make sure you are not sharing nodes with other users\n#SBATCH --exclusive\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load GROMACS module\nmodule load gromacs\n\nexport OMP_NUM_THREADS=1 \nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run using input in test_calc.tpr\nsrun --hint=nomultithread --distribution=block:block gmx_mpi mdrun -s test_calc.tpr\n</code></pre>"},{"location":"software-packages/gromacs/#example-hybrid-mpiopenmp-across-multiple-nodes","title":"Example: hybrid MPI/OpenMP across multiple nodes","text":"<p>The following script will run a GROMACS MD job using 2 nodes (576 cores) with 24 MPI processes per node (48 MPI processes in total), one per CCD and 12 OpenMP threads per MPI process.</p> <pre><code>#!/bin/bash --login\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=gmx_test\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=24\n#SBATCH --cpus-per-task=12\n#SBATCH --time=0:25:0\n# Make sure you are not sharing nodes with other users\n#SBATCH --exclusive\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load GROMACS module\nmodule load gromacs\n\n# Propagate --cpus-per-task to srun\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\n\n# Run using input in test_calc.tpr\nsrun --hint=nomultithread --distribution=block:block gmx_mpi mdrun -s test_calc.tpr\n</code></pre>"},{"location":"software-packages/gromacs/#example-pure-mpi-using-a-subset-of-a-node","title":"Example: pure MPI using a subset of a node","text":"<p>GROMACS can run on a subset of cores in a node (potentially sharing a  node with other users)</p> <p>For example, the following script will run a GROMACS MD job using 36 cores ona single node with pure MPI.</p> <pre><code>#!/bin/bash --login\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=gmx_test\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=36\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0:25:0\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load GROMACS module\nmodule load gromacs\n\nexport OMP_NUM_THREADS=1 \nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run using input in test_calc.tpr\nsrun --hint=nomultithread --distribution=block:block gmx_mpi mdrun -s test_calc.tpr\n</code></pre>"},{"location":"software-packages/helyx/","title":"HELYX\u00ae","text":"<p>HELYX is a comprehensive, general-purpose, computational fluid dynamics (CFD) software package for engineering analysis and design optimisation developed by ENGYS. The package features an advanced open-source CFD simulation engine and a client-server GUI to provide a flexible and cost-effective HPC solver platform for enterprise applications.</p>"},{"location":"software-packages/helyx/#useful-links","title":"Useful Links","text":"<ul> <li>Information about HELYX</li> <li>Information about ENGYS</li> </ul>"},{"location":"software-packages/helyx/#using-helyx-on-cirrus","title":"Using HELYX on Cirrus","text":"<p>HELYX is only available on Cirrus to authorised users with a valid license to use the software. For any queries regarding HELYX on Cirrus, please contact ENGYS or the Cirrus Helpdesk.</p> <p>HELYX applications can be run on Cirrus in two ways:</p> <ul> <li>Manually from the command line, using a SSH terminal to access the   cluster\u2019s master node.</li> <li>Interactively from within the HELYX GUI, using the dedicated   client-server node to connect remotely to the cluster.</li> </ul> <p>A complete user\u2019s guide to access HELYX on demand via Cirrus is provided by ENGYS as part of this service.</p>"},{"location":"software-packages/helyx/#running-helyx-jobs-in-parallel","title":"Running HELYX Jobs in Parallel","text":"<p>The standard execution of HELYX applications on Cirrus is handled through the command line using a submission script to control Slurm. A basic submission script for running multiple HELYX applications in parallel using the SGI-MPT (Message Passing Toolkit) module is included below. In this example the applications <code>helyxHexMesh</code>, <code>caseSetup</code> and <code>helyxSolve</code> are run sequentially using 4 nodes (144 cores).</p> <pre><code>#!/bin/bash --login\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=Test\n#SBATCH --time=1:00:00\n#SBATCH --exclusive\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=36\n#SBATCH --cpus-per-task=1\n#SBATCH --output=test.out\n\n# Replace [budget code] below with your budget code (e.g. t01)\n#SBATCH --account=t01\n\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=standard\n\n# Replace [QoS name] below with your QoS name (e.g. commercial)\n#SBATCH --qos=commercial\n\n# Load any required modules\nmodule load gcc\nmodule load mpt\n\n# Load the HELYX-Core environment v3.5.0 (select version as needed, e.g. 3.5.0)\nsource /scratch/sw/helyx/v3.5.0/CORE/HELYXcore-3.5.0/platforms/activeBuild.shrc\n\n# Set the number of threads to 1\nexport OMP_NUM_THREADS=1\n\n# Launch HELYX applications in parallel\nexport myoptions=\"-parallel\"\njobs=\"helyxHexMesh caseSetup helyxSolve\"\n\nfor job in `echo $jobs`\ndo\n\n   case \"$job\" in\n    *                )   options=\"$myoptions\" ;;\n   esac\n\n   srun $job $myoptions 2&gt;&amp;1 | tee log/$job.$SLURM_JOB_ID.out\n\ndone\n</code></pre> <p>Alternatively, the user can execute most HELYX applications on Cirrus interactively via the GUI by following these simple steps:</p> <ol> <li>Launch HELYX GUI in your local Windows or Linux machine.</li> <li>Create a client-server connection to Cirrus using the dedicated node     provided for this service in the GUI. Enter your Cirrus user login     details and the total number of processors to be employed in the     cluster for parallel execution.</li> <li>Use the GUI in the local machine to access the remote file system in     Cirrus to load a geometry, create a computational grid, set up a     simulation, solve the flow, and post-process the results using the     HPC resources available in the cluster. The Slurm scheduling     associated with every HELYX job is handled automatically by the     client-server.</li> <li>Visualise the remote data from your local machine, perform changes     to the model and complete as many flow simulations in Cirrus as     required, all interactively from within the GUI.</li> <li>Disconnect the client-server at any point during execution, leave a     utility or solver running in the cluster, and resume the connection     to Cirrus from another client machine to reload an existing case in     the GUI when needed.</li> </ol>"},{"location":"software-packages/lammps/","title":"LAMMPS","text":"<p>LAMMPS (large-scale atomic/molecular massively parallel simulator) is a classical molecular dynamics code developed by Sandia Laboratories in the United States. LAMMPS includes potentials for solid-state materials (metals, semiconductors), soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic scale, mesoscale, or continuum scale.</p>"},{"location":"software-packages/lammps/#useful-links","title":"Useful Links","text":"<ul> <li>LAMMPS Home Page</li> <li>LAMMPS Documentation</li> </ul>"},{"location":"software-packages/lammps/#using-lammps-on-cirrus","title":"Using LAMMPS on Cirrus","text":"<p>LAMMPS is Open Source software, and is freely available to all Cirrus users. Centrally installed versions are managed by Spack on Cirrus.</p> <p>To see what versions are available in the current programming environment:</p> <p><pre><code>  $ module avail lammps\n...\n   lammps/20250612\n</code></pre> indicating the release version of 12th June 2025 is available. Centrally installed versions are available in <code>PrgEnv-cray</code> and <code>PrgEnv-gnu</code>.</p>"},{"location":"software-packages/lammps/#optional-lammps-packages","title":"Optional LAMMPS packages","text":"<p>The centrally installed module versions of LAMMPS have a limited standand set of packages compiled. For the full configuration, try <pre><code>$ module load lammps\n$ lmp -h\n...\nInstalled packages:\n\nKSPACE MANYBODY MOLECULE RIGID\n...\n</code></pre> a list which includes available pair, bond, angle, etc, styles, and fix and compute styles (which is omitted here for brevity).</p>"},{"location":"software-packages/lammps/#running-parallel-lammps-jobs-mpi","title":"Running parallel LAMMPS jobs (MPI)","text":"<p>LAMMPS scales well for appropriate problem sizes and can make use of more than one node. For example, the following script will run a LAMMPS job using 2 nodes (576 cores) with MPI in the Cray programming environment.</p> Exclusive SLURM job submission script for LAMMPS <p><pre><code>#!/bin/bash\n\n#SBATCH --export=none\n#SBATCH --time=00:20:00\n\n#SBATCH --nodes=2\n#SBATCH --exclusive\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n#SBATCH --distribution=block:block\n#SBATCH --hint=nomultithread\n\nmodule load PrgEnv-cray\nmodule load lammps\n\nsrun --ntasks=576 --ntasks-per-node=288 --cpus-per-task=1 lmp &lt; in.test\n</code></pre> Here, SLURM is able to allocate resources by knowing 2 complete (exclusive) nodes are required. A relevant budget code may needed in the above script: <pre><code>#SBATCH --account=budget-code\n</code></pre> where an appropriate <code>budget-code</code> is needed.</p>"},{"location":"software-packages/lammps/#non-exclusive-jobs","title":"Non-exclusive jobs","text":"<p>Smaller problem sizes, requiring less than 288 cores (a full node), may be run in non-exclusive mode. Such a job might require only 36 MPI tasks.</p> Non-exclusive SLURM submission script for LAMMPS <p><pre><code>#!/bin/bash\n\n#SBATCH --export=none\n#SBATCH --time=00:20:00\n\n#SBATCH --ntasks=36\n#SBATCH --cpus-per-task=1\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n#SBATCH --distribution=block:block\n#SBATCH --hint=nomultithread\n\nmodule load PrgEnv-cray\nmodule load lammps\n\nsrun lmp &lt; in.test\n</code></pre> Here, SLURM is able to allocate resources by knowning that 36 tasks are required, and each task requires 1 core (<code>--cpus-per-task=1</code>). Again, a valid budget code may be required (see the previous example).</p>"},{"location":"software-packages/lammps/#compiling-lammps-on-cirrus","title":"Compiling LAMMPS on Cirrus","text":"<p>LAMMPS supports a significant number of optional standard packages, and also provides a further large selection of unsupported (\"USER\") packages. If one or more of these are required, and not provided by the central installation, a separation compilation will be required.</p>"},{"location":"software-packages/lammps/#using-spack","title":"Using Spack","text":"<p>LAMMPS may be installed using Spack. For information on availability: <pre><code>$ module load spack\n$ spack info lammps\n</code></pre> See using Spack on Cirrus for further information on Spack.</p>"},{"location":"software-packages/lammps/#using-cmake","title":"Using CMake","text":"<p>LAMMPS offers developers a relatively simple and robust build mechanism using CMake.</p> <p>A standard LAMMPS CMake configuration for \"most\" packages might look like, schematically:</p> <p><pre><code>module load PrgEnv-cray\nmodule load cray-fftw\nmodule load cray-python\n\ncmake -C ../cmake/presets/most.cmake                                       \\\n      -D BUILD_MPI=on                                                      \\\n      -D BUILD_SHARED_LIBS=yes                                             \\\n      -D CMAKE_CXX_COMPILER=CC                                             \\\n      -D CMAKE_CXX_FLAGS=\"-O2\"                                             \\\n      -D CMAKE_Fortran_COMPILER=ftn                                        \\\n      -D CMAKE_INSTALL_PREFIX=${prefix}                                    \\\n      ../cmake/\n</code></pre> where the <code>${prefix}</code> environment variable is used to specify the location of the installation. See Build LAMMPS with CMake for further information.</p>"},{"location":"software-packages/openfoam/","title":"OpenFOAM","text":"<p>OpenFOAM is an open-source toolbox for computational fluid dynamics. OpenFOAM consists of generic tools to simulate complex physics for a variety of fields of interest, from fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics, electromagnetism and the pricing of financial options.</p> <p>The core technology of OpenFOAM is a flexible set of modules written in C++. These are used to build solvers and utilities to perform pre- and post-processing tasks ranging from simple data manipulation to visualisation and mesh processing.</p>"},{"location":"software-packages/openfoam/#useful-links","title":"Useful links","text":"<p>OpenFOAM comes in two main flavours. The two main releases are:</p> <ul> <li>The OpenFOAM Foundation (openfoam.org)</li> <li>OpenFOAM(R) (openfoam.com)</li> </ul>"},{"location":"software-packages/openfoam/#using-openfoam-on-cirrus","title":"Using OpenFOAM on Cirrus","text":"<p>Centrally installed versions of OpenFOAM are available on Cirrus. The central installations are managed by Spack and can seen: <pre><code>$ module avail openfoam\n...\n  openfoam-org/12    openfoam/2412\n</code></pre> Modules named <code>openfoam-org</code> are related to The OpenFOAM Foundation, whose versions have a numbering convention 11, 12, 13, etc, with a new major version released once per year. Modules named <code>openfoam</code> relate to OpenFOAM(R) and have a version convention YYMM so, e.g., 2412 was released in December 2024 (there are usually two releases per year in June and December).</p> <p>The central versions are managed with Spack and are compiled in <code>PrgEnv-gnu</code>.</p>"},{"location":"software-packages/openfoam/#running-openfoam-jobs-mpi","title":"Running OpenFOAM jobs (MPI)","text":"<p>While limited serial work (such as <code>blockMesh</code> and <code>decomposePar</code>) may be run on the front end, parallel simulations should be submitted to SLURM.</p> <p>Any SLURM script which intends to use OpenFOAM should first load the appropriate OpenFOAM module. The module will automatically set the relevant environment variables such as <code>FOAM_TUTORIALS</code> (see also the note below on <code>WM_PROJECT_USER_DIR</code>).</p> <p>You should be able to use OpenFOAM in the usual way.</p>"},{"location":"software-packages/openfoam/#example-slurm-submissions","title":"Example SLURM submissions","text":"<p>Larger OpenFOAM jobs should use an exclusive submission, e.g., for 2 nodes running 288 MPI processes per node:</p> Exclusive SLURM job submission script for OpenFOAM (openfoam.com) <p><pre><code>#!/bin/bash\n\n#SBATCH --export=none\n#SBATCH --time=00:20:00\n\n#SBATCH --nodes=2\n#SBATCH --exclusive\n#SBATCH --ntasks-per-node=288\n#SBATCH --cpus-per-task=1\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n#SBATCH --distribution=block:block\n#SBATCH --hint=nomultithread\n\nmodule load PrgEnv-gnu\nmodule load openfoam/2412\n\n# We should now have access to OpenFOAM executables\n\nsrun --ntasks=576 icoFoam -parallel -fileHandler collated\n</code></pre> In this example, SLURM is able to allocate the appropriate resources by knowing that 2 nodes are required on an exclusive basis. The number of MPI processes per node is set via <code>--ntasks-per-node=228</code> and <code>--cpus-per-task=1</code> indicates that one core be used for each process.</p> <p>A relevant budget code may needed in the above script: <pre><code>#SBATCH --account=budget-code\n</code></pre> where an appropriate <code>budget-code</code> is needed.</p> <p>Smaller OpenFOAM jobs (fewer than 288 cores) may wish to use a non-exclusive submission:</p> None-exclusive SLURM job submission script for OpenFOAM (openfoam.org) <p><pre><code>#!/bin/bash\n\n#SBATCH --export=none\n#SBATCH --time=00:20:00\n\n#SBATCH --ntasks=36\n#SBATCH --cpus-per-task=1\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n#SBATCH --distribution=block:block\n#SBATCH --hint=nomultithread\n\nmodule load PrgEnv-gnu\nmodule load openfoam-org/12\n\n# We should now have access to OpenFOAM executables\n\nsrun icoFoam -parallel -fileHandler collated\n</code></pre> In this example, SLURM is able to allocate appropriate resouves from the combination <code>--ntasks=36</code> and <code>--cpus-per-task=1</code>, ie., 36 cores.</p> <p>A relevant budget code may needed in the above script: <pre><code>#SBATCH --account=budget-code\n</code></pre> where an appropriate <code>budget-code</code> is needed.</p> <p>See Running Jobs on Cirrus for general information on SLURM submissions.</p>"},{"location":"software-packages/openfoam/#efficient-file-handling-for-larger-openfoam-jobs","title":"Efficient file handling for larger OpenFOAM jobs","text":"<p>By default, OpenFOAM will tend to read and write one file per MPI process. As the number of MPI processes becomes large, this can lead to a very large number of small files. The can put due pressure on the Lustre file system, which favours smaller numbers of larger files. Worse, this can cause contention and slow-dowms for all users accessing files on the file system.</p> <p>At a minimum, please consider using collated file operations when running larger parallel jobs. This should give better I/O performance for all jobs.</p> <p>See the relevant user guide for running applications in parallel:</p> <ul> <li>Running applications in parallel in OpenFOAM 13.</li> <li>Section on Parallism for OpenFoam 2312.</li> </ul>"},{"location":"software-packages/openfoam/#compiling-openfoam-on-cirrus","title":"Compiling OpenFOAM on Cirrus","text":""},{"location":"software-packages/openfoam/#general-comments","title":"General comments","text":"<p>Many packages extend the central OpenFOAM functionality in some way. However, there is no completely standardised way in which this works. Some packages assume they have write access to the main OpenFOAM installation. If this is the case, you must install your own version before continuing. This can be done on an individual basis, or a per-project basis using the project shared directories.</p> <p>Some packages are installed in the OpenFOAM user directory, by default this may be set to, e.g., <pre><code>${HOME}/OpenFOAM/${USER}\n</code></pre> Because user home directories are not available on the back end, this must be changed to a location in <code>\\work</code> before any useful simulations can carried out.</p> <p>If an extension to a central module is envisaged, set e.g., <pre><code>export WM_PROJECT_USER_DIR=${HOME/home/work}/OpenFOAM/${USER}\n</code></pre> after the relevant OpenFOAM module has been loaded.</p>"},{"location":"software-packages/openfoam/#using-spack","title":"Using Spack","text":"<p>OpenFOAM versions may be installed via Spack. For information on availability: <pre><code>$ module load spack\n$ spack info openfoam\n$ spack info openfoam-org\n</code></pre></p> <p>See using Spack on Cirrus for further information on Spack.</p>"},{"location":"software-packages/qe/","title":"Quantum Espresso (QE)","text":"<p>Quantum Espresso is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.</p>"},{"location":"software-packages/qe/#useful-links","title":"Useful Links","text":"<ul> <li>QE User Guides</li> <li>QE Tutorials</li> </ul>"},{"location":"software-packages/qe/#using-qe-on-cirrus","title":"Using QE on Cirrus","text":"<p>QE is Open Source software and is freely available to all Cirrus users.</p>"},{"location":"software-packages/qe/#running-parallel-qe-jobs","title":"Running parallel QE jobs","text":"<p>Quantum Espresso can exploit multiple nodes on Cirrus or can be run on a subset of cores on a node.</p> <p>Note</p> <p>You must load the <code>PrgEnv-gnu</code> module before the <code>quantum-espresso</code> module is available to load.</p>"},{"location":"software-packages/qe/#example-multi-core-qe-job","title":"Example: multi-core QE job","text":"<p>For example, the following script will run a QE job using 36 cores on a single  node (which may be shared with other users/jobs).</p> <pre><code>#!/bin/bash\n#\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=pw_test\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=36\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0:20:0\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\nmodule load PrgEnv-gnu\nmodule load quantum-espresso\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=SBATCH_CPUS_PER_TASK\n\n# Run using input in test_calc.in\nsrun --hint=nomultithread --distribution=block:block pw.x -i test_cals.in\n</code></pre>"},{"location":"software-packages/qe/#example-multi-node-qe-job","title":"Example: multi-node QE job","text":"<p>For example, the following script will run a QE job using 2 nodes (576 cores).</p> <pre><code>#!/bin/bash\n#\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=pw_test\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=288\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0:20:0\n# Make sure you are not sharing nodes with other users\n#SBATCH --exclusive\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\nmodule load PrgEnv-gnu\nmodule load quantum-espresso\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=SBATCH_CPUS_PER_TASK\n\n# Run using input in test_calc.in\nsrun --hint=nomultithread --distribution=block:block pw.x -i test_cals.in\n</code></pre>"},{"location":"software-packages/vasp/","title":"VASP","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.</p> <p>VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory (DFT), solving the Kohn-Sham equations, or within the Hartree-Fock (HF) approximation, solving the Roothaan equations. Hybrid functionals that mix the Hartree-Fock approach with density functional theory are implemented as well. Furthermore, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset) are available in VASP.</p> <p>In VASP, central quantities, like the one-electron orbitals, the electronic charge density, and the local potential are expressed in plane wave basis sets. The interactions between the electrons and ions are described using norm-conserving or ultrasoft pseudopotentials, or the projector-augmented-wave method.</p> <p>To determine the electronic groundstate, VASP makes use of efficient iterative matrix diagonalisation techniques, like the residual minimisation method with direct inversion of the iterative subspace (RMM-DIIS) or blocked Davidson algorithms. These are coupled to highly efficient Broyden and Pulay density mixing schemes to speed up the self-consistency cycle.</p>"},{"location":"software-packages/vasp/#useful-links","title":"Useful Links","text":"<ul> <li>VASP Manual</li> <li>VASP   Licensing</li> </ul>"},{"location":"software-packages/vasp/#using-vasp-on-cirrus","title":"Using VASP on Cirrus","text":"<p>VASP is only available to users who have a valid VASP licence. Only  VASP 6 is available on Cirrus and requests for access need to be made via SAFE.</p> <p>If you VASP 6 licence and wish to have access to VASP 6 on Cirrus please request access through SAFE:</p> <ul> <li>How to request access to package   groups</li> </ul> <p>Once your access has been enabled, you access the VASP software using the <code>vasp</code> modules in your job submission script. You can see which versions of VASP are currently available on Cirrus with</p> <pre><code>module avail vasp\n</code></pre> <p>Once loaded, the executables are called:</p> <ul> <li>vasp_std - Multiple k-point version</li> <li>vasp_gam - GAMMA-point only version</li> <li>vasp_ncl - Non-collinear version</li> </ul> <p>All executables include:</p> <ul> <li>libBEEF functionality</li> <li>LibXC functionality</li> <li>Wannier90 support</li> <li>HDF5 support</li> <li>Additional MD algorithms accessed via the <code>MDALGO</code> keyword.</li> </ul>"},{"location":"software-packages/vasp/#running-parallel-vasp-jobs","title":"Running parallel VASP jobs","text":"<p>Tip</p> <p>If you are running \u0393-point calculations, the <code>vasp_gam</code> executable typically runs around 50% faster than <code>vasp_std</code>.</p>"},{"location":"software-packages/vasp/#smaller-than-single-node-vasp-jobs","title":"Smaller than single node VASP jobs","text":"<p>When running on less than a node you omit the <code>--exclusive</code> flag from your job submission scripts. </p> <p>The following example runs a 36-core VASP calculation on a single  node on Cirrus:</p> <pre><code>#!/bin/bash\n\n# job options (name, compute nodes, job time)\n#SBATCH --job-name=VASP\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=36\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0:20:0\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load VASP version 6 module\nmodule load vasp/6\n\n# Set number of OpenMP threads to 1\nexport OMP_NUM_THREADS=1\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run standard VASP executable\n#   --hint=nomultithread - ensures VASP uses physical cores\n#   --distribution=block:block - ensure best placement of MPI tasks for \n#       collective comms performance\nsrun --hint=nomultithread --distribution=block:block vasp_std\n</code></pre>"},{"location":"software-packages/vasp/#multi-node-vasp-jobs","title":"Multi-node VASP jobs","text":"<p>VASP can exploit multiple nodes on Cirrus.</p> <p>Tip</p> <p>If you are running multi-node VASP jobs you will often need to use half the cores on a node or less to achieve good performance due to memory/interconnect contention from the high number of  cores per node.</p> <p>The following script will run a VASP job using 2 nodes with half the  cores on each node (144 cores) being used for VASP MPI processes. This gives 288 cores in total for this VASP job</p> <pre><code>#!/bin/bash\n\n# job options (name, compute nodes, job time)\n#SBATCH --job-name=VASP\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=144\n#SBATCH --cpus-per-task=2\n#SBATCH --exclusive\n#SBATCH --time=0:20:0\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# Replace [partition name] below with your partition name (e.g. standard)\n#SBATCH --partition=[partition name]\n# Replace [qos name] below with your qos name (e.g. standard,long)\n#SBATCH --qos=[qos name]\n\n# Load VASP version 6 module\nmodule load vasp/6\n\n# Set number of OpenMP threads to 1\nexport OMP_NUM_THREADS=1\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Run standard VASP executable\n#   --hint=nomultithread - ensures VASP uses physical cores\n#   --distribution=block:block - ensure best placement of MPI tasks for \n#       collective comms performance\nsrun --hint=nomultithread --distribution=block:block vasp_std\n</code></pre>"},{"location":"software-tools/forge/","title":"Linaro Forge","text":"<p>Linaro Forge provides debugging and profiling tools for MPI parallel applications, and OpenMP or pthreads multi-threaded applications (and also hydrid MPI/OpenMP). Forge DDT is the debugger and MAP is the profiler.</p>"},{"location":"software-tools/forge/#user-interface","title":"User interface","text":"<p>There are two ways of running the Forge user interface. If you have a good internet connection to Cirrus, the GUI can be run on the front-end (with an X-connection). Alternatively, one can download a copy of the Forge remote client to your laptop or desktop, and run it locally. The remote client is recommended.</p> <p>To download the remote client, see the Forge download pages. You will need the latest version of the client to be consistent with the version of Forge installed on Cirrus. The remote client is discussed further in Connecting with the remote client.</p>"},{"location":"software-tools/forge/#one-time-set-up-for-using-forge","title":"One time set-up for using Forge","text":"<p>A preliminary step is required to set up the necessary Forge configuration files that allow DDT and MAP to initialise its environment correctly so that it can, for example, interact with the Slurm queue system. These steps should be performed in the <code>/work</code> file system on Cirrus.</p> <p>It is recommended that these commands are performed in the top-level work file system directory for the user account, i.e., <code>${HOME/home/work}</code>.</p> <pre><code>module load forge\ncd ${HOME/home/work}\nsource ${FORGE_ROOT}/config-init\n</code></pre> <p>Running the <code>source</code> command will create a directory <code>${HOME/home/work}/.forge</code> that contains the following files.</p> <pre><code>system.config  user.config\n</code></pre> <p>Within the <code>system.config</code> file you should find that <code>shared directory</code> is set to the equivalent of <code>${HOME/home/work/.forge}</code>. That directory will also store other relevant files when Forge is run.</p>"},{"location":"software-tools/forge/#using-ddt","title":"Using DDT","text":"<p>DDT (Distributed Debugging Tool) provides an easy-to-use graphical interface for source-level debugging of compiled C/C++ or Fortran codes. It can be used for non-interactive debugging, and there is also some limited support for python debugging.</p>"},{"location":"software-tools/forge/#preparation","title":"Preparation","text":"<p>To prepare your program for debugging, compile and link in the normal way but remember to include the <code>-g</code> compiler option to retain symbolic information in the executable. For some programs, it may be necessary to reduce the optimisation to <code>-O0</code> to obtain full and consistent information. However, this in itself can change the behaviour of bugs, so some experimentation may be necessary.</p>"},{"location":"software-tools/forge/#post-mortem-or-offline-debugging","title":"Post-mortem, or offline, debugging","text":"<p>A non-interactive method of debugging is available which allows information to be obtained on the state of the execution at the point of failure in a batch job.</p> <p>Such a job can be submitted to the batch system in the usual way. The relevant command to start the executable is as follows.</p> <pre><code># ... Slurm batch commands as usual ...\n\nmodule load forge\n\nexport OMP_NUM_THREADS=12\nexport OMP_PLACES=cores\n\nddt --verbose --offline --mpi=slurm --np 12 \\\n    --mem-debug=fast --check-bounds=before \\\n    ./my_executable\n</code></pre> <p>The parallel launch is delegated to <code>ddt</code> and the <code>--mpi=slurm</code> option indicates to <code>ddt</code> that the relevant queue system is Slurm (there is no explicit <code>srun</code>). It will also be necessary to state explicitly to <code>ddt</code> the number of processes required (here <code>--np 12</code>). For other options see, e.g., <code>ddt --help</code>.</p> <p>Note that higher levels of memory debugging can result in extremely slow execution. The example given above uses the default <code>--mem-debug=fast</code> which should be a reasonable first choice.</p> <p>Execution will produce a <code>.html</code> format report which can be used to examine the state of execution at the point of failure.</p>"},{"location":"software-tools/forge/#memory-debugging","title":"Memory debugging","text":"<p>If you are dynamically linking your code and debugging it on the login node then this is fine (just ensure that the Preload the memory debugging library option is ticked in the Details pane.) If you are dynamically linking but intending to debug running on the compute nodes, or statically linking then you need to include the compile option <code>-Wl,--allow-multiple-definition</code> and explicitly link your executable with Forge's memory debugging library. The exactly library to link against depends on your code; <code>-ldmalloc</code> (for no threading with C), <code>-ldmallocth</code> (for threading with C), <code>-ldmallocxx</code> (for no threading with C++) or <code>-ldmallocthcxx</code> (for threading with C++). The library locations are all set up when the <code>forge</code> module is loaded so these libraries should be found without further arguments.</p>"},{"location":"software-tools/forge/#interactive-debugging-using-the-client-to-submit-a-batch-job","title":"Interactive debugging: using the client to submit a batch job","text":"<p>You can also start the client interactively (for details of remote launch, see Connecting with the remote client). ` <pre><code>module load forge\nddt\n</code></pre></p> <p>This should start a window as shown below. Click on the DDT panel on the left, and then on the Run and debug a program option. This will bring up the Run dialogue as shown.</p> <p>Note:</p> <ul> <li> <p>One can start either DDT or MAP by clicking the appropriate panel on the left-hand side;</p> </li> <li> <p>If the license has connected successfully, a serial number will be shown in small text at the lower left (see image below). One can click on the question mark icon next to the license serial number to see current information on the status of the license (number of processes available and so on).</p> </li> </ul> <p></p> <p>In the Application sub panel of the Run dialog box, details of the executable, command line arguments or data files, the working directory and so on should be entered.</p> <p>Click the MPI checkbox and specify the MPI implementation. This is done by clicking the Details button and then the Change button. Choose the SLURM (generic) implementation from the drop-down menu and click OK. You can then specify the required number of nodes/processes and so on.</p> <p>Click the OpenMP checkbox and select the relevant number of threads (if there is no OpenMP in the application itself, select 1 thread).</p> <p>Click the Submit to Queue checkbox and then the associated Configure button. A new set of options will appear such as Submission template file, where you can enter <code>${FORGE_ROOT}/templates/cirrus.qtf</code> and click OK. This template file provides many of the options required for a standard batch job. You will then need to click on the Queue Parameters button in the same section and specify the relevant project budget, see the Account entry.</p> <p>The default queue template file configuration uses a QoS which must be entered in the Submit to Queue Parameters dialogue, and a standard time limit of 20 minutes. An account code is also required. The default template is a non-exclusive submission.</p> <p>Alternatively, one can copy the <code>cirrus.qtf</code> template file to a suitable location in your work file space and make the relevant changes. For example, if the application does not use the default programming eniovironment, or requires additional modules to be loaded, module commands should be added in the new queue template file. This new template file can then be specified in the dialog window.</p> <p>There may be a short delay while the sbatch job starts. Debugging should then proceed as described in the Linaro Forge documentation.</p>"},{"location":"software-tools/forge/#using-map","title":"Using MAP","text":"<p>MAP can be used to generate a profile to investigate the performance of a program. The profile is based on sampling, so should not incur a significant overhead. The program can be compiled in the usual way.</p>"},{"location":"software-tools/forge/#generating-a-profile","title":"Generating a profile","text":"<p>Submit a batch job in the usual way, and include a command of the form:</p> <p><pre><code># ... Slurm batch commands as usual ...\n\nmodule load forge\n\nmap -n &lt;number of MPI processes&gt; --mpiargs=\"&lt;srun options&gt;\" --profile ./my_executable\n</code></pre> The <code>-n</code> option sets the number of MPI processes, and the <code>--mpiargs</code> option should include arguments which would usually be passed to <code>srun</code>. For example, one might set <pre><code>export OMP_NUM_THREADS=12\nexport OMP_PLACES=cores\n\nmap -n 24 --mpiargs=\"--cpus-per-task=${OMP_NUM_THREADS}\" --profile ./my_executable\n</code></pre> to run on 24 MPI processes each with 12 OpenMP threads.</p> <p>Successful execution will generate a file with a <code>.map</code> extension.</p> <p>This <code>.map</code> file may be viewed via the MAP GUI (e.g., using the remote client) by selecting the load profile data option. The <code>.map</code> file should be selected in the file dialogue.</p> <p>Text and HTML summaries can also be generated from the <code>.map</code> file via, e.g., <pre><code>module load forge\nperf-report my-application.map\n</code></pre> For further information on MAP see the Linaro documentation.</p>"},{"location":"software-tools/forge/#connecting-with-the-remote-client","title":"Connecting with the remote client","text":"<p>If one starts the Forge client on e.g., a laptop, one should see the main window as shown above. Select Remote Launch and then Configure from the drop-down menu. In the Configure Remote Connections dialog box click Add. The following window should be displayed. Fill in the fields as shown. The Connection Name is just a tag for convenience (useful if a number of different accounts are in use). The Host Name should be as shown with the appropriate username. The Remote Installation Directory should be exactly as shown. The Remote Script is needed to execute additional environment commands on connection. A default script is provided in the location shown.</p> <pre><code>/work/y07/shared/cirrus-ex/cirrus-ex-software/utils/core/forge/latest/remote-init\n</code></pre> <p>Other settings can be as shown. Remember to click OK when done.</p> <p></p> <p>From the Remote Launch menu you should now see the new Connection Name. Select this, and enter the relevant ssh passphase and machine password to connect. A remote connection will allow you to debug, or view a profile, as discussed above.</p> <p>If different commands are required on connection, a copy of the <code>remote-init</code> script can be placed in, e.g., <code>${HOME/home/work}/.forge</code> and edited as necessary. The full path of the new script should then be specified in the remote launch settings dialog box. Note that the script changes the directory to the <code>/work/</code> file system so that batch submissions via <code>sbatch</code> will not be rejected.</p> <p>Finally, note that <code>ssh</code> may need to be configured so that it picks up the correct local public key file. This may be done, e.g., via the local <code>.ssh/config</code> configuration file.</p>"},{"location":"software-tools/forge/#troubleshooting","title":"Troubleshooting","text":"<p>A common cause of problems in the use of the remote client is incorrect Forge configuration in the <code>.forge/system.config</code> file, particularly in the specification of the shared directory. The should be of the form <pre><code>shared directory = /mnt/lustre/e1000/home/project/project/user/.forge\n</code></pre> (and certainly not the home directory <code>~</code>). The full mount point your work directory can be obtained with e.g., <code>pwd -P</code> (somewhat confusingly, <code>/mnt/lustre/e1000/home</code> is <code>/work</code>).</p> <p>If you submit a job to the queue via the remote client, and the job starts (can check using <code>squeue</code> interactively), but the client does not connect, you may need to check this configuration setting.</p> <p>For hybrid applications where thread placement is critical, the remote client does not provide good control of such placement (or any at all). The <code>--offline</code> approach discussed above is one solution.</p>"},{"location":"software-tools/forge/#licensing","title":"Licensing","text":"<p>Cirrus has a licence for up to 2048 tokens, where a token represents an MPI process. Running Forge DDT or MAP for a code running on 2 nodes using 288 MPI ranks per node would require 576 tokens.</p> <p>Forge licence tokens are shared by all Cirrus (and ARCHER2) users.</p> <p>To see how many tokens are currently in use, you can view the license server status page by first setting up an SSH tunnel to the node hosting the licence server:</p> <pre><code>ssh &lt;username&gt;@login.cirrus.ac.uk -L 4241:dvn04:4241\n</code></pre> <p>You should now be able to view the status page from a browser, with the local HTTP address <code>http://localhost:4241/status.html</code>.</p> <p>The licence status page may contain multiple licenses, indicated by a row of buttons (one per licence) near the top of the page. The usual license is <code>Licence 16891</code>.  Additional buttons may appear at various times for boosted licences that offer more tokens. Such licences are primarily for the benefit of ARCHER2 users. Please contact the Service Desk if you have a specific requirement that exceeds the current Forge licence provision.</p>"},{"location":"software-tools/forge/#useful-links","title":"Useful links","text":"<ul> <li>Forge User Guide</li> <li>More information on X-window connections to Cirrus.</li> </ul>"},{"location":"software-tools/intel-vtune/","title":"Intel VTune","text":""},{"location":"software-tools/intel-vtune/#profiling-using-vtune","title":"Profiling using VTune","text":"<p>Intel VTune allows profiling of compiled codes, and is particularly suited to analysing high performance applications involving threads (OpenMP), and MPI (or some combination thereof).</p> <p>Using VTune is a two-stage process. First, an application is compiled using an appropriate Intel compiler and run in a \"collection\" phase. The results are stored to file, and may then be inspected interactively via the VTune GUI.</p>"},{"location":"software-tools/intel-vtune/#collection","title":"Collection","text":"<p>Compile the application in the normal way, and run a batch job in exclusive mode to ensure the node is not shared with other jobs. An example is given below.</p> <p>Collection of performance data is based on a <code>collect</code> option, which defines which set of hardware counters are monitered in a given run. As not all counters are available at the same time, a number of different collections are available. A different one may be relevant if interested in different aspects of performance. Some standard options are:</p> <p><code>vtune -collect=performance-snapshot</code> may be used to product a text summary of performance (typically to standard output), which can be used as a basis for further investigation.</p> <p><code>vtune -collect=hotspots</code> produces a more detailed analysis which can be used to inspect time taken per function and per line of code.</p> <p><code>vtune -collect=hpc-performance</code> may be useful for HPC codes.</p> <p><code>vtune --collect=meory-access</code> will provide figures for memory-related measures including application memory bandwidth.</p> <p>Use <code>vtune --help collect</code> for a full summary of collection options. Note that not all options are available.</p>"},{"location":"software-tools/intel-vtune/#example-slurm-script","title":"Example SLURM script","text":"<p>Here we give an example of profiling an application which has been compiled with Intel 20.4 and requests the <code>memory-access</code> collection. We assume the application involves OpenMP threads, but no MPI.</p> <pre><code>#!/bin/bash\n\n#SBATCH --time=00:10:00\n#SBATCH --nodes=1\n#SBATCH --exclusive\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\nexport OMP_NUM_THREADS=18\n\n# Load relevant (cf. compile-time) Intel options \nmodule load intel-20.4/compilers\nmodule load intel-20.4/vtune\n\nvtune -collect=memory-access -r results-memory ./my_application\n</code></pre> <p>Profiling will generate a certain amount of additional text information; this appears on standard output. Detailed profiling data will be stored in various files in a sub-directory, the name of which can be specified using the <code>-r</code> option.</p> <p>Notes</p> <ul> <li> <p>Older Intel compilers use <code>amplxe-cl</code> instead of <code>vtune</code> as the   command for collection. Some existing features still reflect this   older name. Older versions do not offer the \"performance-snapshot\"   collection option.</p> </li> <li> <p>Extra time should be allowed in the wall clock time limit to allow for   processing of the profiling data by <code>vtune</code> at the end of the run. In   general, a short run of the application (a few minutes at most) should   be tried first.</p> </li> <li> <p>A warning may be issued:</p> <p>amplxe: Warning: Access to /proc/kallsyms file is limited. Consider changing /proc/sys/kernel/kptr_restrict to 0 to enable resolution of OS kernel and kernel modules symbols.</p> <p>This may be safely ignored.</p> </li> <li> <p>A warning may be issued:</p> <p>amplxe: Warning: The specified data limit of 500 MB is reached. Data collection is stopped. amplxe: Collection detached.</p> <p>This can be safely ignored, as a working result will still be obtained. It is possible to increase the limit via the <code>-data-limit</code> option (500 MB is the default). However, larger data files can take an extremely long time to process in the report stage at the end of the run, and so the option is not recommended.</p> </li> <li> <p>For Intel 20.4, the <code>--collect=hostspots</code> option has been observed to   be problematic. We suggest it is not used.</p> </li> </ul>"},{"location":"software-tools/intel-vtune/#profiling-an-mpi-code","title":"Profiling an MPI code","text":"<p>Intel VTune can also be used to profile MPI codes. It is recommended that the relavant Intel MPI module is used for compilation. The following example uses Intel 18 with the older <code>amplxe-cl</code> command:</p> <pre><code>#!/bin/bash\n\n#SBATCH --time=00:10:00\n#SBATCH --nodes=2\n#SBATCH --exclusive\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\nexport OMP_NUM_THREADS=18\n\nmodule load intel-mpi-18\nmodule load intel-compilers-18\nmodule load intel-vtune-18\n\nmpirun -np 4 -ppn 2 amplxe-cl -collect hotspots -r vtune-hotspots \\\n       ./my_application\n</code></pre> <p>Note that the Intel MPI launcher <code>mpirun</code> is used, and this precedes the VTune command. The example runs a total of 4 MPI tasks (<code>-np 4</code>) with two tasks per node (<code>-ppn 2</code>). Each task runs 18 OpenMP threads.</p>"},{"location":"software-tools/intel-vtune/#viewing-the-results","title":"Viewing the results","text":"<p>We recommend that the latest version of the VTune GUI is used to view results; this can be run interactively with an appropriate X connection. The latest version is available via</p> <pre><code>$ module load oneapi\n$ module load vtune/latest\n$ vtune-gui\n</code></pre> <p>From the GUI, navigate to the appropriate results file to load the analysis. Note that the latest version of VTune will be able to read results generated with previous versions of the Intel compilers.</p>"},{"location":"software-tools/likwid/","title":"LIKWID","text":"<p>LIKWID is an open-source tool suite that can be used to measure node-level hardware performance counters, amongst other functionalities. It offers ways to quantify performance, e.g. when investigating performance bottlenecks or degradation. In this documentation we present guidance for common use cases on Cirrus, focusing on performance counter measurement for parallel applications. For more information on LIKWID functionality and usage see the official LIKWID wiki.</p>"},{"location":"software-tools/likwid/#likwid-perfctr-and-likwid-mpirun","title":"likwid-perfctr and likwid-mpirun","text":"<p>LIKWID provides a number of command line tools. Performance counter measurement is handled by <code>likwid-perfctr</code>, which supports the following usage modes:</p> <ul> <li> <p>wrapper   mode   (default): use <code>likwid-perfctr</code> as a wrapper to launch your   application and measure performance counters while it executes. This   is the easiest way to ensure that measurement starts and stops when   your application starts and stops and that the cores whose counters   are included in measurements are those on which your application   executes.</p> <ul> <li>wrapper mode + marker   API:   when using wrapper mode it is possible to measure performance   counters only during execution of one or more specific regions   in your code, for example to quantify the performance of known   computationally costly kernels. This requires instrumenting your   code to use the LIKWID marker   API   and recompiling.</li> </ul> </li> <li> <p>stethoscope   mode:   launch your application as usual, then instruct <code>likwid-perfctr</code> to   measure performance counters associated with the cores on which your   application is executing. Measurements are aggregated over a   duration of time that you specify. It may be difficult to relate   results to what application code was executed over the measurement   duration. This mode is more suited to obtaining a snapshot of   performance than to performing a systematic performance assessment.</p> </li> <li> <p>timeline   mode:   launch your application as usual, then instruct <code>likwid-perfctr</code> to   periodically output performance counter measurements aggregated over   the time interval specified. As for stethoscope mode you must tell   <code>likwid-perfctr</code> which cores to measure counters for, ensuring these   match where your application is executing. This mode can provide   insight into performance during different phases of your   application, though it may be difficult to relate results to what   application code was executed over any given measurement interval.</p> </li> </ul> <p>Using <code>likwid-perfctr</code> in any of the above modes other than wrapper + marker API can be done without altering or recompiling your application code.</p> <p><code>likwid-perfctr</code> is designed to work with serial or thread-parallel applications. Measuring counters for MPI-parallel applications may be accomplished in principle by combining <code>likwid-perfctr</code> and a parallel application launcher such as <code>srun</code> as described in this tutorial. LIKWID provides a more elegant solution in the form of a wrapper called <code>likwid-mpirun</code>. This launches <code>likwid-perfctr</code> and your MPI-parallel application and aggregates measurement outputs across ranks.</p> <p>Note</p> <p><code>likwid-mpirun</code> only supports <code>likwid-perfctr</code>'s wrapper mode (with or without marker API)</p>"},{"location":"software-tools/likwid/#likwid-on-cirrus","title":"LIKWID on Cirrus","text":"<p>For the sake of simplicity and convenience we provide guidance on how to use <code>likwid-mpirun</code> to measure performance counters on Cirrus regardless of whether the application is serial, thread-parallel, MPI-parallel, or hybrid MPI + thread-parallel. This unified approach has the added benefit of being closer to the Cirrus default <code>srun</code>-based application launch approach used in existing job scripts than using <code>likwid-perfctr</code>.</p> <p>Using <code>likwid-mpirun</code> restricts measurement functionality to <code>likwid-perfctr</code>'s wrapper mode, which supports performance characterisation through either whole-application or kernel-specific measurement. Users interested in running <code>likwid-perfctr</code> directly on Cirrus, for example to access timeline or stethoscope mode, may consult the example job script for pure threaded applications that uses <code>likwid-perfctr</code> below as well as the wiki page on LIKWID's pinning syntax, and can contact the Cirrus service desk to request assistance.</p> <p>Note</p> <p>LIKWID on Cirrus uses the perf-event backend with <code>perf_event_paranoid</code> set to -1 (no restrictions), which has some implications for features/functionality</p>"},{"location":"software-tools/likwid/#summary-of-likwid-mpirun-options","title":"Summary of likwid-mpirun options","text":"<p>The following options are important to be aware of when using <code>likwid-mpirun</code> on ARCHER2. For additional information, try <code>likwid-mpirun --help</code> and see the LIKWID wiki, especially the <code>likwid-mpirun</code> page.</p> <p><code>-n/-np &lt;count&gt;</code></p> <p>Specify the total number of processes to launch</p> <p><code>-t &lt;count&gt;</code></p> <p>The number of threads or threads per MPI rank. Defaults to 1 if not specified. Can be used to \"space\" processes for placement in the case of hybrid MPI + threaded applications and act as an alternative to <code>-pin</code> below in these cases. </p> <p><code>-pin &lt;list&gt;</code></p> <p>Specify pinning of processes (and their threads if applicable). Follows the <code>likwid-pin</code> syntax. </p> <p><code>-g/--group &lt;perf&gt;</code></p> <p>Specify which predefined group of performance counters and derived metrics to measure and compute. Details about these groups and available counters for the Zen2 architecture of ARCHER2's AMD EPYC processors can be found at https://github.com/RRZE-HPC/likwid/wiki/Zen2.</p> <p><code>--nocpubind</code> (ARCHER2 only)</p> <p>Suppress <code>likwid-mpirun</code>'s binding of application processes to CPUs (cores) that would otherwise take place through generation of a CPU mask list passed to <code>srun</code>. We recommend always using <code>--nocupbind</code> on ARCHER2 to avoid conflicts with binding/affinity specified following the usual approach on ARCHER2 and this is shown in example job scripts.</p> <p><code>-s/--skip &lt;hex&gt;</code></p> <p>This tells <code>likwid-mpirun</code> how many threads to skip when generating its specification of which cores to pin to/measure (see this discussion of LIKWID and shepherd threads for a detailed explanation). On ARCHER2 we have checked and confirmed that there are no shepherd threads involved using any of <code>PrgEnv-gnu</code>, <code>PrgEnv-cray</code> or <code>PrgEnv-aocc</code>, therefore not skipping any threads (<code>-s 0x0</code>) is the correct choice (this differs from <code>likwid-mpirun</code> defaults), reflected in the example job scripts above.</p> <p><code>-d/--debug</code></p> <p>To check how exactly <code>likwid-mpirun</code> calls <code>srun</code> and <code>likwid-perfctr</code> to launch and measure your application, use the <code>--debug</code> option, which will generate additional output that includes the relevant commands. For the modified <code>likwid-mpirun</code> on ARCHER2 the otherwise temporary files <code>.likwidscript_*.txt</code> referenced in debug output that contain these commands persist after execution, enabling closer inspection.</p> <p><code>--mpiopts</code></p> <p>Any desired options accepted by <code>srun</code> can be passed to the <code>srun</code> command through the <code>--mpiopts</code> option, for example as follows:</p> <p><code>likwid-mpirun --mpiopts \"--exact --hint=nomultithread --distribution=block:block\"</code> </p> <p><code>-m/-marker</code></p> <p>Activate Marker API mode</p>"},{"location":"software-tools/likwid/#example-job-scripts","title":"Example job scripts","text":"<p>Below we provide example job scripts covering different cases of application parallelism using <code>likwid-perfctr</code> in wrapper mode either through <code>likwid-mpirun</code> or directly. All examples perform whole-application measurement but can be adapted to use the marker API.</p> <p>Each of the example job scripts that uses <code>likwid-mpirun</code> makes use of the srun/sbatch options <code>--hint=nomultithread</code> and <code>--distribution=block:block</code>. We have set these as SBATCH options at the top of the job script. Alternatively these as well as any other <code>srun</code> options could be passed explicitly to the underlying <code>srun</code> command through the <code>--mpiopts</code> option as follows:</p> <p><code>likwid-mpirun --mpiopts \"--hint=nomultithread --distribution=block:block\"</code> </p> <p>Each example job script includes a suggested command to run <code>xthi</code> launched identically to the application you wish to measure in order to check and confirm that process and thread placement for your application is as intended. Details on checking process placement with <code>xthi</code> can be found in the User Guide page on Running jobs.</p> <p>Note</p> <p>You are encouraged to check how <code>likwid-mpirun</code> uses <code>srun</code> to launch <code>likwid-perfctr</code> and your application by running in debug mode (<code>likwid-mpirun --debug</code>) and examining both the job output and the <code>.likwidscript-####</code> file mentioned therein.</p> <p>For pure threaded and MPI+thread parallel jobs using either the <code>-t</code> option to specify the number of threads or <code>-pin</code> with an appropriate pinning expression (or both) can accomplish the same desired application placement and measurement scenario. The same applies to pure MPI applications in the case of underpopulating nodes (i.e. fewer than 128 ranks per node), where <code>-t</code> can be used to space processes out across a node and/or <code>-pin</code> used to specify which cores on each node application processes should execute and be measured on.</p> <p>For further explanation of <code>likwid-mpirun</code> options used, see the section Summary of likwid-mpirun options</p>"},{"location":"software-tools/likwid/#pure-mpi-jobs","title":"Pure MPI jobs","text":""},{"location":"software-tools/likwid/#fully-populated-nodes","title":"Fully populated node(s)","text":"<p>Two fully populated nodes (128 ranks per node):</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=128\n#SBATCH --cpus-per-task=1\n#SBATCH --hint=nomultithread\n#SBATCH --distribution=block:block\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=1\nexport SRUN_CPUS_PER_TASK=1\n\nlikwid-mpirun -n $SLURM_NTASKS --nocpubind -s 0x0 -g FLOPS_DP --debug xthi_mpi &amp;&gt; xthi_mpi.out\nlikwid-mpirun -n $SLURM_NTASKS --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre>"},{"location":"software-tools/likwid/#underpopulated-nodes","title":"Underpopulated node(s)","text":"<p>Two nodes, two ranks per node, one rank per socket (i.e. per 64-core AMD EPYC processor):</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --cpus-per-task=64\n#SBATCH --hint=nomultithread\n#SBATCH --distribution=block:block\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=1\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\nlikwid-mpirun -n $SLURM_NTASKS -pin N:0_N:64 --nocpubind -s 0x0 -g FLOPS_DP --debug xthi_mpi &amp;&gt; xthi_mpi.out\nlikwid-mpirun -n $SLURM_NTASKS -pin N:0_N:64 --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre> <p>The same application placement and measurement scenario can be accomplished by specifying the first core on each socket directly with <code>-pin S0:0_S1:0</code> instead of <code>-pin N:0_N:64</code></p> <p>One node, four ranks, one rank per NUMA region all on the same socket:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=4\n#SBATCH --cpus-per-task=16\n#SBATCH --hint=nomultithread\n#SBATCH --distribution=block:block\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=1\nexport SRUN_CPUS_PER_TASK=16\n\nlikwid-mpirun -n $SLURM_NTASKS -pin N:0_N:16_N:32_N:48 --nocpubind -s 0x0 -g FLOPS_DP --debug xthi_mpi &amp;&gt; xthi_mpi.out\nlikwid-mpirun -n $SLURM_NTASKS -pin N:0_N:16_N:32_N:48 --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre> <p>The same application placement and measurement scenario can be accomplished by specifying the first core on each NUMA node directly with <code>-pin M0:0_M1:0_M2:0_M3:0</code> instead of <code>-pin N:0_N:16_N:32_N:48</code></p>"},{"location":"software-tools/likwid/#pure-threaded-jobs","title":"Pure threaded jobs","text":""},{"location":"software-tools/likwid/#fully-populated-node","title":"Fully populated node","text":"<pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=128\n#SBATCH --hint=nomultithread\n#SBATCH --distribution=block:block\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=128\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=128\n\nlikwid-mpirun -n 1 -t 128 --nocpubind -s 0x0 -g FLOPS_DP --debug xthi &amp;&gt; xthi.out\nlikwid-mpirun -n 1 -t 128 --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre> <p>The same application placement and measurement scenario can be accomplished using the pinning option <code>-pin N:0-127</code> instead of <code>-t 128</code>.</p> <p>For pure threaded applications the <code>likwid-perfctr</code> command can also be used directly instead of <code>likwid-mpirun</code>, bypassing <code>srun</code>. This is shown below in the equivalent job script to the fully populated <code>likwid-mpirun</code> example above.</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=1\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=128\nexport OMP_PLACES=cores\n\nlikwid-perfctr -C N:0-127 -s 0x0 -g FLOPS_DP --debug xthi &amp;&gt; xthi.out\nlikwid-perfctr -C N:0-127 -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre> <p>The <code>-C</code> option simultaneously sets pinning of application threads to cores and specifies those same cores to measure counters for. The following pinning expressions are equivalent:</p> <pre><code>-C N:0-127\n-C E:N:128:1:2\n</code></pre> <p>The second form uses LIKWID's expression based pinning syntax (see the <code>likwid-pin</code> wiki page). This can be understood by examining CPU numbering using the <code>likwid-topology</code> command, which shows that adjacent hardware threads on the same physical cores are numbered n and n+128 respectively, hence CPUs (logical cores) numbered 0 through 127 correspond to single hardware threads on each of the 128 distinct physical cores on an ARCHER2 compute node. The final <code>:2</code> in the expression based syntax skips the second hardware thread for each physical core when pinning, thereby accomplishing the same as the domain-based pinning expression that specifies direct core number assignment for the N domain (all cores).</p>"},{"location":"software-tools/likwid/#underpopulated-node","title":"Underpopulated node","text":"<p>Launching fewer than 128 threads placed consecutively is a straightforward variation on the fully occupied node case above. We can achieve more varied placements, for example four threads in total, each bound to the first core of a different CCX complex on the same socket. The 4 cores in a CCX share a common L3 cache, hence this scenario results in none of the threads sharing the same L3 cache.</p> <p>This is easiest to accomplish using <code>likwid-perfctr</code> directly rather than through <code>likwid-mpirun</code>, as follows:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=1\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=4\nexport OMP_PLACES=cores\n\nlikwid-perfctr -C 0,4,8,12 -s 0x0 -g FLOPS_DP xthi &amp;&gt; xthi_perfctr_list.out\n</code></pre> <p>The same application placement and measurement scenario can be accomplished using the pinning option <code>-C C0:0@C1:0@C2:0@C3:0</code> instead. This specifies threads be assigned to the first core of successive last cache level (L3) domains. The expression syntax version <code>-C E:N:4:1:8</code> would achieve the same.</p>"},{"location":"software-tools/likwid/#hybrid-mpithreaded-jobs","title":"Hybrid MPI+threaded jobs","text":"<p>2 fully populated nodes with 2 ranks per node and 64 threads per rank:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --cpus-per-task=64\n#SBATCH --hint=nomultithread\n#SBATCH --distribution=block:block\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=64\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=64\n\nlikwid-mpirun -n $SLURM_NTASKS -t 64 --nocpubind -s 0x0 -g FLOPS_DP --debug xthi &amp;&gt; xthi.out\nlikwid-mpirun -n $SLURM_NTASKS -t 64 --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre> <p>The same application placement and measurement scenario can be accomplished using the pinning option <code>-pin N:0-63_N:64-127</code> instead of <code>-t 64</code>.</p>"},{"location":"software-tools/likwid/#serial-job","title":"Serial job","text":"<p>Using <code>likwid-mpirun</code>:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=1\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=1\nexport SRUN_CPUS_PER_TASK=1\n\nlikwid-mpirun -n 1 --nocpubind -s 0x0 -g FLOPS_DP --debug xthi &amp;&gt; xthi.out\nlikwid-mpirun -n 1 --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre> <p>Alternatively, using <code>likwid-perfctr</code>:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account=[your project]\n#SBATCH --partition=standard\n#SBATCH --qos=short\n#SBATCH --time=00:20:00 \n#SBATCH --nodes=1\n\nmodule load likwid\nmodule load xthi\n\nexport OMP_NUM_THREADS=1\n\nlikwid-perfctr -C 0 --nocpubind -s 0x0 -g FLOPS_DP --debug xthi &amp;&gt; xthi.out\nlikwid-perfctr -C 0 --nocpubind -s 0x0 -g FLOPS_DP myApplication &amp;&gt; application.out\n</code></pre>"},{"location":"software-tools/likwid/#likwid-marker-api-instrumenting-an-application-for-fine-grained-measurement","title":"LIKWID Marker API: instrumenting an application for fine-grained measurement","text":"<p>Another important feature of LIKWID is the ability to measure performance counters for specified regions of your code, such as a computationally intensive  kernel. You can instrument your code using the LIKWID Marker API to instruct  LIKWID when to start and stop taking measurements. This requires code changes and  recompilation to include the <code>likwid-marker.h</code> header and to call required macros or functions  as illustrated below. Moreover, the code must be compiled with <code>-DLIKWID_PERFMON</code>  to turn Marker API on and with reference to a location where the header files  are found (<code>-I $LIKWID_DIR/include</code>). As <code>LD_PRELOAD</code> mechnaism is used, the application must be linked dynamically with reference to likwid library (<code>-L $LIKWID_DIR/lib -llikwid</code>). Markers are recognised by  <code>likwid-perfctr</code> and <code>likwid-mpirun</code> with the <code>-m</code> option enabled. The location of  the headers and libraries can be viewed via <code>module show likwid</code>. The module also sets the value of the<code>$LIKWID_DIR</code> environment variable on loading. It may also  may be necessary to adjust <code>LD_LIBRARY_PATH</code> to include <code>$LIBLIKWID_DIR/lib</code> so that the library is picked up at runtime.</p> <p>The example below demonstrates the use of the C API (a similar API is also  supported for Fortran and some other languages). The initialisation and closing macros or functions must be called from the serial section of the code.</p> <pre><code>... // other includes\n#include &lt;likwid-marker.h&gt;  \n...\nLIKWID_MARKER_INIT;   // macro call to setup measurement system\n...\nLIKWID_MARKER_REGISTER(\"myregion\"); // recommended to reduce overhead\n// if OpenMP is used then this should be called in a parallel region\n// same is true for stop/start calls\n\nLIKWID_MARKER_START(\"myregion\");\n... // code region of interest\nLIKWID_MARKER_STOP(\"myregion\");\n\nLIKWID_MARKER_CLOSE;\n</code></pre> <p>Note that to allow for conditional compilation so that the original uninstrumented  code could be recompiled without <code>-DLIKWID_PERFMON</code>, the macros can be redefined  as empty macros using the <code>#ifndef LIKWID_PERFMON</code> preprocessor directive.  For an example and more detailed discussion please refer to the  Marker API tutorial. For each macro there exists a corresponding function which you can call instead. Additionally, similar APIs are also defined for NVIDIA and AMD GPUs.</p>"},{"location":"software-tools/likwid/#roofline-analysis-using-likwid","title":"Roofline Analysis using LIKWID","text":"<p>The Roofline model allows to determine whether an application or kernel  is memory or compute bound by measuring its performance (FLOPS persecond) and  operational intensity (FLOPS per byte of memory traffic). This allows us to  assess the optimisation potential of the selected application, function or loop.  The Roofline is typically defined by measuring attainable performance of a memory-bound code such as a version of the STREAM benchmark (this would appear as a slanted roof in the Roofline plot).  For the compute-bound ceiling a theoretical maximum or a dense matrix multiplication kernel can be used or another appropriate compute-bound microbenchmark. Multiple Rooflines  can be defined, with no particular order: e.g. RAM vs last-level cache in a cache-aware Roofline for the slanted Rooflines and single vs double precision with or without vector intrinsics such as SSE or AVX could constitute additional horizontal Rooflines. Roofline analysis usually focuses on node-level performance optimisations.</p> <p>Then the performance of an entire application or of a loop can be plotted under those Rooflines. In can be interpreted as follows. If the point is on the left of the vertical line through ridge point where memory and compute rooflines meet, then it is memory-bound, otherwise compute bound. Compute-bound kernels usually exhibit operational intensity substantially higher than 1. The distance to the roofline represents the opportunities for optimisations. If the performance of  a loop is already close to attainable maximum it maybe tricky to find any improvement for it The Roofline model tells us that something may or may not be worth optimising but it does  not suggest a specific optimisation. </p> <p>As described above, Marker API can be used to define regions of interest such as a hot loop where most compute time is spent, identified via an initial profiling run. Additionally <code>likwid-bench</code> can be used to get empirical peak memory and compute bounds by using a version of stream load benchmark and a peakflops kernels. </p> <p>Results from <code>likwid-bench</code> can be used by running the appropriate microbenchmark  (add those lines below to the submissiomn script assuming a single-node run):</p> <pre><code>likwid-bench -t peakflops_avx_fma -W N:4GB:128:1:2 &amp;&gt; peak_flops.out\nlikwid-bench -t load_avx -W N:4GB:128:1:2          &amp;&gt; peak_bw.out\n</code></pre>"},{"location":"software-tools/perftools/","title":"Profiling using PerfTools","text":"<p>In this section, we discuss the HPE Cray PerfTools performance measurement and analysis tool, accessed via various <code>perftools</code> modules. These tools are also referred to as CrayPat-lite or CrayPat (from an earlier name Performance Analysis Toolkit).</p> <p>PerfTools can be used with compiled programs (typically C/C++ or Fortran), interpreted python, and for executables where the source code is not available. The PerfTools modules admit a very general and flexible approach to profiling, depending on the level of user experience. We start with the least invasive approach, and then describe methods requiring increasing levels of user intervention.</p>"},{"location":"software-tools/perftools/#generating-a-profile-with-pat_run","title":"Generating a profile with <code>pat_run</code>","text":"<p>For an existing executable, or for a python script, a simple profile can be generated with minimal intervention using <code>pat_run</code>. This is also relevant for executables for which the source code is not available (but are dynamically linked).</p>"},{"location":"software-tools/perftools/#existing-executable","title":"Existing executable","text":"<ol> <li> <p>Make sure the <code>perftools-base</code> module is loaded. This provides the    common underlying functionality of PerfTools, and should be present    by default.</p> </li> <li> <p>Run the application in the usual way, but insert <code>pat_run</code> just    before the executable name:    <pre><code>srun --ntasks=36 pat_run ./myapp.x\n</code></pre></p> </li> <li> <p>At the start of execution, a new directory will appear in the current    working directory which contains    the profiling data files. The default directory name is that of the    executable followed by a unique series of numbers for each profiling    instance, e.g., <code>myapp.x+2048522-1010655545s</code>.</p> </li> <li> <p>To produce a summary report of the profiling data, run <code>pat_report</code>    with the new directory name as the argument, e.g.,    <pre><code>pat_report myapp.x+2048522-1010655545s\n</code></pre></p> </li> </ol> <p>The typical output will provide a summary, and a series of tables which    are discussed in more detail in the following sections.</p> Typical PerfTools summary output using <code>pat_run</code> <p><pre><code>CrayPat/X:  Version 25.03.0 Revision 5ef9ac5bb rhel9.5_x86_64  01/22/25 23:09:16\nNumber of PEs (MPI ranks):    36\nNumber of Nodes:               1\nNumbers of PEs per Node:      36\nNumbers of Threads per PE:     1\nNumber of Cores per Socket:  144\nExecution start time:  Wed Dec  3 14:28:07 2025\nSystem name and speed:  cs-n0000  2.326 GHz (nominal)\nAMD   Turin                CPU  Family: 26  Model: 17  Stepping:  0\nCore Performance Boost:  288 PEs have CPB capability\n...\n...\n</code></pre> Further output is omitted.</p>"},{"location":"software-tools/perftools/#python","title":"Python","text":"<p>Support for python profiling when using module <code>cray-python</code> is available. For example, <pre><code>module load cray-python\nsrun --ntasks=36 pat_run $(which python3) myapp.py\n</code></pre> Note that the absolute path to the python interpreter is used after <code>pat_run</code>. Again, a summary report can be generated with <code>pat_report</code>, e.g.: <pre><code>module load cray-python\npat_report myapp.py+2048522-1010655545s\n</code></pre></p>"},{"location":"software-tools/perftools/#profiling-following-re-compilation","title":"Profiling following re-compilation","text":"<p>To allow PerfTools access to fuller information about a compiled program, re-compilation is required to allow the introduction of instrumentation. There are a number of ways to do this using PerfTools. A lightweight starting point is to use the <code>perftools-lite</code> module.</p>"},{"location":"software-tools/perftools/#perftools-lite","title":"perftools-lite","text":"<ol> <li> <p>Make sure the <code>perftools-base</code> module is loaded (it should be present     by default), and load <code>perftools-lite</code> in addition:     <pre><code>module load perftools-lite\n</code></pre></p> </li> <li> <p>Compile your application normally (make sure any existing objects are     removed). A message will appear at the link stage     indicating that the executable has been instrumented. For example:</p> <p><pre><code>cc -o myapp.x myapp.c\nINFO: creating the PerfTools-instrumented executable 'myapp.x' (lite-samples)\n</code></pre> You must use the compiler wrappers <code>cc</code>, <code>CC</code> or <code>ftn</code>.</p> </li> <li> <p>Run the new executable by submitting a job in the usual way.     There are no special additions related to profiling once     the executable exists. E.g.,     <pre><code>srun --ntasks=36 ./myapp.x\n</code></pre></p> </li> <li> <p>At the start of execution, a new directory will be created to hold     the profiling data files. The directory name is based on the     executable name and a unique string of numbers for each profiling     run, e.g., <code>myapp.x+1607079-1010655545s</code>. Note the <code>s</code> at the end,     indicating this was a sampling exercise.</p> </li> <li> <p>When the job finishes executing, summary profile report will be     directed to standard output (i.e., at the end of the job's output file).</p> <p>The <code>perftools</code> report is structured as a series of tables which are designed to be self-explanatory.</p> </li> </ol> Typical PerfTools sampling output via <code>perftools-lite</code> <p><pre><code>...\nSummary output omitted\n...\nTable 1:  Sample Profile by Function\n\nSamp%     |    Samp |  Imb. |  Imb. | Group\n          |         |  Samp | Samp% |  Function\n          |         |       |       |   PE=HIDE\n\n   100.0% | 6,113.2 |    -- |    -- | Total\n |-----------------------------------------------------------------\n |  92.5% | 5,654.4 |    -- |    -- | USER\n||----------------------------------------------------------------\n||  22.4% | 1,371.3 |  86.7 |  6.1% | function_a\n||  11.0% |   674.6 |  61.4 |  8.6% | function_b\n||   8.9% |   545.9 |  59.1 | 10.0% | function_c\n||   8.2% |   502.8 |  26.2 |  5.1% | function_d\n||   7.8% |   474.5 |  18.5 |  3.9% | function_e\n||================================================================\n |   3.9% |   235.7 |    -- |    -- | MPI\n||----------------------------------------------------------------\n||   3.6% |   222.8 | 213.2 | 50.3% | MPI_Waitall\n||================================================================\n...\n...\n</code></pre> In this incomplete example, the table provides an overview of the propartion of the time (i.e., the samples) spent in different parts of the code. Over 90% of the time is spent in the user code, and 3.9% of time is spent in message passing (MPI). By default, only a subset of the most significant functions are shown. Further output is omitted.</p> <p>For <code>perftools-lite</code> the default profiling is a sampling exercise, where a statistical picture of performance is obtained based on the proportion of samples taken in different parts of the program. The report should include the default sampling interval: <pre><code>Sampling interval:  10000 microsecs\n</code></pre> This choice should keep the overhead of profiling low compared to running the program without profiling.</p>"},{"location":"software-tools/perftools/#perftools-lite-events","title":"<code>perftools-lite-events</code>","text":"<p>In the bare <code>perftools-lite</code> sampling approach, a statistical picture of performance is obtained. If more detailed information is required, an event-based approach can be employed. This is typically based on the time-stamp of events such as the entry to and exit from a particular function. This comes at the cost of higher overhead in time taken and the size of the report files generated.</p> <p>To prepare an executable for event profiling, follow the same process as for sampling, that is:</p> <ol> <li> <p>Make sure the <code>perftools-base</code> module is loaded and load the     <code>perftools-lite-events</code> module</p> <p><code>module load perftools-lite-events</code></p> </li> <li> <p>Compile your application normally. For example:</p> <pre><code>ftn -o myapp.x myapp.f90\nINFO: creating the PerfTools-instrumented executable 'myapp.x' (lite-events)\n</code></pre> </li> <li> <p>Run the new executable by submitting a job in the usual way.</p> </li> <li> <p>Analyse the data. Again, a summary will appear at the end of execution    in the standard output. As this is now event-based, additional information    such as the exact number of calls to a given function can be presented.</p> </li> </ol> Example PerfTools event output via <code>perftools-lite-events</code> <p><pre><code>...\nSummary output omitted\n...\n\nTable 1:  Profile by Function Group and Function\n\nTime%     |      Time |     Imb. |  Imb. |        Calls | Group\n          |           |     Time | Time% |              |  Function=[MAX10]\n          |           |          |       |              |   PE=HIDE\n          |           |          |       |              |    Thread=HIDE\n\n   100.0% | 15.801060 |       -- |    -- | 10,958,605.4 | Total\n|-----------------------------------------------------------------------------\n|   89.9% | 14.203748 |       -- |    -- | 10,932,614.3 | USER\n||----------------------------------------------------------------------------\n||  13.9% |  2.191225 | 0.094256 |  4.3% |  1,621,433.3 | function_a\n||  11.3% |  1.781757 | 0.029626 |  1.7% |        100.0 | function_b.LOOP@li.164\n||   8.3% |  1.311064 | 0.045393 |  3.5% |  1,621,433.3 | function_c\n||   6.7% |  1.054684 | 0.050999 |  4.8% |        100.0 | function_d.LOOP@li.264\n||============================================================================\n|    5.4% |  0.856573 |       -- |    -- |     23,434.0 | MPI\n||----------------------------------------------------------------------------\n||   5.0% |  0.792062 | 0.297903 | 28.5% |        313.0 | MPI_Waitall\n||============================================================================\n|    2.5% |  0.388409 |       -- |    -- |      2,030.0 | OMP\n|=============================================================================\n</code></pre> In contrast to the sampling approach, event-based profiling can provide the actual time (seconds) spent in various parts of the program, along with the exact number of calls to particlular functions. As compile time information is available based on the source code, the profiler is also be able to associate time with specific line numbers in the code.</p> <p>Profiling with events can generate large amounts of data, so it is best to start with a small problem size of short duration. Additional measures to reduce the overhead of profiling by targeting specific part of a program are also discussed below.</p>"},{"location":"software-tools/perftools/#viewing-profiling-data","title":"Viewing profiling data","text":""},{"location":"software-tools/perftools/#using-pat_report","title":"Using <code>pat_report</code>","text":"<p>The default reports produced by <code>perftools-lite</code> and <code>perftools-lite-events</code> give information on a relatively small number of the most significant routines in the instrumented program in terms of samples, or time taken. The <code>pat_report</code> utility can be used to interrogate the profiling data to give additional information, particularly when event tracing is used.</p> <p>The report format can be controlled with the <code>-O</code> flag to <code>pat_report</code>. A number of examples are:</p> <ul> <li><code>-O calltree</code>   Show top-down call tree with inclusive times (or samples).</li> <li><code>-O callers</code>   Show the calls leading to the routines that have a high use in the   report (bottom-up).</li> <li><code>-O callers+src</code>   Append the relevant source code line numbers in the callers list.</li> <li><code>-O load_balance</code>   Show load-balance statistics for the high-use routines in the program.   Parallel processes with minimum, maximum and median times for routines   will be displayed. Only available with event profiling.</li> <li><code>-O mpi_callers</code>   Show MPI message statistics. Only available with event profiling.</li> </ul> <p>Other <code>pat_report</code> options include:</p> <ul> <li><code>-T</code> Set threshold for reporting to zero; this will show all functions     called by the program.</li> <li><code>-v</code> Give verbose information and suggestions in the Table notes.</li> </ul> <p>See <code>man pat_report</code> for further information.</p>"},{"location":"software-tools/perftools/#using-the-apprentice-gui","title":"Using the Apprentice GUI","text":"<p>A graphical user interface to PerfTools results is provided by Apprentice, for which a suitable X-windows LINK PENING connection will be required.</p> <p>Apprentice is invoked with, e.g., <pre><code>module load perftools-base\napp3 myapp.x+606388-1010655545t\n</code></pre> where the <code>myapp.x+606388-1010655545t</code> is the relevant profiling directory. The text report (cf <code>pat_report</code>) or various graphical representations can be explored.</p>"},{"location":"software-tools/perftools/#general-perftools-instrumentation","title":"General <code>perftools</code> instrumentation","text":"<p>The <code>perftools-lite</code> and <code>perftools-lite-events</code> modules provide a simple way to generate sampling and event-based profiles, respectively. However, for a large production run, event sampling might come with an unduly large overhead. In this situation, it would be disirable to be able to combine the low overhead of the sampling approach with the detail generated by the event-based profile. This can be done using the general <code>perftools</code> module.</p>"},{"location":"software-tools/perftools/#sampling-via-pat_build","title":"Sampling via <code>pat_build</code>","text":"<ol> <li> <p>Ensure the <code>perftools-base</code> module is loaded, and load the <code>perftools</code>     module:     <pre><code>module load perftools\n</code></pre></p> </li> <li> <p>Compile or re-compile your code using the compiler     wrappers (<code>cc</code>, <code>CC</code> or <code>ftn</code>). Object files (or libraries) need to be     made available to PerfTools to be able to build an instrumented     executable for profiling.     This may mean that the compile and link stage need to be     separated, e.g..     <pre><code>cc -c myapp.c\ncc -o myapp.x myapp.o\n</code></pre></p> </li> <li> <p>To instrument the binary, use the <code>pat_build</code> command. This will     generate a new executable with <code>+pat</code> appended, e.g.:     <pre><code>pat_build myapp.x\n</code></pre>     will generate a new executable <code>myapp.x+pat</code> (it will leave the     original unchanged).</p> </li> <li> <p>Run the new executable with <code>+pat</code> extension to generate a sampling     result. This will produce a new directory with the raw sampling     results, e.g., <code>myapp.x+pat+540878-1010655545s</code>.</p> </li> <li> <p>At this point the sampling results directory will contain a single     subdirectory with the raw results (typically <code>xf-files</code>). Use     <code>pat_report</code> to generate a report in the usual way. This will     also create a new file in the results directory called     <code>build-options.apa</code>.</p> </li> </ol>"},{"location":"software-tools/perftools/#targeted-event-profiling","title":"Targeted event profiling","text":"<p>What we can now do is to use the sampling information produced by the program with the <code>+pat</code> extension to generate an event based profile which consists of only those routines identified in the sampling as significant. This reduces the overhead of the event profiling.</p> <ol> <li> <p>Generate a further executable using <code>pat_build</code> from the build options    file produced at the previous sampling report stage, e.g.:    <pre><code>pat_build -O myapp.x+pat+540878-1010655545s/build-options.apa\n</code></pre>    The new execytable will have the exetension <code>myapp.x+apa</code> (for    automatic program analysis).</p> </li> <li> <p>Run the new executable with the <code>.apa</code> extension to produce a    new event-based profile. This with create a new results    directory, e.g., <code>myapp.x+apa+933004-1010655545t</code>. The <code>t</code> at    the end of the directory name indicates this is a trace, or    event-based profile.</p> </li> <li> <p>A report on the new event-based profile can now be generated    in the usual way using <code>pat_report</code>.</p> </li> </ol>"},{"location":"software-tools/perftools/#manual-event-specification","title":"Manual event specification","text":"<p>If the automatic program analysis does not produce the correct events, <code>pat_build</code> can be used to specify explicitly the set to be collected. The are a number of options to do this, e.g.:</p> <ol> <li> <p><code>pat_build -w myapp.x</code> selects all functions/events.</p> </li> <li> <p><code>pat_build -w -g mpi myapp.x</code> selects a group of functions (here MPI    calls). Other groups include <code>libsci</code>, <code>lapack</code>, <code>omp</code> (for OpenMP    runtime API functions).</p> </li> <li><code>pat_build -T function1 myapp.x</code> selects a named function (this is    the mangled name for Fortran and C++).</li> </ol> <p>See the manual page for <code>pat_build</code> for further details.</p>"},{"location":"software-tools/perftools/#hardware-counter-groups","title":"Hardware counter groups","text":"<p>Profiling will collect a default set of hardware counters which will appear int the report, e.g., <pre><code>  PAPI_TOT_CYC      2,664,774,564,580   # Total number of CPU cycles\n  PAPI_TOT_INS      5,310,155,653,321   # Instructions completed\n  PAPI_TLB_DM             125,957,932   # Translation lookaside buffer misses\n  PAPI_FP_OPS       3,437,814,054,580   # Floating point operations\n</code></pre> (the annonations are added here). A number of different groups are available which can be selected via the environment variable, e.g., <pre><code>export PAT_RT_PERFCTR=fp_stats\n</code></pre> to select floating point operations. Only one counter group can be selected in any one profileing run. A full list of individual counter descriptions is given by <pre><code>papi_avail\n</code></pre> which also indicates whether the counters are available or not.</p>"},{"location":"software-tools/perftools/#further-information","title":"Further information","text":"<p>Additional information is available interactively via <code>pat_help</code>, and via man pages for <code>pat_report</code>, <code>pat_build</code> etc.</p> <ul> <li>HPE Performance Analysis Tools User Guide</li> </ul>"},{"location":"software-tools/scalasca/","title":"Profiling using Scalasca","text":"<p>Scalasca is installed on Cirrus, which is an open source performance profiling tool. Two versions are provided, using GCC 8.2.0 and the Intel 19 compilers; both use the HPE MPT library to provide MPI and SHMEM. An important distinction is that the GCC+MPT installation cannot be used to profile Fortran code as MPT does not provide GCC Fortran module files. To profile Fortran code, please use the Intel+MPT installation.</p> <p>Loading the one of the modules will autoload the correct compiler and MPI library:</p> <pre><code>module load scalasca/2.6-gcc8-mpt225\n</code></pre> <p>or</p> <pre><code>module load scalasca/2.6-intel19-mpt225\n</code></pre> <p>Once loaded, the profiler may be run with the <code>scalasca</code> or <code>scan</code> commands, but the code must first be compiled first with the Score-P instrumentation wrapper tool. This is done by prepending the compilation commands with <code>scorep</code>, e.g.:</p> <pre><code>scorep mpicc -c main.c -o main\nscorep mpif90 -openmp main.f90 -o main\n</code></pre> <p>Advanced users may also wish to make use of the Score-P API. This allows you to manually define function and region entry and exit points.</p> <p>You can then profile the execution during a Slurm job by prepending your <code>srun</code> commands with one of the equivalent commands <code>scalasca -analyze</code> or <code>scan -s</code>:</p> <pre><code>scalasca -analyze srun ./main\nscan -s srun ./main\n</code></pre> <p>You will see some output from Scalasca to stdout during the run. Included in that output will be the name of an experiment archive directory, starting with scorep_, which will be created in the working directory. If you want, you can set the name of the directory by exporting the <code>SCOREP_EXPERIMENT_DIRECTORY</code> environment variable in your job script.</p> <p>There is an associated GUI called Cube which can be used to process and examine the experiment results, allowing you to understand your code's performance. This has been made available via a Singularity container. To start it, run the command <code>cube</code> followed by the file in the experiment archive directory ending in .cubex (or alternatively the whole archive), as seen below:</p> <pre><code>cube scorep_exp_1/profile.cubex\n</code></pre> <p>The Scalasca quick reference guide found here provides a good overview of the toolset's use, from instrumentation and use of the API to analysis with Cube.</p>"},{"location":"software-tools/spack/","title":"Spack","text":"<p>Spack is a package manager, a tool to assist with building and installing software as well as determining what dependencies are required and installing those. It was originally designed for use on HPC systems, where several variations of a given package may be installed alongside one another for different use cases -- for example different versions, built with different compilers, using MPI or hybrid MPI+OpenMP. Spack is principally written in Python but has a component written in Answer Set Programming (ASP) which is used to determine the required dependencies for a given package installation.</p> <p>Users are welcome to install Spack themselves in their own directories, but we are making an experimental installation tailored for Cirrus available centrally. This page provides documentation on how to activate and install packages using the central installation on Cirrus. For more in-depth information on using Spack itself please see the developers' documentation.</p>"},{"location":"software-tools/spack/#activating-spack","title":"Activating Spack","text":"<p>Several modules with <code>spack</code> in their name are visible to you. You should load the <code>spack</code> module:</p> <pre><code>auser@login01:~&gt; module load spack\n</code></pre> <p>This configures Spack to place its cache on and install software to a directory called <code>.spack</code> in your base work directory, e.g. at <code>/work/t01/t01/auser/.spack</code>.</p> <p>At this point Spack is available to you via the <code>spack</code> command. You can get started with <code>spack help</code>, reading the Spack documentation, or by testing a package's installation.</p>"},{"location":"software-tools/spack/#using-spack-on-cirrus","title":"Using Spack on Cirrus","text":""},{"location":"software-tools/spack/#installing-software","title":"Installing software","text":"<p>At its simplest, Spack installs software with the <code>spack install</code> command:</p> <pre><code>auser@login01:~&gt; spack install gromacs\n</code></pre> <p>This very simple <code>gromacs</code> installation specification, or spec, would install GROMACS using the default options given by the Spack <code>gromacs</code> package. The spec can be expanded to include which options you like. For example, the command</p> <pre><code>auser@login01:~&gt; spack install gromacs@2024.2%gcc+mpi\n</code></pre> <p>would use the GCC compiler to install an MPI-enabled version of GROMACS version 2024.2.</p> <p>Tip</p> <p>Spack needs to bootstrap the installation of some extra software in order to function, principally <code>clingo</code> which is used to solve the dependencies required for an installation. The first time you ask Spack to concretise a spec into a precise set of requirements, it will take extra time as it downloads this software and extracts it into a local directory for Spack's use.</p> <p>You can find information about any Spack package and the options available to use with the <code>spack info</code> command:</p> <pre><code>auser@login01:~&gt; spack info gromacs\n</code></pre> <p>Tip</p> <p>The Spack developers also provide a website at https://packages.spack.io/ where you can search for and examine packages, including all information on options, versions and dependencies.</p> <p>When installing a package, Spack will determine what dependencies are required to support it. If they are not already available to Spack, either as packages that it has installed beforehand or as external dependencies, then Spack will also install those, marking them as implicitly installed, as opposed to the explicit installation of the package you requested. If you want to see the dependencies of a package before you install it, you can use <code>spack spec</code> to see the full concretised set of packages:</p> <pre><code>auser@login01:~&gt; spack spec gromacs@2024.2%gcc+mpi\n</code></pre> <p>Tip</p> <p>Spack on Cirrus has been configured to use as much of the HPE Cray Programming Environment as possible. For example, this means that Cray LibSci will be used to provide the BLAS, LAPACK and ScaLAPACK dependencies and Cray MPICH will provide MPI. It is also configured to allow it to re-use as dependencies any packages that the Cirrus CSE team has <code>spack install</code>ed centrally, potentially helping to save you build time and storage quota.</p>"},{"location":"software-tools/spack/#using-spack-packages","title":"Using Spack packages","text":"<p>Spack provides a module-like way of making software that you have installed available to use. If you have a GROMACS installation, you can make it available to use with <code>spack load</code>:</p> <pre><code>auser@login01:~&gt; spack load gromacs\n</code></pre> <p>At this point you should be able to use the software as normal. You can then remove it once again from the environment with <code>spack unload</code>:</p> <pre><code>auser@login01:~&gt; spack unload gromacs\n</code></pre> <p>If you have multiple variants of the same package installed, you can use the spec to distinguish between them. You can always check what packages have been installed using the <code>spack find</code> command. If no other arguments are given it will simply list all installed packages, or you can give a package name to narrow it down:</p> <pre><code>auser@login01:~&gt; spack find gromacs\n</code></pre> <p>You can see your packages' install locations using <code>spack find --paths</code> or <code>spack find -p</code>.</p>"},{"location":"software-tools/spack/#maintaining-your-spack-installations","title":"Maintaining your Spack installations","text":"<p>In any Spack command that requires as an argument a reference to an installed package, you can provide a hash reference to it rather than its spec. You can see the first part of the hash by running <code>spack find -l</code>, or the full hash with <code>spack find -L</code>. Then use the hash in a command by prefixing it with a forward slash, e.g. <code>wjy5dus</code> becomes <code>/wjy5dus</code>.</p> <p>If you have two packages installed which appear identical in <code>spack find</code> apart from their hash, you can differentiate them with <code>spack diff</code>:</p> <pre><code>auser@login01:~&gt; spack diff /wjy5dus /bleelvs\n</code></pre> <p>You can uninstall your packages with <code>spack uninstall</code>:</p> <pre><code>auser@login01:~&gt; spack uninstall gromacs@2024.2\n</code></pre> <p>and of course, to be absolutely certain that you are uninstalling the correct package, you can provide the hash:</p> <pre><code>auser@login01:~&gt; spack uninstall /wjy5dus\n</code></pre> <p>Uninstalling a package will leave behind any implicitly installed packages that were installed to support it. Spack may have also installed build-time dependencies that aren't actually needed any more -- these are often packages like <code>autoconf</code>, <code>cmake</code> and <code>m4</code>. You can run the garbage collection command to uninstall any build dependencies and implicit dependencies that are no longer required:</p> <pre><code>auser@login01:~&gt; spack gc\n</code></pre> <p>If you commonly use a set of Spack packages together you may want to consider using a Spack environment to assist you in their installation and management. Please see the Spack documentation for more information.</p>"},{"location":"software-tools/spack/#custom-configuration","title":"Custom configuration","text":"<p>Spack is configured using YAML files. The central installation on Cirrus made available to users is configured to use the HPE Cray Programming Environment and to allow you to start installing software to your <code>/work</code> directories right away, but if you wish to make any changes you can provide your own overriding userspace configuration.</p> <p>Your own configuration should fit in the user level scope. On Cirrus Spack is configured to, by default, place and look for your configuration files in your work directory at e.g. <code>/work/t01/t01/auser/.spack</code>. You can however override this to have Spack use any directory you choose by setting the <code>SPACK_USER_CONFIG_PATH</code> environment variable, for example:</p> <pre><code>auser@login01:~&gt; export SPACK_USER_CONFIG_PATH=/work/t01/t01/auser/spack-config\n</code></pre> <p>Of course this will need to be a directory where you have write permissions, such in your home or work directories, or in one of your project's <code>shared</code> directories.</p> <p>You can edit the configuration files directly in a text editor or by running, for example:</p> <pre><code>auser@login01:~&gt; spack config edit repos\n</code></pre> <p>which would open your <code>repos.yaml</code> in <code>vim</code>.</p> <p>Tip</p> <p>If you would rather not use <code>vim</code>, you can change which editor is used by Spack by setting the <code>SPACK_EDITOR</code> environment variable.</p> <p>The final configuration used by Spack is a compound of several scopes, from the Spack defaults which are overridden by the Cirrus system configuration files, which can then be overridden in turn by your own configurations. You can see what options are in use at any point by running, for example:</p> <pre><code>auser@login01:~&gt; spack config get config\n</code></pre> <p>which goes through any and all <code>config.yaml</code> files known to Spack and sets the options according to those files' level of precedence. You can also get more information on which files are responsible for which lines in the final active configuration by running, for example to check <code>packages.yaml</code>:</p> <pre><code>auser@login01:~&gt; spack config blame packages\n</code></pre> <p>Unless you have already written a <code>packages.yaml</code> of your own, this will show a mix of options originating from the Spack defaults and also from an <code>cirru-user</code> directory which is where we have told Spack how to use packages from the HPE Cray Programming Environment.</p> <p>If there is some behaviour in Spack that you want to change, looking at the output of <code>spack config get</code> and <code>spack config blame</code> may help to show what you would need to do. You can then write your own user scope configuration file to set the behaviour you want, which will override the option as set by the lower-level scopes.</p> <p>Please see the Spack documentation to find out more about writing configuration files.</p>"},{"location":"software-tools/spack/#writing-new-packages","title":"Writing new packages","text":"<p>A Spack package is at its core a Python <code>package.py</code> file which provides instructions to Spack on how to obtain source code and compile it. A very simple package will allow it to build just one version with one compiler and one set of options. A more fully-featured package will list more versions and include logic to build them with different compilers and different options, and to also pick its dependencies correctly according to what is chosen.</p> <p>Spack provides several thousand packages in its <code>builtin</code> repository. You may be able to use these with no issues on Cirrus by simply running <code>spack install</code> as described above, but if you do run into problems in the interaction between Spack and the CPE compilers and libraries then you may wish to write your own. Where the ARCHER2 CSE service has encountered problems with packages we have provided our own in a repository located at <code>$SPACK_ROOT/var/spack/repos/cirrus</code>.</p>"},{"location":"software-tools/spack/#creating-your-own-package-repository","title":"Creating your own package repository","text":"<p>A package repository is a directory containing a <code>repo.yaml</code> configuration file and another directory called <code>packages</code>. Directories within the latter are named for the package they provide, for example <code>cp2k</code>, and contain in turn a <code>package.py</code>. You can create a repository from scratch with the command</p> <pre><code>auser@login01:~&gt; spack repo create dirname\n</code></pre> <p>where <code>dirname</code> is the name of the directory holding the repository. This command will create the directory in your current working directory, but you can choose to instead provide a path to its location. You can then make the new repository available to Spack by running:</p> <pre><code>auser@login01:~&gt; spack repo add dirname\n</code></pre> <p>This adds the path to <code>dirname</code> to the <code>repos.yaml</code> file in your user scope configuration directory as described above. If your <code>repos.yaml</code> doesn't yet exist, it will be created.</p> <p>A Spack repository can similarly be removed from the config using:</p> <pre><code>auser@login01:~&gt; spack repo rm dirname\n</code></pre>"},{"location":"software-tools/spack/#namespaces-and-repository-priority","title":"Namespaces and repository priority","text":"<p>A package can exist in several repositories. For example, the Quantum Espresso package is provided by both the <code>builtin</code> repository provided with Spack and also by the <code>cirrus</code> repository; the latter has been patched to work on Cirrus.</p> <p>To distinguish between these packages, each repository's packages exist within that repository's namespace. By default the namespace is the same as the name of the directory it was created in, but Spack does allow it to be different. Both <code>builtin</code> and <code>cirrus</code> use the same directory name and namespace.</p> <p>Tip</p> <p>If you want your repository namespace to be different from the name of the directory, you can change it either by editing the repository's <code>repo.yaml</code> or by providing an extra argument to <code>spack repo create</code>:</p> <pre><code>auser@login01:~&gt; spack repo create dirname namespace\n</code></pre> <p>Running <code>spack find -N</code> will return the list of installed packages with their namespace. You'll see that they are then prefixed with the repository namespace, for example <code>builtin.bison@3.8.2</code> and <code>cirrus.quantum-espresso@7.2</code>. In order to avoid ambiguity when managing package installation you can always prefix a spec with a repository namespace.</p> <p>If you don't include the repository in a spec, Spack will search in order all the repositories it has been configured to use until it finds a matching package, which it will then use. The earlier in the list of repositories, the higher the priority. You can check this with:</p> <pre><code>auser@login01:~&gt; spack repo list\n</code></pre> <p>If you run this without having added any repositories of your own, you will see that the two available repositories are <code>cirrus</code> and <code>builtin</code>, in this order. This means that <code>cirrus</code> has higher priority. Because of this, running <code>spack install quantum-espresso</code> would install <code>cirrus.quantum-espresso</code>, but you could still choose to install from the other repository with <code>spack install builtin.quantum-espresso</code>.</p>"},{"location":"software-tools/spack/#creating-a-package","title":"Creating a package","text":"<p>Once you have a repository of your own in place, you can create new packages to store within it. Spack has a <code>spack create</code> command which will do the initial setup and create a boilerplate <code>package.py</code>. To create an empty package called <code>packagename</code> you would run:</p> <pre><code>auser@login01:~&gt; spack create --name packagename\n</code></pre> <p>However, it will very often be more efficient if you instead provide a download URL for your software as the argument. For example, the Code_Saturne 8.0.3 source is obtained from <code>https://www.code-saturne.org/releases/code_saturne-8.0.3.tar.gz</code>, so you can run:</p> <pre><code>auser@login01:~&gt; spack create https://www.code-saturne.org/releases/code_saturne-8.0.3.tar.gz\n</code></pre> <p>Spack will determine from this the package name, the download URLs for all versions X.Y.Z matching the <code>https://www.code-saturne.org/releases/code_saturne-X.Y.Z.tar.gz</code> pattern. It will then ask you interactively which of these you want to use. Finally, it will download the <code>.tar.gz</code> archives for those versions and calculate their checksums, then place all this information in the initial version of the package for you. This takes away a lot of the initial work!</p> <p>At this point you can get to work on the package. You can edit an existing package by running</p> <pre><code>auser@login01:~&gt; spack edit packagename\n</code></pre> <p>or by directly opening <code>packagename/package.py</code> within the repository with a text editor.</p> <p>The boilerplate code will note several sections for you to fill out. If you did provide a source code download URL, you'll also see listed the versions you chose and their checksums.</p> <p>A package is implemented as a Python class. You'll see that by default it will inherit from the <code>AutotoolsPackage</code> class which defines how a package following the common <code>configure</code> &gt; <code>make</code> &gt; <code>make install</code> process should be built. You can change this to another build system, for example <code>CMakePackage</code>. If you want, you can have the class inherit from several different types of build system classes and choose between them at install time.</p> <p>Options must be provided to the build. For an <code>AutotoolsPackage</code> package, you can write a <code>configure_args</code> method which very simply returns a list of the command line arguments you would give to <code>configure</code> if you were building the code yourself. There is an identical <code>cmake_args</code> method for <code>CMakePackage</code> packages.</p> <p>Finally, you will need to provide your package's dependencies. In the main body of your package class you should add calls to the <code>depends_on()</code> function. For example, if your package needs MPI, add <code>depends_on(\"mpi\")</code>. As the argument to the function is a full Spack spec, you can provide any necessary versioning or options, so, for example, if you need PETSc 3.18.0 or newer with Fortran support, you can call <code>depends_on(\"petsc+fortran@3.18.0:\")</code>.</p> <p>If you know that you will only ever want to build a package one way, then providing the build options and dependencies should be all that you need to do. However, if you want to allow for different options as part of the install spec, patch the source code or perform post-install fixes, or take more manual control of the build process, it can become much more complex. Thankfully the Spack developers have provided excellent documentation covering the whole process, and there are many existing packages you can look at to see how it's done.</p>"},{"location":"software-tools/spack/#tips-when-writing-packages-for-cirrus","title":"Tips when writing packages for Cirrus","text":"<p>Here are some useful pointers when writing packages for use with the HPE Cray Programming Environment on Cirrus.</p>"},{"location":"software-tools/spack/#cray-compiler-wrappers","title":"Cray compiler wrappers","text":"<p>An important point of note is that Spack does not use the Cray compiler wrappers <code>cc</code>, <code>CC</code> and <code>ftn</code> when compiling code. Instead, it uses the underlying compilers themselves. Remember that the wrappers automate the use of Cray LibSci, Cray FFTW, Cray HDF5 and Cray NetCDF. Without this being done for you, you may need to take extra care to ensure that the options needed to use those libraries are correctly set.</p>"},{"location":"software-tools/spack/#using-cray-libsci","title":"Using Cray LibSci","text":"<p>Cray LibSci provides optimised implementations of BLAS, BLACS, LAPACK and ScaLAPACK on Cirrus. These are bundled together into single libraries named for variants on <code>libsci_cray.so</code>. Although Spack itself knows about LibSci, many applications don't and it can sometimes be tricky to get them to use these libraries when they are instead looking for <code>libblas.so</code> and the like.</p> <p>The <code>configure</code> or <code>cmake</code> or equivalent step for your software will hopefully allow you to manually point it to the correct library. For example, Code_Saturne's <code>configure</code> can take the options <code>--with-blas-lib</code> and <code>--with-blas-libs</code> which respectively tell it the location to search and the libraries to use in order to build against BLAS.</p> <p>Spack can provide the correct BLAS library search and link flags to be passed on to <code>configure</code> via <code>self.spec[\"blas\"].libs</code>, a <code>LibraryList</code> object. So, the Code_Saturne package uses the following <code>configure_args()</code> method:</p> <pre><code>def configure_args(self):\n    blas = self.spec[\"blas\"].libs\n    args = [\"--with-blas-lib={0}\".format(blas.search_flags),\n            \"--with-blas-libs={0}\".format(blas.link_flags)]\n    return args\n</code></pre> <p>Here the <code>blas.search_flags</code> attribute is resolved to a <code>-L</code> library search flag using the path to the correct LibSci directory, taking into account whether the libraries for the Cray, GCC or AOCC compilers should be used. <code>blas.link_flags</code> similarly gives a <code>-l</code> flag for the correct LibSci library. Depending on what you need, the <code>LibraryList</code> has other attributes which can help you pass the options needed to get <code>configure</code> to find and use the correct library.</p>"},{"location":"user-guide/batch/","title":"Running Jobs on Cirrus","text":"<p>As with most HPC services, Cirrus uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. Cirrus uses the Slurm software to schedule jobs.</p> <p>Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below.</p> <p>Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below.</p> <p>Hint</p> <p>If you have any questions on how to run jobs on Cirrus do not hesitate to contact the Cirrus Service Desk.</p> <p>You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts.</p>"},{"location":"user-guide/batch/#use-of-the-work-file-system","title":"Use of the <code>/work</code> file system","text":"<p>Jobs executing in the batch system should use the <code>/work</code> file system. In particular, the <code>/home</code> file system (including home directories <code>${HOME}</code>) are not available to batch jobs. All relevant software installation, scripts, executable files, and input data should be stored in an appropriate location in <code>/work</code> before submitting batch jobs.</p>"},{"location":"user-guide/batch/#basic-slurm-commands","title":"Basic Slurm commands","text":"<p>There are three key commands used to interact with the Slurm on the command line:</p> <ul> <li><code>sinfo</code> - Get information on the partitions and resources available</li> <li><code>sbatch jobscript.slurm</code> - Submit a job submission script (in this   case called: <code>jobscript.slurm</code>) to the scheduler</li> <li><code>squeue</code> - Get the current status of jobs submitted to the scheduler</li> <li><code>scancel 12345</code> - Cancel a job (in this case with the job ID <code>12345</code>)</li> </ul> <p>We cover each of these commands in more detail below.</p>"},{"location":"user-guide/batch/#sinfo-information-on-resources","title":"<code>sinfo</code>: information on resources","text":"<p><code>sinfo</code> is used to query information about available resources and partitions. Without any options, <code>sinfo</code> lists the status of all resources and partitions, e.g.</p> <pre><code>[auser@login01 ~]$ sinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nCompute*     up   infinite      4 drain* cs-n[0086-0087,0112-0113]\nCompute*     up   infinite      3  down* cs-n[0050-0051,0170]\nCompute*     up   infinite      4  drain cs-n[0019,0104,0165,0239]\nCompute*     up   infinite      1  alloc cs-n0166\nCompute*     up   infinite    244   idle cs-n[0000-0018,0020-0049,0052-0085,0088-0103,0105-0111,0114-0164,0167-0169,0171-0238,0240-0255]\n</code></pre>"},{"location":"user-guide/batch/#sbatch-submitting-jobs","title":"<code>sbatch</code>: submitting jobs","text":"<p><code>sbatch</code> is used to submit a job script to the job submission system. The script will typically contain one or more <code>srun</code> commands to launch parallel tasks.</p> <p>When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE.</p> <pre><code>[auser@login01 ~]$ sbatch test-job.slurm\nSubmitted batch job 12345\n</code></pre>"},{"location":"user-guide/batch/#squeue-monitoring-jobs","title":"<code>squeue</code>: monitoring jobs","text":"<p><code>squeue</code> without any options or arguments shows the current status of all jobs known to the scheduler. For example:</p> <pre><code>[auser@login01 ~]$ squeue\n          JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          1554  comp-cse CASTEP_a  auser  R       0:03      2 cs-n01[18-19]\n</code></pre> <p>will list all jobs on Cirrus.</p> <p>The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the <code>--me</code> option:</p> <pre><code>[auser@login01 ~]$ squeue --me\n</code></pre>"},{"location":"user-guide/batch/#scancel-deleting-jobs","title":"<code>scancel</code>: deleting jobs","text":"<p><code>scancel</code> is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. You need to provide the job ID of the job you wish to cancel/stop. For example:</p> <p>[auser@login01 ~]$ scancel 12345</p> <p>will cancel (if waiting) or stop (if running) the job with ID <code>12345</code>.</p>"},{"location":"user-guide/batch/#resource-limits","title":"Resource Limits","text":"<p>Note</p> <p>If you have requirements which do not fit within the current QoS, please</p> <p>contact the Service Desk and we can discuss how to accommodate your requirements.</p> <p>There are different resource limits on Cirrus for different purposes. There are three different things you need to specify for each job:</p> <ul> <li>The amount of primary resource you require (more information on this   below)</li> <li>The partition that you want to use - this specifies the nodes that   are eligible to run your job</li> <li>The Quality of Service (QoS) that you want to use - this specifies   the job limits that apply</li> </ul> <p>Each of these aspects are described in more detail below.</p> <p>Warning</p> <p>On Cirrus, you cannot specify the memory for a job using the <code>--mem</code> options to Slurm (e.g. <code>--mem</code>, <code>--mem-per-cpu</code>). The amount of memory you are assigned is calculated from the amount of primary resource you request.</p>"},{"location":"user-guide/batch/#primary-resources-on-cpu-compute-nodes","title":"Primary resources on CPU compute nodes","text":"<p>The primary resource you request on standard compute nodes are CPU cores. The maximum amount of memory you are allocated is computed as the number of CPU cores you requested multiplied by 1/288th of the total memory available (as there are 288 CPU cores per node). So, if you request the full node (288 cores), then you will be allocated a maximum of all of the memory (768 GB or 1,536 GB) available on the node; however, if you request 1 core, then you will be assigned a maximum of 768/288 = 2.66 GB of the memory available on a standard memory node or 5.33 GB on a high  memory node.</p> <p>Note</p> <p>Using the <code>--exclusive</code> option in jobs will give you access to the full node memory even if you do not explicitly request all of the CPU cores on the node.</p> <p>Warning</p> <p>Using the <code>--exclusive</code> option will charge your account for the usage of the entire node, even if you don't request all the cores in your scripts.</p> <p>Note</p> <p>You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes.</p>"},{"location":"user-guide/batch/#partitions","title":"Partitions","text":"<p>On Cirrus, compute nodes are grouped into partitions. You will have to specify a partition using the <code>--partition</code> option in your submission script. The following table has a list of active partitions on Cirrus.</p> Partition Description Total nodes available Notes standard CPU nodes with 2x 144-core AMD EPYC 9825 processors, 768 GB or 1,536 GB memory 256 highmem CPU nodes with 2x 144-core AMD EPYC 9825 processors, 1,536 GB memory 64 <p>You can list the active partitions using</p> <pre><code>sinfo\n</code></pre> <p>Note</p> <p>You may not have access to all the available partitions.</p>"},{"location":"user-guide/batch/#quality-of-service-qos","title":"Quality of Service (QoS)","text":"<p>On Cirrus Quality of Service (QoS) is used alongside partitions to set resource limits. The following table has a list of active QoS on Cirrus. Note that the number of jobs queued includes both pending and running jobs.</p> QoS Name Jobs Running Per User Jobs Queued Per User Max Walltime Max Size Applies to Partitions Notes standard No limit 512 jobs 48 hours 64 nodes standard, highmem Maximum of 64 nodes in use by any one user at any time. largescale 1 job 4 jobs 24 hours 192 nodes standard long No limit 128 jobs 4 days 16 nodes standard Maximum of 64 nodes in use by any one user at any time. Maximum of 128 nodes in use by this QoS. highpriority No limit 256 jobs 48 hours 128 nodes standard, highmem Chargd at 1.5x normal rate. Maximum of 128 nodes in use by any one user at any time. Maximum of 128 nodes in use by this QoS. short 1 job 2 jobs 20 minutes 2 nodes standard lowpriority No limit 100 jobs 24 hours 64 nodes standard Usage is not charged. Not available to industrial projects. reservation No limit No limit No limit No limit standard, highmem Only usable within reservation."},{"location":"user-guide/batch/#cirrus-qos","title":"Cirrus QoS","text":"<p>You can find out the QoS that you can use by running the following command:</p> <pre><code>sacctmgr show assoc user=$USER cluster=cirrus format=cluster,account,user,qos%50\n</code></pre>"},{"location":"user-guide/batch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/batch/#slurm-error-handling","title":"Slurm error handling","text":""},{"location":"user-guide/batch/#mpi-jobs","title":"MPI jobs","text":"<p>Users of MPI codes may wish to ensure termination of all tasks on the failure of one individual task by specifying the <code>--kill-on-bad-exit</code> argument to <code>srun</code>. E.g.,</p> <pre><code>srun -n 288 --kill-on-bad-exit ./my-mpi-program\n</code></pre> <p>This can prevent effective \"hanging\" of the job until the wall time limit is reached.</p>"},{"location":"user-guide/batch/#automatic-resubmission","title":"Automatic resubmission","text":"<p>Jobs that fail are not automatically resubmitted by Slurm on Cirrus. Automatic resubmission can be enabled for a job by specifying the <code>--requeue</code> option to <code>sbatch</code>.</p>"},{"location":"user-guide/batch/#slurm-error-messages","title":"Slurm error messages","text":"<p>An incorrect submission will cause Slurm to return an error. Some common problems are listed below, with a suggestion about the likely cause:</p> <ul> <li> <p><code>sbatch: unrecognized option &lt;text&gt;</code></p> </li> <li> <p>One of your options is invalid or has a typo. <code>man sbatch</code> to help.</p> </li> <li> <p><code>error: Batch job submission failed: No partition specified or system default partition</code></p> </li> </ul> <p>A <code>--partition=</code> option is missing. You must specify the partition (see the list above). This is most often <code>--partition=standard</code>.</p> <ul> <li><code>error: invalid partition specified: &lt;partition&gt;</code></li> </ul> <p><code>error: Batch job submission failed: Invalid partition name specified</code></p> <p>Check the partition exists and check the spelling is correct.</p> <ul> <li><code>error: Batch job submission failed: Invalid account or account/partition combination specified</code></li> </ul> <p>This probably means an invalid account has been given. Check the <code>--account=</code> options against valid accounts in SAFE.</p> <ul> <li><code>error: Batch job submission failed: Invalid qos specification</code></li> </ul> <p>A QoS option is either missing or invalid. Check the script has a <code>--qos=</code> option and that the option is a valid one from the table above. (Check the spelling of the QoS is correct.)</p> <ul> <li><code>error: Your job has no time specification (--time=)...</code></li> </ul> <p>Add an option of the form <code>--time=hours:minutes:seconds</code> to the submission script. E.g., <code>--time=01:30:00</code> gives a time limit of 90 minutes.</p> <ul> <li><code>error: QOSMaxWallDurationPerJobLimit</code> <code>error: Batch job submission failed: Job violates accounting/QOS policy</code> <code>(job submit limit, user's size and/or time limits)</code></li> </ul> <p>The script has probably specified a time limit which is too long for   the corresponding QoS. E.g., the time limit for the short QoS is 20   minutes.</p>"},{"location":"user-guide/batch/#slurm-queued-reasons","title":"Slurm queued reasons","text":"<p>The <code>squeue</code> command allows users to view information for jobs managed by Slurm. Jobs typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED. The first table provides a description of some job state codes. The second table provides a description of the reasons that cause a job to be in a state.</p> Status Code Description PENDING PD Job is awaiting resource allocation. RUNNING R Job currently has an allocation. SUSPENDED S Job currently has an allocation. COMPLETING CG Job is in the process of completing. Some processes on some nodes may still be active. COMPLETED CD Job has terminated all processes on all nodes with an exit code of zero. TIMEOUT TO Job terminated upon reaching its time limit. STOPPED ST Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. OUT_OF_MEMORY OOM Job experienced out of memory error. FAILED F Job terminated with non-zero exit code or other failure condition. NODE_FAIL NF Job terminated due to failure of one or more allocated nodes. CANCELLED CA Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. <p>Slurm Job State codes</p> <p>For a full list of see Job State Codes</p> Reason Description Priority One or more higher priority jobs exist for this partition or advanced reservation. Resources The job is waiting for resources to become available. BadConstraints The job's constraints can not be satisfied. BeginTime The job's earliest start time has not yet been reached. Dependency This job is waiting for a dependent job to complete. Licenses The job is waiting for a license. WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason. Prolog Its PrologSlurmctld program is still running. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc. NonZeroExitCode The job terminated with a non-zero exit code. InvalidAccount The job's account is invalid. InvalidQOS The job's QOS is invalid. QOSUsageThreshold Required QOS threshold has been breached. QOSJobLimit The job's QOS has reached its maximum job count. QOSResourceLimit The job's QOS has reached some resource limit. QOSTimeLimit The job's QOS has reached its time limit. NodeDown A node required by the job is down. TimeLimit The job exhausted its time limit. ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available. <p>Slurm Job Reasons</p> <p>For a full list of see Job Reasons</p>"},{"location":"user-guide/batch/#output-from-slurm-jobs","title":"Output from Slurm jobs","text":"<p>Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file <code>slurm_&lt;JobID&gt;.out</code>. This file appears in the job's working directory once your job starts running.</p> <p>Note</p> <p>This file is plain text and can contain useful information to help debugging if a job is not working as expected. The Cirrus Service Desk team will often ask you to provide the contents of this file if you contact them for help with issues.</p>"},{"location":"user-guide/batch/#specifying-resources-in-job-scripts","title":"Specifying resources in job scripts","text":"<p>You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive <code>#SBATCH</code>.</p> <p>Note</p> <p>Options provided using <code>#SBATCH</code> directives can also be specified as command line options to <code>sbatch</code>.</p> <p>If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too, the partition they wish to use and the QoS they want to use with the options:</p> <ul> <li><code>--account=&lt;budgetID&gt;</code> your budget ID is usually something like    <code>t01</code> or <code>t01-test</code>. You can see which budget codes you can charge    to in SAFE.</li> <li><code>--partition=&lt;partition&gt;</code> The partition specifies the set of nodes    you want to run on. More information on available partitions is    given above.</li> <li><code>--qos=&lt;QoS&gt;</code> The QoS specifies the limits to apply to your job.    More information on available QoS are given above.</li> </ul> <p>Other common options that are used are:</p> <ul> <li><code>--time=&lt;hh:mm:ss&gt;</code> the maximum walltime for your job. e.g. For a    6.5 hour walltime, you would use <code>--time=6:30:0</code>.</li> <li><code>--job-name=&lt;jobname&gt;</code> set a name for the job to help identify it in    Slurm command output.</li> </ul> <p>In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require.</p> <ul> <li><code>--exclusive</code> to ensure that you have exclusive access to a compute    node</li> <li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li> <li><code>--tasks-per-node=&lt;processes per node&gt;</code> the number of parallel    processes (e.g. MPI ranks) per node.</li> <li><code>--cpus-per-task=&lt;threads per task&gt;</code> the number of threads per    parallel process (e.g. number of OpenMP threads per MPI task for    hybrid MPI/OpenMP jobs). Note: you must also set the    <code>OMP_NUM_THREADS</code> environment variable if using OpenMP in your job    and usually add the <code>--cpu-bind=cores</code> option to <code>srun</code>. See the    example below for an MPI+OpenMP job for further notes on    <code>--cpus-per-task</code>.</li> </ul> <p>Note</p> <p>For parallel jobs, you should generally request exclusive node access with the <code>--exclusive</code> option to ensure you get the expected resources and performance.</p> <p>Note</p> <p>When submitting your job, the scheduler will check that the requested resources are available e.g. that your account is a member of the requested budget, that the requested QoS exists.</p> <p>If things change before the job starts and e.g. your account has been removed from the requested budget or the requested QoS has been deleted then the job will not be able to start.</p> <p>In such cases, the job will be removed from the pending queue by our systems team, as it will no longer be eligible to run.</p>"},{"location":"user-guide/batch/#srun-launching-parallel-jobs","title":"<code>srun</code>: Launching parallel jobs","text":"<p>If you are running parallel jobs, your job submission script should contain one or more <code>srun</code> commands to launch the parallel executable across the compute nodes. As well as launching the executable, <code>srun</code> also allows you to specify the distribution and placement (or pinning) of the parallel processes and threads.</p> <p>If you are running MPI jobs that do not also use OpenMP threading, then you typically use <code>srun</code> with the following options:</p> <ul> <li><code>--hint=nomultithread</code> - Use physical cores only and ignore SMT on logical   cores</li> <li><code>--distribution=block:block</code> - Place parallel (usually MPI) processes sequentially   on cores on the node. This typically gives the best parallel performance for   MPI collective communications. </li> </ul> <p><code>srun</code> will use the specification of nodes and tasks from your job script, <code>sbatch</code> or <code>salloc</code> command to launch the correct number of parallel tasks so these should not be specified again for the <code>srun</code> command unless you want to use resources in a  different way from the original specification.</p> <p>Note</p> <p>See the example job submission scripts below for examples of using <code>srun</code> for pure MPI jobs and for jobs that use OpenMP threading.</p>"},{"location":"user-guide/batch/#example-parallel-job-submission-scripts","title":"Example parallel job submission scripts","text":"<p>A subset of example job submission scripts are included in full below.</p> <p>Hint</p> <p>Do not replace <code>srun</code> with <code>mpirun</code> in the following examples. Although this might work under special circumstances, it is not guaranteed and therefore not supported.</p>"},{"location":"user-guide/batch/#example-job-submission-script-for-mpi-parallel-job","title":"Example: job submission script for MPI parallel job","text":"<p>A simple MPI job submission script to submit a job using 2 compute nodes and 576 MPI processes per node for 20 minutes would look like:</p> <pre><code>#!/bin/bash\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=Example_MPI_Job\n#SBATCH --time=0:20:0\n#SBATCH --exclusive\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=288\n#SBATCH --cpus-per-task=1\n\n# Replace [budget code] below with your budget code (e.g. t01)\n#SBATCH --account=[budget code]\n# We use the \"standard\" partition as we are running on CPU nodes\n#SBATCH --partition=standard\n# We use the \"standard\" QoS as our runtime is less than 4 days\n#SBATCH --qos=standard\n\n# Set the number of threads to 1\n#   This prevents any threaded system libraries from automatically\n#   using threading.\nexport OMP_NUM_THREADS=1\n\n# Launch the parallel job\n#   Using 144 MPI processes and 36 MPI processes per node\n#   srun picks up the distribution from the sbatch options\nsrun --hint=nomultithread --distribution=block:block ./my_mpi_executable.x\n</code></pre> <p>This will run your executable \"my_mpi_executable.x\" in parallel on 576 MPI processes using 2 nodes (288 cores per node, i.e. not using SMT). Slurm will allocate 2 nodes to your job and srun will place 288 MPI processes on each node (one per physical core).</p> <p>By default, srun will launch an MPI job that uses all of the cores you have requested via the \"nodes\" and \"tasks-per-node\" options. If you want to run fewer MPI processes than cores you will need to change the script.</p> <p>For example, to run this program on 512 MPI processes you have two options:</p> <ul> <li>set <code>--tasks-per-node=256</code> for an even distribution across nodes    (this may not always be possible depending on the exact combination    of nodes requested and MPI tasks required)</li> <li>set the number of MPI tasks explicitly using <code>#SBATCH --ntasks=512</code></li> </ul> <p>Note</p> <p>If you specify <code>--ntasks</code> explicitly and it is not compatible with the value of <code>tasks-per-node</code> then you will get a warning message from srun such as <code>srun: Warning: can't honor --ntasks-per-node set to 288</code>.</p> <p>In this case, srun does the sensible thing and allocates MPI processes as evenly as it can across nodes. For example, the second option above would result in 256 MPI processes on each of the 2 nodes.</p> <p>See above for a more detailed discussion of the different <code>sbatch</code> options.</p>"},{"location":"user-guide/batch/#note-on-task-placement","title":"Note on task placement","text":"<p>By default, Slurm will distribute processss to physical cores (cores 0-35 on NUMA region 0, cores 36-71 on NUMA region 1, etc.) in a cyclic fashion. That is, rank 0 would be placed on core 0, rank 1 on core 36, rank 2 on core 72, and so on (in a single-node job). This may be undesirable. Block, rather than cyclic, distribution can be obtained with</p> <pre><code>#SBATCH --distribution=block:block\n</code></pre> <p>The <code>block:block</code> here refers to the distribution on both nodes and NUMA regions (Slurm on Cirrus is configured to treat NUMA regions as separate sockets). This will distribute rank 0 for core 0, rank 1 to core 1, rank 2 to core 2, and so on.</p>"},{"location":"user-guide/batch/#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job","title":"Example: job submission script for MPI+OpenMP (mixed mode) parallel job","text":"<p>Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one node. This means that the number of shared memory threads should be a factor of 288.</p> <p>In the example below, we are using 2 nodes for 6 hours. There are 48 MPI processes in total (24 MPI processes per node) and 12 OpenMP threads per MPI process. This results in all 288 physical cores per node being used.</p> <p>Important</p> <p>Using 24 MPI processes per node (1 per CCD) and 12 OpenMP processes per MPI process is likely to be the highest number of OpenMP threads that will produce good performance as each 12-core CCD shares L3  cache.</p> <p>Note</p> <p>the use of the <code>--cpu-bind=cores</code> option to generate the correct affinity settings.</p> <pre><code>#!/bin/bash\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=Example_MPI_Job\n#SBATCH --time=0:20:0\n#SBATCH --exclusive\n#SBATCH --nodes=2\n#SBATCH --ntasks=48\n#SBATCH --tasks-per-node=24\n#SBATCH --cpus-per-task=12\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n# We use the \"standard\" partition as we are running on CPU nodes\n#SBATCH --partition=standard\n# We use the \"standard\" QoS as our runtime is less than 4 days\n#SBATCH --qos=standard\n\n# Change to the submission directory\ncd $SLURM_SUBMIT_DIR\n\n# Set the number of threads to 12\n#   There are 12 OpenMP threads per MPI process\nexport OMP_NUM_THREADS=12\nexport OMP_PLACES=cores\n\n# Make sure the cpus-per-task option is passed to srun\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Launch the parallel job\n#   Using 48 MPI processes\n#   24 MPI processes per node\n#   12 OpenMP threads per MPI process\n\nsrun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2\n</code></pre> <p>In the above we add <code>export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK</code> to the job script. This is required to ensure that the correct assignment of threads to physical cores takes place. The reason for this is that the value specified to <code>SBATCH</code> does not propagate automatically to <code>srun</code>. The alternative is to specify an additional option to <code>srun</code>, e.g.:</p> <pre><code>srun --hint=nomultithread --distribution=block:block --cpus-per-task=12\n</code></pre> <p>Failure to use either of these approaches may result in threads using the same physical core, which will cause a significant degradation in performance compared with that expected.</p>"},{"location":"user-guide/batch/#example-job-submission-script-for-openmp-parallel-job","title":"Example: job submission script for OpenMP parallel job","text":"<p>A simple OpenMP job submission script to submit a job using 1 CCD nodes and 12 threads for 20 minutes would look like:</p> <pre><code>#!/bin/bash\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=Example_OpenMP_Job\n#SBATCH --time=0:20:0\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=12\n\n# Replace [budget code] below with your budget code (e.g. t01)\n#SBATCH --account=[budget code]\n# We use the \"standard\" partition as we are running on CPU nodes\n#SBATCH --partition=standard\n# We use the \"standard\" QoS as our runtime is less than 4 days\n#SBATCH --qos=standard\n\n# Change to the submission directory\ncd $SLURM_SUBMIT_DIR\n\n# Set the number of threads to the CPUs per task\n# and propagate `--cpus-per-task` value to srun\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport OMP_PLACES=cores\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Launch the parallel job\n#   Using 12 threads\n#   srun picks up the distribution from the sbatch options\nsrun --hint=nomultithread --distribution=block:block ./my_openmp_executable.x\n</code></pre> <p>This will run your executable <code>my_openmp_executable.x</code> in parallel on 12 threads. Slurm will allocate 1 CCD to your job and srun will place 12 threads (one per physical core).</p> <p>See above for a more detailed discussion of the different <code>sbatch</code> options</p>"},{"location":"user-guide/batch/#job-arrays","title":"Job arrays","text":"<p>The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data.</p> <p>Each job in a job array is called a subjob. The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs.</p> <p>All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob.</p>"},{"location":"user-guide/batch/#job-script-for-a-job-array","title":"Job script for a job array","text":"<p>As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 288 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob:</p> <pre><code>#!/bin/bash\n# Slurm job options (name, compute nodes, job time)\n\n#SBATCH --name=Example_Array_Job\n#SBATCH --time=04:00:00\n#SBATCH --exclusive\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=288\n#SBATCH --cpus-per-task=1\n#SBATCH --array=0-55\n\n# Replace [budget code] below with your budget code (e.g. t01)\n#SBATCH --account=[budget code]\n# We use the \"standard\" partition as we are running on CPU nodes\n#SBATCH --partition=standard\n# We use the \"standard\" QoS as our runtime is less than 4 days\n#SBATCH --qos=standard\n\n# Change to the submission directory\ncd $SLURM_SUBMIT_DIR\n\n# Set the number of threads to 1\n#   This prevents any threaded system libraries from automatically\n#   using threading.\nexport OMP_NUM_THREADS=1\n\nsrun --hint=nomultithread --distribution=block:block /path/to/exe $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"user-guide/batch/#submitting-a-job-array","title":"Submitting a job array","text":"<p>Job arrays are submitted using <code>sbatch</code> in the same way as for standard jobs:</p> <pre><code>sbatch job_script.slurm\n</code></pre>"},{"location":"user-guide/batch/#job-chaining","title":"Job chaining","text":"<p>Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps.</p> <p>Note</p> <p>The <code>--parsable</code> option to <code>sbatch</code> can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands.</p> <p>For example:</p> <pre><code>jobid=$(sbatch --parsable first_job.sh)\nsbatch --dependency=afterok:$jobid second_job.sh\n</code></pre> <p>or for a longer chain:</p> <pre><code>jobid1=$(sbatch --parsable first_job.sh)\njobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh)\njobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh)\nsbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh\n</code></pre>"},{"location":"user-guide/batch/#interactive-jobs","title":"Interactive Jobs","text":"<p>When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run small/short MPI jobs. However, you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One way to achieve this on Cirrus is to use an interactive jobs.</p> <p>Interactive jobs via SLURM take two slightly different forms. The first uses <code>srun</code> directly to allocate resource to be used interactively; the second uses both <code>salloc</code> and <code>srun</code>.</p>"},{"location":"user-guide/batch/#using-srun","title":"Using srun","text":"<p>An interactive job via <code>srun</code> allows you to execute commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal.</p> <p>A convenient way to do this is as follows.</p> <pre><code>[user@login04 ~]$ srun --exclusive --nodes=1 --time=00:20:00 --partition=standard --qos=standard --account=z04 --pty /usr/bin/bash --login\n</code></pre> <p>This requests the exclusive use of one node for the given time (here, 20 minutes). The <code>--pty /usr/bin/bash --login</code> requests an interactive login shell be started. (Note the prompt has changed.) Interactive commands can then be used as normal and will execute on the compute node. When no longer required, you can type <code>exit</code> or CTRL-D to release the resources and return control to the front end shell.</p> <pre><code>[user@cs-n0035]$ exit\nlogout\n[user@login04 ~]$\n</code></pre> <p>Note that the new interactive shell will reflect the environment of the original login shell. If you do not wish this, add the <code>--export=none</code> argument to <code>srun</code> to provide a clean login environment.</p> <p>Within an interactive job, one can use <code>srun</code> to launch parallel jobs in the normal way, e.g.,</p> <pre><code>[user@cs-n0035]$ srun -n 2 ./a.out\n</code></pre>"},{"location":"user-guide/batch/#using-salloc-with-srun","title":"Using <code>salloc</code> with <code>srun</code>","text":"<p>This approach uses the<code>salloc</code> command to reserve compute nodes and then <code>srun</code> to launch relevant work.</p> <p>To submit a request for a job reserving 2 nodes (72 physical cores) for 1 hour you would issue the command:</p> <pre><code>[user@login04 ~]$ salloc --exclusive --nodes=2 --tasks-per-node=288 --cpus-per-task=1 --time=01:00:00  --partition=standard --qos=standard --account=t01\nsalloc: Granted job allocation 8699\nsalloc: Waiting for resource configuration\nsalloc: Nodes cs-n00[35-36] are ready for job\n[user@login04 ~]$\n</code></pre> <p>Note that this starts a new shell on the login node associated with the allocation (the prompt has not changed). The allocation may be released by exiting this new shell.</p> <pre><code>[user@login04 ~]$ exit\nsalloc: Relinquishing job allocation 8699\n[user@login04 ~]$\n</code></pre> <p>While the allocation lasts you will be able to run parallel jobs on the compute nodes by issuing the <code>srun</code> command in the normal way. The resources available are those specified in the original <code>salloc</code> command. For example, with the above allocation,</p> <pre><code>[user@login04 ~]$ srun --hint=nomultithread --distribution=block:block ./mpi-code.out\n</code></pre> <p>will run 576 MPI tasks per node on two nodes.</p> <p>If your allocation reaches its time limit, it will automatically be termintated and the associated shell will exit. To check that the allocation is still running, use <code>squeue</code>:</p> <pre><code>[user@cirrus-login1]$ squeue -u user\n           JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n            8718  standard     bash    user   R       0:07      2 r1i7n[18-19]\n</code></pre> <p>Choose a time limit long enough to allow the relevant work to be completed.</p> <p>The <code>salloc</code> method may be useful if one wishes to associate operations on the login node with work in the allocation itself.</p>"},{"location":"user-guide/batch/#reservations","title":"Reservations","text":"<p>Reservations are available on Cirrus. These allow users to reserve a number of nodes for a specified length of time starting at a particular time on the system.</p> <p>Reservations require justification. They will only be approved if the request could not be fulfilled with the standard queues. For example, you require a job/jobs to run at a particular time e.g. for a demonstration or course.</p> <p>Note</p> <p>Reservation requests must be submitted at least 120 hours in advance of   the reservation start time. We cannot guarantee to meet all reservation   requests due to potential conflicts with other demands on the service   but will do our best to meet all requests.</p> <p>Important</p> <p>The minimum unit of reservation is a full node (288 cores). It is not possible to request reservations smaller than this. Reservation sizes are in increments of full nodes.</p> <p>Reservations will be charged at 1.5 times the usual rate and our policy is that they will be charged the full rate for the entire reservation at the time of booking, whether or not you use the nodes for the full time. In addition, you will not be refunded the compute time if you fail to use them due to a job crash unless this crash is due to a system failure.</p> <p>To request a reservation you complete a form on SAFE:</p> <ol> <li>[Log into SAFE](https://safe.epcc.ed.ac.uk)</li> <li>Under the \"Login accounts\" menu, choose the \"Request reservation\"      option</li> </ol> <p>On the first page, you need to provide the following:</p> <ul> <li>The start time and date of the reservation.</li> <li>The end time and date of the reservation.</li> <li>Your justification for the reservation -- this must be provided or    the request will be rejected.</li> <li>The number of nodes required.</li> </ul> <p>On the second page, you will need to specify which username you wish the reservation to be charged against and, once the username has been selected, the budget you want to charge the reservation to. (The selected username will be charged for the reservation but the reservation can be used by all members of the selected budget.)</p> <p>Your request will be checked by the Cirrus User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add <code>--reservation=&lt;reservation ID&gt;</code> and <code>--qos=reservation</code> options to your job submission script or Slurm job submission command.</p> <p>Tip</p> <p>You can submit jobs to a reservation as soon as the reservation has been   set up; jobs will remain queued until the reservation starts.</p>"},{"location":"user-guide/batch/#serial-jobs","title":"Serial jobs","text":"<p>Unlike parallel jobs, serial jobs will generally not need to specify the number of nodes and exclusive access (unless they want access to all of the memory on a node). You usually only need the <code>--ntasks=1</code> specifier. For example, a serial job submission script could look like:</p> <pre><code>#!/bin/bash\n\n# Slurm job options (name, compute nodes, job time)\n#SBATCH --job-name=Example_Serial_Job\n#SBATCH --time=0:20:0\n#SBATCH --ntasks=1\n\n# Replace [budget code] below with your budget code (e.g. t01)\n#SBATCH --account=[budget code]\n# We use the \"standard\" partition as we are running on CPU nodes\n#SBATCH --partition=standard\n# We use the \"standard\" QoS as our runtime is less than 4 days\n#SBATCH --qos=standard\n\n# Change to the submission directory\ncd $SLURM_SUBMIT_DIR\n\n# Enforce threading to 1 in case underlying libraries are threaded\nexport OMP_NUM_THREADS=1\n\n# Launch the serial job\n#   Using 1 thread\nsrun ./my_serial_executable.x\n</code></pre> <p>Note</p> <p>Remember that you will be allocated memory based on the number of tasks (i.e. CPU cores) that you request. You will get ~2.7 GB per task/core on  standard memory nodess and ~5.3 GB per task/core on high memory nodes. If you need more than this for your serial job then you should ask for the number of tasks you need for the required memory (or use the <code>--exclusive</code> option to get access to all the memory on a node) and launch specifying a single task using <code>srun --ntasks=1 --cpu-bind=cores</code>.</p>"},{"location":"user-guide/batch/#temporary-files-and-tmp-in-batch-jobs","title":"Temporary files and <code>/tmp</code> in batch jobs","text":"<p>Applications which normally read and write temporary files from <code>/tmp</code> may require some care in batch jobs on Cirrus. As the size of <code>/tmp</code> on backend nodes is relatively small (\\&lt; 150 MB), applications should use a different location to prevent possible failures.</p> <p>Note also that the default value of the variable <code>TMPDIR</code> in batch jobs is a memory-resident file system location specific to the current job (typically in the <code>/dev/shm</code> directory). Files here reduce the available capacity of main memory on the node.</p> <p>Tip</p> <p>Applications should not hard-code specific locations such as <code>/tmp</code>. Parallel applications should further ensure that there are no collisions in temporary file names on separate processes/nodes.</p>"},{"location":"user-guide/connecting/","title":"Connecting to Cirrus","text":"<p>On the Cirrus system, interactive access can be achieved via SSH, either directly from a command line terminal or using an SSH client. In addition data can be transferred to and from the Cirrus system using <code>scp</code> from the command line or by using a file transfer client.</p> <p>Before following the process below, we assume you have set up an account on Cirrus through the EPCC SAFE. Documentation on how to do this can be found at:</p> <p>SAFE Guide for  Users</p> <p>This section covers the basic connection methods.</p> <p>Tip</p> <p>The current login address for Cirrus is <code>preview.cirrus.ac.uk</code>.</p>"},{"location":"user-guide/connecting/#access-credentials-mfa","title":"Access credentials: MFA","text":"<p>To access Cirrus, you need to use two credentials (this is known as multi-factor authentication or MFA): your SSH key pair, protected by a passphrase, and a time-based one-time passcode (sometimes known as a TOTP code). You can find more detailed instructions on how to set up your credentials to access Cirrus from Windows, macOS and Linux below.</p>"},{"location":"user-guide/connecting/#ssh-key-pairs","title":"SSH Key Pairs","text":"<p>You will need to generate an SSH key pair protected by a passphrase to access Cirrus.</p>"},{"location":"user-guide/connecting/#generate-ssh-key-pair","title":"Generate SSH key pair","text":"<p>Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key:</p> <pre><code>$ ssh-keygen -t rsa -C \"your@email.com\"\n</code></pre> <pre><code>Generating public/private rsa key pair.\nEnter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter]\nEnter passphrase (empty for no passphrase): [Passphrase]\nEnter same passphrase again: [Passphrase]\nYour identification has been saved in /Home/user/.ssh/id_rsa.\nYour public key has been saved in /Home/user/.ssh/id_rsa.pub.\nThe key fingerprint is:\n03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|    . ...+o++++. |\n| . . . =o..      |\n|+ . . .......o o |\n|oE .   .         |\n|o =     .   S    |\n|.    +.+     .   |\n|.  oo            |\n|.  .             |\n| ..              |\n+-----------------+\n</code></pre> <p>(remember to replace \"your@email.com\" with your e-mail address).</p>"},{"location":"user-guide/connecting/#upload-public-part-of-key-pair-to-safe","title":"Upload public part of key pair to SAFE","text":"<p>You should now upload the public part of your SSH key pair to the SAFE by following the instructions at:</p> <p>Login to SAFE. Then:</p> <ol> <li>Go to the Menu Login accounts and select the Cirrus account you      want to add the SSH key to</li> <li>On the subsequent Login account details page click the Add Credential      button</li> <li>Select SSH public key as the Credential Type and click Next</li> <li>Either copy and paste the public part of your SSH key into the      SSH Public key box or use the button to select the public key file      on your computer.</li> <li>Click Add to associate the public SSH key part with your account</li> </ol> <p>Once you have done this, your SSH key will be added to your Cirrus account.</p> <p>Tip</p> <p>You can upload multiple public keys to SAFE to associate with your login account. This can be helpful if you want to log into Cirrus from multiple machines - you can have a different SSH key pair on each machine.</p>"},{"location":"user-guide/connecting/#time-based-one-time-passcode-totp-code","title":"Time-based one-time passcode (TOTP code)","text":"<p>Remember, you will need to use both an SSH key and time-based one-time passcode (TOTP code) to log into Cirrus so you will also need to set up a method for generating a TOTP code before you can log into Cirrus. </p>"},{"location":"user-guide/connecting/#ssh-clients","title":"SSH Clients","text":"<p>Interaction with Cirrus is done remotely, over an encrypted communication channel, Secure Shell version 2 (SSH-2). This allows command-line access to one of the login nodes of a Cirrus, from which you can run commands or use a command-line text editor to edit files.</p> <p>Linux distributions, MacOS and Windows each come installed with a terminal application that can be use for SSH access to the login nodes.</p> <p>Linux users will have different terminals depending on their distribution and window manager (e.g. GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal.</p> <p>MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder.</p> <p>Windows users can use Powershell (which has an SSH client installed) or a WSL2 terminal.</p>"},{"location":"user-guide/connecting/#logging-in","title":"Logging in","text":"<p>You can use the following command from the terminal window to login into Cirrus:</p> <pre><code>ssh username@preview.cirrus.ac.uk\n</code></pre> <p>You will first be prompted for the passphrase associated with your SSH key pair. Once you have entered your passphrase successfully, you will then be prompted for your MFA TOTP. You need to enter both correctly to be able to access Cirrus.</p> <p>Note</p> <p>If your SSH key pair is not stored in the default location (usually <code>~/.ssh/id_rsa</code>) on your local system, you may need to specify the path to the private part of the key with the <code>-i</code> option to <code>ssh</code>. For example, if your key is in a file called <code>keys/id_rsa_cirrus</code> you would use the command <code>ssh -i keys/id_rsa_cirrus username@login.cirrus.ac.uk</code> to log in.</p>"},{"location":"user-guide/connecting/#host-keys","title":"Host Keys","text":"<p>Adding the host keys to your SSH configuration file provides an extra level of security for your connections to Cirrus. The host keys are checked against the login nodes when you login to Cirrus and if the remote server key does not match the one in the configuration file, the connection will be refused. This provides protection against potential malicious servers masquerading as the Cirrus login nodes.</p>"},{"location":"user-guide/connecting/#previewcirrusacuk","title":"preview.cirrus.ac.uk","text":"<pre><code>preview.cirrus.ac.uk ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFBvqlziPNvqf4DRbTMi5RsdJB6x7sw6zNZKxAE0klnchuWvX3feLslEteKazfQ6NmtJ/bOanbtxAOLXR/Fv9+E=\n\npreview.cirrus.ac.uk ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDEtumDZQYQFczLoAUOQRCkt+93mFEyTGtJLtOh8DXunogsQsc1S98XlaJMwn6j5LpAvUBzpdnqyDCOX3ksS+B6VvFc5A73c9SNt8Xvl+V8cW7V57fu/tK7PCWliboIF3WmL+cWTh5QGfbjLiOY6MZSjEBvJOPOZ+jS0K9jyUIIFuHOGGDpjq16n8FQTLcU/NXr5H1AJN61WCFWTdg4P5mcjwpVrbEes9gC6e5ecFXX616aLwAiSf4wTepVPy0YGCm5/QKpsbyRcAG6QA30+3SllvvC2sdOK0jqUnAFhBbeWaqBk51CmERek1+mwxjU/w0zs7ezDumhdbI40r2zHcfheT65k5NFhJkLoV7JxHxW2zOZoEAxIvt8cNojeMmZcYiBow0KWzfOdT80uoQySXsC1VWke1Nyw1a3gmDJ8HS8zhLBo+J840JIFjdGDDXLlp5+3oZAbv6whLab6lXRC2CkEtWKbzJmo0PDPadqjVChg3+ApCOuEgEOYsGj/vbPBDk=\n\npreview.cirrus.ac.uk ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAII0Rb8J6LEilTRiglRytJRw3+Ak2t1JTFcMsF5NBucPe\n</code></pre> <p>Host key verification can fail if this key is out of date, a problem which can be fixed by removing the offending entry in <code>~/.ssh/known_hosts</code> and replacing it with the new key published here.  We recommend users should check this page for any key updates and not just accept a new key from the server without confirmation.</p>"},{"location":"user-guide/connecting/#making-access-more-convenient-using-the-ssh-configuration-file","title":"Making access more convenient using the SSH configuration file","text":"<p>Typing in the full command to login or transfer data to Cirrus can become tedious as it often has to be repeated many times. You can use the SSH configuration file, usually located on your local machine at <code>.ssh/config</code> to make things a bit more convenient.</p> <p>Each remote site (or group of sites) can have an entry in this file which may look something like:</p> <pre><code>Host cirrus\n    HostName preview.cirrus.ac.uk\n    User username\n</code></pre> <p>(remember to replace <code>username</code> with your actual username!).</p> <p>The <code>Host cirrus</code> line defines a short name for the entry. In this case, instead of typing <code>ssh username@preview.cirrus.ac.uk</code> to access the Cirrus login nodes, you could use <code>ssh cirrus</code> instead. The remaining lines define the options for the <code>cirrus</code> host.</p> <ul> <li><code>Hostname preview.cirrus.ac.uk</code> - defines the full address of the host</li> <li><code>User username</code> - defines the username to use by default for this host   (replace <code>username</code> with your own username on the remote host)</li> </ul> <p>Now you can use SSH to access Cirrus without needing to enter your username or the full hostname every time:</p> <pre><code>$ ssh cirrus\n</code></pre> <p>You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config man page (or <code>man ssh_config</code> on any machine with SSH installed) for a description of the SSH configuration file. You may find the <code>IdentityFile</code> option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system.</p> <p>Note</p> <p>There is a known bug with Windows ssh-agent. If you get the error message: <code>Warning:  agent returned different signature type ssh-rsa (expected rsa-sha2-512)</code>, you will need to either specify the path to your ssh key in the command line (using the <code>-i</code> option as described above) or add the path to your SSH config file by using the <code>IdentityFile</code> option.</p>"},{"location":"user-guide/connecting/#accessing-cirrus-from-more-than-1-machine","title":"Accessing Cirrus from more than 1 machine","text":"<p>It is common for users to want to access Cirrus from more than one local machine (e.g. a desktop linux, and a laptop) - this can be achieved by adding multiple public keys to your account through SAFE as described above.</p>"},{"location":"user-guide/connecting/#ssh-forwarding-to-use-cirrus-from-a-second-remote-machine","title":"SSH forwarding (to use Cirrus from a second remote machine)","text":"<p>If you want to access Cirrus from a machine you already access remotely (e.g. to copy data from Cirrus onto a different cluster), you can forward your local Cirrus SSH keys so that you don't need to create a new key pair on the intermediate machine.</p> <p>If your local machine is MacOS or Linus, add your Cirrus SSH key to the SSH Agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n</code></pre> <p>(If you created your key with a different name, replace <code>id_rsa</code> in the command with the name of your private key file). You will be prompted for your SSH key's passphrase.</p> <p>You can then use the <code>-A</code> flag when connecting to your intermediate cluster:</p> <pre><code>ssh -A &lt;user&gt;@&lt;host&gt;\n</code></pre> <p>Once on the intermediate cluster, you should be able to SSH to Cirrus directly:</p> <pre><code>ssh &lt;user&gt;@preview.cirrus.ac.uk\n</code></pre>"},{"location":"user-guide/connecting/#ssh-debugging-tips","title":"SSH debugging tips","text":"<p>If you find you are unable to connect via SSH there are a number of ways you can try and diagnose the issue. Some of these are collected below - if you are having difficulties connecting we suggest trying these before contacting the Cirrus service desk.</p>"},{"location":"user-guide/connecting/#can-you-connect-to-the-login-node","title":"Can you connect to the login node?","text":"<p>Try the command <code>ping -c 3 preview.cirrus.ac.uk</code>. If you successfully connect to the login node, the output should include:</p> <pre><code>--- preview.dyn.cirrus.ac.uk ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 38ms\n</code></pre> <p>(the ping time '38ms' is not important). If not all packets are received there could be a problem with your internet connection, or the login node could be unavailable.</p>"},{"location":"user-guide/connecting/#ssh-key","title":"SSH key","text":"<p>If you get the error message <code>Permission denied (publickey)</code> this can indicate a problem with your SSH key. Some things to check:</p> <ul> <li> <p>Have you uploaded the key to SAFE? Please note that if the same key    is reuploaded SAFE will not map the \"new\" key to cirrus. If for some   reason this is required, please delete the key first, then reupload.</p> </li> <li> <p>Is ssh using the correct key? You can check which keys are being   found and offered by ssh using <code>ssh -vvv</code>. If your private key has a   non-default name you can use the <code>-i</code> flag to provide it to ssh,    i.e. <code>ssh -i path/to/key username@preview.cirrus.ac.uk</code>.</p> </li> <li> <p>Are you entering the passphrase correctly? You will be asked for    your private key's passphrase first. If you enter it incorrectly you    will usually be asked to enter it again, and usually up to three    times in total, after which ssh will fail with    <code>Permission denied (publickey)</code>. If you would like to confirm your    passphrase without attempting to connect, you can use    <code>ssh-keygen -y -f /path/to/private/key</code>. If successful, this command   will print the corresponding public key. You can also use this to    check it is the one uploaded to SAFE.</p> </li> <li> <p>Are permissions correct on the ssh key? One common issue is that the    permissions are incorrect on the either the key file, or the    directory it's contained in. On Linux/MacOS for example, if your    private keys are held in <code>~/.ssh/</code> you can check this with    <code>ls -al ~/.ssh</code>. This should give something similar to the following    output:</p> </li> </ul> <pre><code>    $ ls -al ~/.ssh/\n    drwx------.  2 user group    48 Jul 15 20:24 .\n    drwx------. 12 user group  4096 Oct 13 12:11 ..\n    -rw-------.  1 user group   113 Jul 15 20:23 authorized_keys\n    -rw-------.  1 user group 12686 Jul 15 20:23 id_rsa\n    -rw-r--r--.  1 user group  2785 Jul 15 20:23 id_rsa.pub\n    -rw-r--r--.  1 user group  1967 Oct 13 14:11 known_hosts\n</code></pre> <p>The important section here is the string of letters and dashes at    the start, for the lines ending in <code>.</code>, <code>id_rsa</code>, and <code>id_rsa.pub</code>,    which indicate permissions on the containing directory, private key,    and public key respectively. If your permissions are not correct,    they can be set with <code>chmod</code>. Consult the table below for the    relevant <code>chmod</code> command. On Windows, permissions are handled    differently but can be set by right-clicking on the file and    selecting Properties &gt; Security &gt; Advanced. The user, SYSTEM, and    Administrators should have <code>Full control</code>, and no other permissions    should exist for both public and private key files, and the    containing folder.</p> Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 <p><code>chmod</code> can be used to set permissions on the target in the following way: <code>chmod &lt;code&gt; &lt;target&gt;</code>. So for example to set correct permissions on the private key file <code>id_rsa_cirrus</code> one would use the command <code>chmod 600 id_rsa_cirrus</code>.</p> <p>Note</p> <p>Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users, (owning) groups, and others. The available permissions are read, write, and execute. </p> <p>The first character indicates whether the target is a file <code>-</code>, or directory <code>d</code>. The next three characters indicate the owning user's permissions. The first character is <code>r</code> if they have read permission, <code>-</code> if they don't, the second character is <code>w</code> if they have write permission, <code>-</code> if they don't, the third character is <code>x</code> if they have execute permission, <code>-</code> if they don't. This pattern is then repeated for group, and other permissions. </p> <p>For example the pattern <code>-rw-r--r--</code> indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The <code>chmod</code> codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string <code>-rwx------</code> becomes <code>111 000 000</code> -&gt; <code>700</code>.</p>"},{"location":"user-guide/connecting/#mfa","title":"MFA","text":"<p>If your TOTP passcode is being consistently rejected, you can remove MFA from your account and then re-enable it.</p>"},{"location":"user-guide/connecting/#ssh-verbose-output","title":"SSH verbose output","text":"<p>Verbose debugging output from <code>ssh</code> can be very useful for diagnosing the issue. In particular, it can be used to distinguish between problems with the SSH key and password - further details are given below. To enable verbose output add the <code>-vvv</code> flag to your SSH command. For example:</p> <pre><code>ssh -vvv username@preview.cirrus.ac.uk\n</code></pre> <p>The output is lengthy, but somewhere in there you should see lines similar to the following:</p> <pre><code>debug1: Next authentication method: publickey\ndebug1: Offering public key: RSA SHA256:&lt;key-hash&gt; &lt;path_to_private_key&gt;\ndebug3: send_pubkey_test\ndebug3: send packet: type 50\ndebug2: we sent a publickey packet, wait for reply\ndebug3: receive packet: type 60\ndebug1: Server accepts key: pkalg ssh-rsa vlen 2071\ndebug2: input_userauth_pk_ok: fp SHA256:&lt;key-hash&gt;\ndebug3: sign_and_send_pubkey: RSA SHA256:&lt;key-hash&gt;\nEnter passphrase for key '&lt;path_to_private_key&gt;':\ndebug3: send packet: type 50\ndebug3: receive packet: type 51\nAuthenticated with partial success.\n</code></pre> <p>Most importantly, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line <code>Authenticated with partial success</code> indicates that the SSH key has been accepted, and you will next be asked for your password. By default ssh will go through a list of standard private key files, as well as any you have specified with <code>-i</code> or a config file. This is fine, as long as one of the files mentioned is the one that matches the public key uploaded to SAFE.</p> <p>If you do not see <code>Authenticated with partial success</code> anywhere in the verbose output, consider the suggestions under SSH key above. If you do, but are unable to connect, consider the suggestions under Password above.</p> <p>The equivalent information can be obtained in PuTTY or MobaXterm by enabling all logging in settings.</p>"},{"location":"user-guide/connecting/#default-shell-environment","title":"Default shell environment","text":"<p>Usually, when a new login shell is created, the commands on <code>$HOME/.bashrc</code> are executed. This tipically includes setting user-defined alias, changing environment variables, and, in the case of an HPC system, loading modules.</p> <p>Cirrus does not currently read the <code>$HOME/.bashrc</code> file, but it does read the <code>$HOME/.bash_profile</code> file, so, if you wish to read a <code>$HOME/.bashrc</code> file, you can add the following to your <code>$HOME/.bash_profile</code> file (or create one, if it doesn't exist):</p> <pre><code>$ $HOME/.bash_profile\n$ load $HOME/.bashrc, if it exists\nif [ -f $HOME/.bashrc ]; then\n        . $HOME/.bashrc\nfi\n</code></pre>"},{"location":"user-guide/containers/","title":"Using Containers","text":"<p>This page was originally based on the documentation at the University of Sheffield HPC service.</p> <p>Designed around the notion of mobility of compute and reproducible science, containers enable users to have full control of their operating system environment. This means that a non-privileged user can \"swap out\" the Linux operating system and environment on the host for a Linux OS and environment that they control. So if the host system is running CentOS Linux but your application runs in Ubuntu Linux with a particular software stack, you can create an Ubuntu image, install your software into that image, copy the image to another host (e.g. Cirrus), and run your application on that host in its native Ubuntu environment.</p> <p>Containers also allow you to leverage the resources of whatever host you are on. This includes high-speed interconnects (e.g. Infiniband), file systems (e.g. Lustre) and potentially other resources.</p>"},{"location":"user-guide/containers/#apptainer","title":"Apptainer","text":"<p>The container software supported on Cirrus is Apptainer.</p> <p>Note</p> <p>Apptainer only supports Linux containers. You cannot create images that use Windows or macOS (this is a restriction of the containerisation model rather than of Apptainer).</p>"},{"location":"user-guide/containers/#useful-links","title":"Useful Links","text":"<ul> <li>Apptainer website</li> <li>Apptainer documentation</li> </ul>"},{"location":"user-guide/containers/#about-apptainer-container-images","title":"About Apptainer container images","text":"<p>Similar to Docker or Podman, an Apptainer container image (or, more commonly, image) is a self-contained software stack. As Apptainer does not require a root-level daemon to run its images (as is required by Docker) it is suitable for use on a multi-user HPC system such as Cirrus. Within a  running container created from the container image, you have exactly the same permissions as you do in a standard login session on the system.</p> <p>In principle, this means that a container image created on your local machine with all your research software installed for local development will also run on Cirrus.</p> <p>Pre-built container images (such as those on DockerHub or Quay.io can simply be downloaded and used on Cirrus (or anywhere else Singularity is installed).</p> <p>Creating and modifying container images requires root permission and so must be done on a system where you have such access (in practice, this is usually your laptop/workstation using Docker or Podman).</p>"},{"location":"user-guide/containers/#using-container-images-on-cirrus","title":"Using container images on Cirrus","text":"<p>Container images can be used on Cirrus in a number of ways.</p> <ol> <li>Interactively on the login nodes</li> <li>Interactively on compute nodes</li> <li>As serial processes within a non-interactive batch script</li> <li>As parallel processes within a non-interactive batch script</li> </ol> <p>We provide information on each of these scenarios. First, we describe briefly how to get existing container images onto Cirrus and converted to the Apptainer format so that you can use them.</p>"},{"location":"user-guide/containers/#getting-existing-container-images-onto-cirrus","title":"Getting existing container images onto Cirrus","text":"<p>Container images are most usually downloaded onto Cirrus from a  container image repository (such as Dockerhub or Quay.io) so we  discuss that mechanism in detail. If you already have your  container images in Apptainer format as an image file then you  can copy these to Cirrus in the same way as any other file.</p> <p>Singularity images are simply files, so if you already have an image file, you can use <code>scp</code> to copy the file to Cirrus as you would with any other file.</p> <p>To fetch a container image from a container image repository and convert to an Apptainer container image file on the fly, you can  use a command such as:</p> <pre><code>apptainer build hello_world.sif docker://hub.docker.com/hello-world\n</code></pre> <p>This will download the \"hello-world\" container image from DockerHub and save it as an Apptainer image file in the file <code>hello-world.sif</code>.</p>"},{"location":"user-guide/containers/#interactive-use-on-the-login-nodes","title":"Interactive use on the login nodes","text":"<p>To run a container created from a container image file on the login node you would use (note mention of Docker as this was downloaded from Docker Hub, however we are running the container using Apptainer):</p> <pre><code>[user@login01 ~]$ apptainer run hello-world.sif \nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n</code></pre> <p>We can also run a container created from the container image file and get an interactive shell prompt in the running container:</p> <pre><code>[user@login01 ~]$ apptainer shell hello-world.sif\nApptainer&gt; ls /\nbin  boot  dev  environment  etc  home  lib  lib64  lustre  media  mnt  opt  proc  rawr.sh  root  run  sbin  singularity  srv  sys  tmp  usr  var\nApptainer&gt; exit\nexit\n[user@login01 ~]$ \n</code></pre>"},{"location":"user-guide/containers/#interactive-use-on-the-compute-nodes","title":"Interactive use on the compute nodes","text":"<p>The process for using an image interactively on the compute nodes is very similar to that for using them on the login nodes. The only difference is that you first have to submit an interactive serial job to get interactive access to the compute node.</p> <p>First though, move to a suitable location on <code>/work</code> and copy the <code>hello-world</code> container image file. This step is necessary as the compute nodes do not have access to the <code>/home</code> file system. (This step assumes you already downloaded the image as described above, if you have not done this you can download directly to the <code>/work</code> file system by following the instructions above.)</p> <pre><code>[user@login01 ~]$ cd ${HOME/home/work}\n[user@login01 ~]$ cp ~/hello-world.sif .\n</code></pre> <p>Now reserve a full node to work on interactively by issuing an <code>salloc</code> command, see below.</p> <pre><code>[user@login01 ~]$ salloc --exclusive --nodes=1 \\\n    --tasks-per-node=288 --cpus-per-task=1 --time=00:20:00 \\\n    --partition=standard --qos=standard --account=[budget code] \nsalloc: Pending job allocation 14507\nsalloc: job 14507 queued and waiting for resources\nsalloc: job 14507 has been allocated resources\nsalloc: Granted job allocation 14507\nsalloc: Waiting for resource configuration\nsalloc: Nodes cs-n0030 are ready for job\n[user@login01 ~]$ ssh cs-n0030\n</code></pre> <p>Tip</p> <p>You will need to setup an SSH key pair and upload the public part    to SAFE in the usual way to allow SSH login from login    nodes to compute nodes.</p> <p>Note the prompt has changed to show you are on a compute node. Once you are logged in to the compute node, move to a suitable location on <code>/work</code> as before. You can now create a container from the <code>hello-world.sif</code> container image file in the same way you did on the login node.</p> <pre><code>[user@cs-n0030 ~]$ cd ${HOME/home/work}\n[user@cs-n0030 ~]$ apptainer shell hello-world.sif\nApptainer&gt; exit\nexit\n[user@cs-n0030 ~]$ exit\nlogout\nConnection to cs-n0030 closed.\n[user@login01 ~]$ exit\nexit\nsalloc: Relinquishing job allocation 14507\nsalloc: Job allocation 14507 has been revoked.\n[user@login01 ~]$\n</code></pre> <p>We used <code>exit</code> to leave the interactive container shell and then called <code>exit</code> twice more to close the interactive job on the compute node.</p>"},{"location":"user-guide/containers/#serial-processes-within-a-non-interactive-batch-script","title":"Serial processes within a non-interactive batch script","text":"<p>You can also use Apptainer to run containers within a non-interactive batch script as you would any other command. If your image contains a runscript then you can use <code>apptainer run</code> to execute the runscript in the job. You can also use <code>apptainer exec</code> command to execute arbitrary commands (or scripts) within the image.</p> <p>An example job submission script to run a serial job that executes the runscript within the <code>hello-world.sif</code> container image file we downloaded above on Cirrus would be as follows. Assuming we submit from the same directory that the <code>ello-world.sif</code> file is stored in/</p> <pre><code>#!/bin/bash --login\n\n# job options (name, compute nodes, job time)\n#SBATCH --job-name=hello-world\n#SBATCH --ntasks=1\n#SBATCH --exclusive\n#SBATCH --time=0:20:0\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n\n# Run the serial executable\nsrun --cpu-bind=cores apptainer run hello-world.sif\n</code></pre> <p>Submit this script using the <code>sbatch</code> command and once the job has finished, you should see the same output you got interactively in the Slurm output file when the job finishes.</p>"},{"location":"user-guide/containers/#parallel-processes-within-a-non-interactive-batch-script","title":"Parallel processes within a non-interactive batch script","text":"<p>Running a container in parallel on the compute nodes using Apptainer is not too different from launching a normal parallel application.  he submission script below shows how the <code>srun</code> command can be used along with Apptainer to run a container created from a container  image file in parallel.</p> <pre><code>#!/bin/bash --login\n\n# job options (name, compute nodes, job time)\n#SBATCH --job-name=[name of application]\n#SBATCH --nodes=4\n#SBATCH --tasks-per-node=36\n#SBATCH --cpus-per-task=1\n#SBATCH --exclusive\n#SBATCH --time=0:20:0\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n\n# The host bind paths for the Singularity container.\nBIND_ARGS=/work/y07/shared/cirrus-ex,/path/to/input/files\n\n# The file containing environment variable settings that will allow\n# the container to find libraries on the host, e.g., LD_LIBRARY_PATH . \nENV_PATH=/path/to/container/environment/file\n\nIMAGE_PATH=/path/to/apptainer/image/file\n\nAPP_PATH=/path/to/containerized/application/executable\nAPP_PARAMS=[application parameters]\n\nsrun --distribution=block:block --hint=nomultithread \\\n    singularity exec --bind ${BIND_ARGS} --env-file ${ENV_PATH} ${IMAGE_PATH}\n        ${APP_PATH} ${APP_PARAMS}\n</code></pre> <p>The script above runs a containerised application such that each of the four nodes requested is fully populated. In general, the containerised application's input and output will be read from and written to a location on the host; hence, it is necessary to pass a suitable bind path to the <code>apptainer</code> command (<code>/path/to/input/files</code>).</p> <p>Note</p> <p>The paths in the submission script that begin <code>/path/to</code> should be provided by the user. All but one of these paths are locations on the host (rather than in the running container). The exception being <code>APP_PATH</code>, which should be given a path relative to the container file system.</p>"},{"location":"user-guide/containers/#creating-your-own-apptainer-container-images","title":"Creating your own Apptainer container images","text":"<p>As we saw above, you can create Apptainer container image files by importing from DockerHub or other repositories on Cirrus itself. If you wish to create your own custom container image to use with Apptainer then you must use a system where you have root (or administrator) privileges - often your own laptop or workstation.</p> <p>There are a number of different options to create container images on your local system to use with Apptainer on Cirrus. We are going to use Podman on our  local system to create the container image, push the new container image to Docker Hub and then use Apptainer on Cirrus to convert the Docker container image to an Apptainer container image file.</p> <p>For macOS and Windows users we recommend installing Podman Desktop. For Linux users, we recommend installing Podman directly on your local system. See the Podman documentation for full details on how to install Podman Desktop/Podman.</p>"},{"location":"user-guide/containers/#building-container-images-using-podman","title":"Building container images using Podman","text":"<p>Note</p> <p>We assume that you are familiar with using Podman/Docker in these instructions. You  can find an introduction to Docker at Reproducible Computational Environments Using Containers: Introduction to Docker. Podman uses very similar commands to Docker.</p> <p>As usual, you can build container images with a command similar to:</p> <pre><code>podman build --platform linux/amd64 -t &lt;username&gt;/&lt;image name&gt;:&lt;version&gt; .\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;username&gt;</code> is your Docker Hub username</li> <li><code>&lt;image name&gt;</code> is the name of the container image you wish to create</li> <li><code>&lt;version&gt;</code> - specifies the version of the image you are creating (e.g. \"latest\", \"v1\")</li> <li><code>.</code> is the build context - in this example it is the location of the Dockerfile</li> </ul> <p>Note, you should use the <code>--platform linux/amd64</code> option to ensure that the container image is compatible with the processor architecture on Cirrus.</p>"},{"location":"user-guide/containers/#using-apptainer-with-mpi-on-cirrus","title":"Using Apptainer with MPI on Cirrus","text":"<p>MPI on Cirrus is provided by the Cray MPICH libraries with the interface to the high-performance Slingshot interconnect provided via the OFI interface. Therefore, as per the Apptainer MPI Hybrid model, we will build our container image such that it contains a version of the MPICH MPI library compiled with support for OFI. Below, we provide instructions on creating a container image with a version of MPICH compiled in this way. We then provide an  example of how to run an Apptainer container with MPI over multiple  Cirrus compute nodes.</p>"},{"location":"user-guide/containers/#building-an-image-with-mpi-from-scratch","title":"Building an image with MPI from scratch","text":"<p>Warning</p> <p>Remember, all these steps should be executed on your local system where you have administrator privileges and Podman installed, not on Cirrus.</p> <p>We will illustrate the process of building an Apptainer container image with MPI from scratch by building a container image that contains MPI provided by MPICH and the OSU MPI benchmarks. As part of the container image creation we need to download the source code for both MPICH and the OSU benchmarks. At the time of writing, the stable MPICH 3 release is 3.4.3 and the stable OSU benchmark release is 7.5.1 - this may have changed by the time you are following these instructions.</p> <p>First, create a Dockerfile that describes how to build the image:</p> <pre><code>FROM ubuntu:22.04\n\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install the necessary packages (from repo)\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n apt-utils \\\n build-essential \\\n curl \\\n libcurl4-openssl-dev \\\n libzmq3-dev \\\n pkg-config \\\n software-properties-common\nRUN apt-get clean\nRUN apt-get install -y dkms\nRUN apt-get install -y autoconf automake build-essential numactl libnuma-dev autoconf automake gcc g++ git libtool\n\n# Download and build an ABI compatible MPICH\nRUN curl -sSLO http://www.mpich.org/static/downloads/3.4.2/mpich-3.4.2.tar.gz \\\n   &amp;&amp; tar -xzf mpich-3.4.2.tar.gz -C /root \\\n   &amp;&amp; cd /root/mpich-3.4.2 \\\n   &amp;&amp; ./configure --prefix=/usr --with-device=ch4:ofi --disable-fortran \\\n   &amp;&amp; make -j8 install \\\n   &amp;&amp; cd / \\\n   &amp;&amp; rm -rf /root/mpich-3.4.2 \\\n   &amp;&amp; rm /mpich-3.4.2.tar.gz\n\n# OSU benchmarks\nRUN curl -sSLO http://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-7.5.1.tar.gz \\\n   &amp;&amp; tar -xzf osu-micro-benchmarks-7.5.1.tar.gz -C /root \\\n   &amp;&amp; cd /root/osu-micro-benchmarks-7.5.1 \\\n   &amp;&amp; ./configure --prefix=/usr/local CC=/usr/bin/mpicc CXX=/usr/bin/mpicxx \\\n   &amp;&amp; make -j8 install \\\n   &amp;&amp; cd / \\\n   &amp;&amp; rm -rf /root/osu-micro-benchmarks-7.5.1 \\\n   &amp;&amp; rm /osu-micro-benchmarks-7.5.1.tar.gz\n\n# Add the OSU benchmark executables to the PATH\nENV PATH=/usr/local/libexec/osu-micro-benchmarks/mpi/startup:$PATH\nENV PATH=/usr/local/libexec/osu-micro-benchmarks/mpi/pt2pt:$PATH\nENV PATH=/usr/local/libexec/osu-micro-benchmarks/mpi/collective:$PATH\nENV OSU_DIR=/usr/local/libexec/osu-micro-benchmarks/mpi\n\n# path to mlx IB libraries in Ubuntu\nENV LD_LIBRARY_PATH=/usr/lib/libibverbs:$LD_LIBRARY_PATH\n</code></pre> <p>A quick overview of what the above Dockerfile is doing:</p> <ul> <li>The image is being bootstrapped from the <code>ubuntu:22.04</code> Docker image (this contains GLIBC compatible with the Cirrus Linux kernel).</li> <li>The first set of <code>RUN</code> sections with <code>apt-get</code> commands: install the base packages required from the Ubuntu package repos</li> <li>MPICH install: downloads and compiles the MPICH 3.4.3 in a way that is compatible with Cray MPICH on ARCHER2</li> <li>OSU MPI benchmarks install: downloads and compiles the OSU micro benchmarks</li> <li><code>ENV</code> sections: add the OSU benchmark executables to the PATH so they can be executed in the container without specifying the full path; set the correct paths to the network libraries within the container.</li> </ul> <p>Now we can go ahead and build the container image using Podman (this assumes that you issue the command in the same directory as the Dockerfile you created based on the specification above):</p> <pre><code>podman build --platform linux/amd64 -t auser/osu-benchmarks:7.5.1 .\n</code></pre> <p>(Remember to change <code>auser</code> to your Dockerhub username.)</p> <p>Once you have successfully built your container image, you should push it to Dockerhub:</p> <pre><code>podman push auser/osu-benchmarks:7.5.1\n</code></pre> <p>Finally, you need to use Apptainer on Cirrus to convert the Docker container image to an Apptainer container image file. Log into Cirrus, move to the work file system and then use a command like:</p> <pre><code>auser@login01:/work/t01/t01/auser&gt; apptainer build osu-benchmarks-7.5.1.sif docker://auser/osu-benchmarks:7.5.1\n</code></pre> <p>Tip</p> <p>You can find a copy of the <code>osu-benchmarks_7.5.1.sif</code> image on Cirrus in the directory <code>$EPCC_CONTAINER_DIR</code> if you do not want to build it yourself but still want to test.</p>"},{"location":"user-guide/containers/#running-parallel-mpi-jobs-using-apptainer-containers","title":"Running parallel MPI jobs using Apptainer containers","text":"<p>Tip</p> <p>These instructions assume you have built an Apptainer container image file on  Cirrus that includes MPI provided by MPICH with the OFI interface. See the sections above for how to build such container images.</p> <p>Once you have built your Apptainer container image file that includes MPICH built with OFI for ARCHER2, you can use it to run parallel jobs in a similar way to non-Apptainer jobs. The example job submission script below uses the container image file we built above with MPICH and the OSU benchmarks to run the Allreduce benchmark on two nodes where all 288 cores on each node are used for MPI processes (so, 576 MPI processes in total).</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=apptainer_parallel\n#SBATCH --time=0:10:0\n#SBATCH --exclusive\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=288\n#SBATCH --cpus-per-task=1\n\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n#SBATCH --account=[budget code]\n\n# Load the module to make the Cray MPICH ABI available\nmodule load cray-mpich-abi/8.1.32\n\nexport OMP_NUM_THREADS=1\nexport SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\n\n# Set the LD_LIBRARY_PATH environment variable within the container\n# to ensure that it used the correct MPI libraries.\nexport APPTAINERENV_LD_LIBRARY_PATH=\"/opt/cray/pe/mpich/8.1.32/ofi/gnu/11.2/lib-abi-mpich:/opt/cray/libfabric/1.22.0/lib64:/opt/cray/pals/1.6/lib:/opt/cray/pe/lib64:/opt/xpmem/lib64:/lib64\"\n\n# This makes sure HPE Cray Slingshot interconnect libraries are available\n# from inside the container.\nexport APPTAINER_BIND=\"/opt/cray,/var/spool,/opt/cray/pe/mpich/8.1.32/ofi/gnu/11.2/lib-abi-mpich,/etc/host.conf,/etc/libibverbs.d/mlx5.driver,/etc/libnl/classid,/etc/resolv.conf,/opt/cray/libfabric/1.22.0/lib64/libfabric.so.1,/lib64/libatomic.so.1,/lib64/libgcc_s.so.1,/lib64/libgfortran.so.5,/lib64/libquadmath.so.0,/opt/cray/pals/1.6/lib/libpals.so.0,/opt/cray/pe/lib64/libpmi2.so.0,/opt/cray/pe/lib64/libpmi.so.0,/opt/xpmem/lib64/libxpmem.so.0,/run/munge/munge.socket.2,/lib64/libmunge.so.2,/lib64/libnl-3.so.200,/lib64/libnl-genl-3.so.200,/lib64/libnl-route-3.so.200,/lib64/librdmacm.so.1,/lib64/libcxi.so.1,/lib64/libm.so.6\"\n\n# Launch the parallel job.\nsrun --hint=nomultithread --distribution=block:block \\\n    apptainer run osu-benchmarks-7.5.1.sif \\\n        osu_allreduce\n</code></pre> <p>The only changes from a standard submission script are:</p> <ul> <li>We set the environment variable <code>APPTAINER_LD_LIBRARY_PATH</code> to ensure that the excutable can find the correct libraries are available within the container to be able to use HPE Cray Slingshot interconnect.</li> <li>We set the environment variable <code>APPTAINER_BIND</code> to ensure that the correct libraries are available within the container to be able to use HPE Cray Slingshot interconnect.</li> <li><code>srun</code> calls the <code>apptainer</code> software with the container image file we created rather than the parallel program directly.</li> </ul> <p>Important</p> <p>Remember that the image file must be located on <code>/work</code> to run jobs on the compute nodes.</p> <p>If the job runs correctly, you should see output similar to the following in your <code>slurm-*.out</code>  file:</p> <pre><code>Lmod is automatically replacing \"cray-mpich/8.1.32\" with\n\"cray-mpich-abi/8.1.32\".\n\n\n# OSU MPI Allreduce Latency Test v7.5\n# Datatype: MPI_INT.\n# Size       Avg Latency(us)\n4                      10.05\n8                      10.84\n16                     11.19\n32                     11.49\n64                     13.08\n128                    17.09\n256                    22.97\n512                    22.23\n1024                   23.24\n2048                   26.16\n4096                   60.79\n8192                   75.56\n16384                  80.11\n32768                 120.08\n65536                 214.08\n131072                378.17\n262144                764.93\n524288                515.50\n1048576              1064.92\n</code></pre>"},{"location":"user-guide/data/","title":"Data Management and Transfer","text":"<p>This section covers the storage and file systems available on the system and the different ways that you can transfer data to and from Cirrus.</p> <p>In all cases of data transfer, users should use the Cirrus login nodes.</p>"},{"location":"user-guide/data/#cirrus-file-systems-and-storage","title":"Cirrus file systems and storage","text":"<p>There are two different data storage types available to users:</p> <ul> <li>Home file system (CephFS)</li> <li>Work file systems (Lustre)</li> </ul> <p>Each type of storage has different characteristics and policies, and is suitable for different types of use.</p> <p>There are also two different types of node available to users:</p> <ul> <li>Login nodes</li> <li>Compute nodes</li> </ul> <p>Each type of node sees a different combination of the storage types. The following table shows which storage options are available on different node types:</p> Storage Login nodes Compute nodes Notes Home yes no No backup Work yes yes No backup"},{"location":"user-guide/data/#home-file-system","title":"Home file system","text":"<p>Important</p> <p>There are no backups of any data on the home file system. You should ensure you have copies of any critical data in a secure location to protect against loss of data from hardware failures.</p> <p>Every project has an allocation on the home file system and your project's space can always be accessed via the path <code>/home/[project-code]</code>. The home file system is approximately 1.5 PB in size and is implemented using the Ceph technology. This means that this storage is not particularly high performance but are well suited to standard operations like compilation and file editing. This file systems is visible from the Cirrus login nodes.</p>"},{"location":"user-guide/data/#quotas-on-home-file-system","title":"Quotas on home file system","text":"<p>All projects are assigned a quota on the home file system. The project PI or manager can split this quota up between groups of users if they wish.</p> <p>You can view any home file system quotas that apply to your account by logging into SAFE and navigating to the page for your Cirrus login account.</p> <ol> <li>Log into SAFE</li> <li>Use the \"Login accounts\" menu and select your Cirrus login account</li> <li>The \"Login account details\" table lists any user or group quotas     that are linked with your account. (If there is no quota shown for a     row then you have an unlimited quota for that item, but you may     still may be limited by another quota.)</li> </ol> <p>Quota and usage data on SAFE is updated twice daily so may not be exactly up to date with the situation on the system itself.</p>"},{"location":"user-guide/data/#from-the-command-line","title":"From the command line","text":"<p>Some useful information on the current contents of directories on the <code>/home</code> file system is available from the command line by using the Ceph command <code>getfattr</code>. This is to be preferred over standard Unix commands such as <code>du</code> for reasons of efficiency.</p> <p>For example, the number of entries (files plus directories) in a home directory can be queried via</p> <pre><code>[auser@login01]$ cd\n[auser@login01]$ getfattr -n ceph.dir.entries .\n# file: .\nceph.dir.entries=\"33\"\n</code></pre> <p>The corresponding attribute <code>rentries</code> gives the recursive total in all subdirectories, that is, the total number of files and directories:</p> <pre><code>[auser@login01]$ getfattr -n ceph.dir.rentries .\n# file: .\nceph.dir.rentries=\"1619179\"\n</code></pre> <p>Other useful attributes (all prefixed with <code>ceph.dir.</code>) include <code>files</code> which is the number of ordinary files, <code>subdirs</code> the number of subdirectories, and <code>bytes</code> the total number of bytes used. All these have a corresponding recursive version, respectively: <code>rfiles</code>, <code>rsubdirs</code>, and <code>rbytes</code>.</p> <p>A full path name can be specified if required.</p>"},{"location":"user-guide/data/#work-file-system","title":"Work file system","text":"<p>Important</p> <p>There are no backups of any data on the work file system. You should ensure you have copies of any critical data in a secure location to protect against loss of data from hardware failures.</p> <p>Every project has an allocation on the work file system and your project's space can always be accessed via the path <code>/work/[project-code]</code>. The work file system is approximately 1 PB in size and is implemented using the Lustre parallel file system technology. They are designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good.</p> <p>Ideally, the work file system should only contain data that is:</p> <ul> <li>actively in use;</li> <li>recently generated and in the process of being saved elsewhere; or</li> <li>being made ready for up-coming work.</li> </ul> <p>In practice it may be convenient to keep copies of datasets on the work file system that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file system was lost.</p> <p>If you have data on the work file system that you are not going to need in the future please delete it.</p>"},{"location":"user-guide/data/#quotas-on-the-work-file-system","title":"Quotas on the work file system","text":"<p>As for the home file system, all projects are assigned a quota on the work file system. The project PI or manager can split this quota up between groups of users if they wish.</p> <p>You can view any work file system quotas that apply to your account by logging into SAFE and navigating to the page for your Cirrus login account.</p> <ol> <li>Log into SAFE</li> <li>Use the \"Login accounts\" menu and select your Cirrus login account</li> <li>The \"Login account details\" table lists any user or project quotas     that are linked with your account. (If there is no quota shown for a     row then you have an unlimited quota for that item, but you may     still may be limited by another quota.)</li> </ol> <p>Quota and usage data on SAFE is updated twice daily so may not be exactly up to date with the situation on the system itself.</p> <p>You can also examine up to date quotas and usage on the Cirrus system itself using the <code>lfs quota</code> command. To do this:</p> <p>Change directory to the work directory where you want to check the quota. For example, if I wanted to check the quota for user <code>auser</code> in project <code>t01</code> then I would:</p> <pre><code>[auser@login01:~]$ cd /work/t01/t01/auser\n\n[auser@login01:auser]$ lfs quota -hu auser .\nDisk quotas for usr auser (uid 68826):\n        Filesystem    used   quota   limit   grace   files   quota   limit   grace\n                .  5.915G      0k      0k       -   51652       0       0       -\nuid 68826 is using default block quota setting\nuid 68826 is using default file quota setting\n</code></pre> <p>the quota and limit of 0k here indicate that no user quota is set for this user.</p> <p>To check your project quota, you would use the command:</p> <p><pre><code>[auser@login01:auser]$ lfs quota -hp $(id -g)'01' .\nDisk quotas for prj 3773301 (pid 3773301):\n    Filesystem    used   quota   limit   grace   files   quota   limit   grace\n            .   958.3G     0k  13.57T       - 9038326       0       0       -\npid 3773301 is using default file quota setting\n</code></pre> the limit of <code>13.57T</code> indicates the quota for the project.</p>"},{"location":"user-guide/data/#archiving","title":"Archiving","text":"<p>If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like <code>tar</code> and <code>zip</code>.</p>"},{"location":"user-guide/data/#tar","title":"tar","text":"<p>The <code>tar</code> command packs files into a \"tape archive\" format. The command has general form:</p> <pre><code>tar [options] [file(s)]\n</code></pre> <p>Common options include:</p> <ul> <li><code>-c</code> create a new archive</li> <li><code>-v</code> verbosely list files processed</li> <li><code>-W</code> verify the archive after writing</li> <li><code>-l</code> confirm all file hard links are included in the archive</li> <li><code>-f</code> use an archive file (for historical reasons, tar writes its      output to stdout by default rather than a file).</li> </ul> <p>Putting these together:</p> <pre><code>tar -cvWlf mydata.tar mydata\n</code></pre> <p>will create and verify an archive.</p> <p>To extract files from a tar file, the option <code>-x</code> is used. For example:</p> <pre><code>tar -xf mydata.tar\n</code></pre> <p>will recover the contents of <code>mydata.tar</code> to the current working directory.</p> <p>To verify an existing tar file against a set of data, the <code>-d</code> (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows:</p> <pre><code>[auser@login01:auser]$ tar -df mydata.tar mydata/*\nmydata/damaged_file: Mod time differs\nmydata/damaged_file: Size differs\n</code></pre> <p>Note</p> <p>tar files do not store checksums with their data, requiring the original data to be present during verification.</p> <p>Tip</p> <p>Further information on using <code>tar</code> can be found in the <code>tar</code> manual (accessed via <code>man tar</code> or at man tar).</p>"},{"location":"user-guide/data/#zip","title":"zip","text":"<p>The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as:</p> <pre><code>zip [options] mydata.zip [file(s)]\n</code></pre> <p>Common options are:</p> <ul> <li><code>-r</code> used to zip up a directory</li> <li><code>-#</code> where \"#\" represents a digit ranging from 0 to 9 to specify      compression level, 0 being the least and 9 the most. Default      compression is -6 but we recommend using -0 to speed up the      archiving process.</li> </ul> <p>Together:</p> <pre><code>zip -0r mydata.zip mydata\n</code></pre> <p>will create an archive.</p> <p>Note</p> <p>Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout.</p> <p>The corresponding <code>unzip</code> command is used to extract data from the archive. The simplest use case is:</p> <pre><code>unzip mydata.zip\n</code></pre> <p>which recovers the contents of the archive to the current working directory.</p> <p>Files in a zip archive are stored with a CRC checksum to help detect data loss. <code>unzip</code> provides options for verifying this checksum against the stored files. The relevant flag is <code>-t</code> and is used as follows:</p> <pre><code>[auser@login01:auser]$ unzip -t mydata.zip\nArchive:  mydata.zip\n    testing: mydata/                 OK\n    testing: mydata/file             OK\nNo errors detected in compressed data of mydata.zip.\n</code></pre> <p>Tip</p> <p>Further information on using <code>zip</code> can be found in the <code>zip</code> manual (accessed via <code>man zip</code> or at man zip).</p>"},{"location":"user-guide/data/#data-transfer","title":"Data transfer","text":""},{"location":"user-guide/data/#before-you-start","title":"Before you start","text":"<p>Read Harry Mangalam's guide on How to transfer large amounts of data via network. This tells you all you want to know about transferring data.</p>"},{"location":"user-guide/data/#data-transfer-via-ssh","title":"Data Transfer via SSH","text":"<p>The easiest way of transferring data to/from Cirrus is to use one of the standard programs based on the SSH protocol such as <code>scp</code>, <code>sftp</code> or <code>rsync</code>. These all use the same underlying mechanism (ssh) as you normally use to login to Cirrus. So, once the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine.</p> <p>To avoid having to type in your password multiple times you can set up a ssh-key as documented in the User Guide at <code>connecting</code></p>"},{"location":"user-guide/data/#ssh-transfer-performance-considerations","title":"SSH Transfer Performance Considerations","text":"<p>The ssh protocol encrypts all traffic it sends. This means that file-transfer using ssh consumes a relatively large amount of CPU time at both ends of the transfer. The encryption algorithm used is negotiated between the ssh-client and the ssh-server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The arcfour algorithm is usually quite fast assuming both hosts support it.</p> <p>A single ssh based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories.</p> <p>In addition, you should consider the following when transferring data.</p> <ul> <li>Only transfer those files that are required. Consider which data you   really need to keep.</li> <li>Combine lots of small files into a single tar archive, to reduce the   overheads associated in initiating many separate data transfers (over   SSH each file counts as an individual transfer).</li> <li>Compress data before sending it, e.g. using gzip.</li> </ul>"},{"location":"user-guide/data/#scp-command","title":"scp command","text":"<p>The <code>scp</code> command creates a copy of a file, or if given the <code>-r</code> flag, a directory, on a remote machine.</p> <p>For example, to transfer files to Cirrus:</p> <pre><code>scp [options] source user@login.cirrus.ac.uk:[destination]\n</code></pre> <p>(Remember to replace <code>user</code> with your Cirrus username in the example above.)</p> <p>In the above example, the <code>[destination]</code> is optional, as when left out <code>scp</code> will simply copy the source into the user's home directory. Also the <code>source</code> should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory.</p> <p>Tip</p> <p>If your local version of OpenSSL (the library underlying <code>scp</code>) is  very new you may see errors transferring data to Cirrus using <code>scp</code> where the version of OpenSSL is older. The errors typically look like <code>scp: upload \"mydata\": path canonicalization failed</code>. You can get around this issue by adding the <code>-O</code> option to <code>scp</code>.</p> <p>If you want to request a different encryption algorithm add the <code>-c [algorithm-name]</code> flag to the <code>scp</code> options. For example, to use the (usually faster) aes128-ctr encryption algorithm you would use:</p> <pre><code>scp [options] -c aes128-ctr source user@login.cirrus.ac.uk:[destination]\n</code></pre> <p>(Remember to replace <code>user</code> with your Cirrus username in the example above.)</p>"},{"location":"user-guide/data/#rsync-command","title":"rsync command","text":"<p>The <code>rsync</code> command can also transfer data between hosts using a <code>ssh</code> connection. It creates a copy of a file or, if given the <code>-r</code> flag, a directory at the given destination, similar to <code>scp</code> above.</p> <p>Given the <code>-a</code> option rsync can also make exact copies (including permissions), this is referred to as mirroring. In this case the <code>rsync</code> command is executed with ssh to create the copy on a remote machine.</p> <p>To transfer files to Cirrus using <code>rsync</code> the command should have the form:</p> <pre><code>rsync [options] -e ssh source user@login.cirrus.ac.uk:[destination]\n</code></pre> <p>(Remember to replace <code>user</code> with your Cirrus username in the example above.)</p> <p>In the above example, the <code>[destination]</code> is optional, as when left out <code>rsync</code> will simply copy the source into the users home directory. Also the <code>source</code> should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory.</p> <p>Additional flags can be specified for the underlying <code>ssh</code> command by using a quoted string as the argument of the <code>-e</code> flag. e.g.</p> <pre><code>rsync [options] -e \"ssh -c aes128-ctr\" source user@login.cirrus.ac.uk:[destination]\n</code></pre> <p>(Remember to replace <code>user</code> with your Cirrus username in the example above.)</p>"},{"location":"user-guide/data/#data-transfer-using-rclone","title":"Data transfer using rclone","text":"<p>Rclone is a command-line program to manage files on cloud storage. You can transfer files directly to/from cloud storage services, such as MS OneDrive and Dropbox. The program preserves timestamps and verifies checksums at all times.</p> <p>First of all, you must download and unzip rclone on Cirrus:</p> <pre><code>wget https://downloads.rclone.org/v1.62.2/rclone-v1.62.2-linux-amd64.zip\nunzip rclone-v1.62.2-linux-amd64.zip\ncd rclone-v1.62.2-linux-amd64/\n</code></pre> <p>The previous code snippet uses rclone v1.62.2, which was the latest version when these instructions were written.</p> <p>Configure rclone using <code>./rclone config</code>. This will guide you through an interactive setup process where you can make a new remote (called <code>remote</code>). See the following for detailed instructions for:</p> <ul> <li>Microsoft OneDrive</li> <li>Dropbox</li> </ul> <p>Please note that a token is required to connect from Cirrus to the cloud service. You need a web browser to get the token. The recommendation is to run rclone in your laptop using <code>rclone authorize</code>, get the token, and then copy the token from your laptop to Cirrus. The rclone website contains further instructions on configuring rclone on a remote machine without web browser.</p> <p>Once all the above is done, you are ready to go. If you want to copy a directory, please use:</p> <pre><code>rclone copy &lt;cirrus_directory&gt; remote:&lt;cloud_directory&gt;\n</code></pre> <p>Please note that \"remote\" is the name that you have chosen when running <code>rclone config</code>. To copy files, please use:</p> <pre><code>rclone copyto &lt;cirrus_file&gt; remote:&lt;cloud_file&gt;\n</code></pre> <p>Note</p> <p>If the session times out while the data transfer takes place, adding the <code>-vv</code> flag to an rclone transfer forces rclone to output to the terminal and therefore avoids triggering the timeout process.</p>"},{"location":"user-guide/development/","title":"Application development environment","text":""},{"location":"user-guide/development/#whats-available","title":"What's available","text":"<p>Cirrus runs the RHEL 9, and provides a development environment which includes:</p> <ul> <li>Software modules via a standard module framework</li> <li>Four different compiler environments (AMD, Cray, Intel and GNU)</li> <li>MPI, OpenMP, and SHMEM</li> <li>Scientific and numerical libraries</li> <li>Parallel Python and R</li> <li>Parallel debugging and profiling</li> <li>Apptainer container software</li> </ul> <p>Access to particular software, and particular versions, is managed by an  Lmod module framework. Most software is available by loading modules, including the different compiler environments</p> <p>You can see what compiler environments are available with:</p> <pre><code>[auser@uan01:~]$ module avail PrgEnv\n\n-------------------------------------- /opt/cray/pe/lmod/modulefiles/core ---------------------------------------\n   PrgEnv-aocc/8.6.0    PrgEnv-cray/8.6.0    PrgEnv-gnu/8.6.0 (L)    PrgEnv-intel/8.6.0\n\n  Where:\n   L:  Module is loaded\n\nModule defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree.\nSee https://lmod.readthedocs.io/en/latest/060_locating.html for details.\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>Other software modules can be searched using the <code>module spider</code> command:</p> <pre><code>[auser@uan01:~]$ module spider\n\n--------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n--------------------------------------------------------------------------------------------------------------\n  PrgEnv-aocc: PrgEnv-aocc/8.6.0\n\n  PrgEnv-cray: PrgEnv-cray/8.6.0\n\n  PrgEnv-gnu: PrgEnv-gnu/8.6.0\n\n  PrgEnv-intel: PrgEnv-intel/8.6.0\n\n  aocc: aocc/5.0.0\n\n  aocc-mixed: aocc-mixed/5.0.0\n\n  atp: atp/3.15.6\n\n  castep: castep/24.1\n\n  cce: cce/19.0.0\n\n  cce-mixed: cce-mixed/19.0.0\n\n  cmake: cmake/4.1.2\n\n  cp2k: cp2k/2025.1\n\n  cpe: cpe/25.03\n\n  cray-R: cray-R/4.4.0\n\n  cray-ccdb: cray-ccdb/5.0.6\n\n  cray-cti: cray-cti/2.19.1\n\n  cray-dsmml: cray-dsmml/0.3.1\n\n  cray-dyninst: cray-dyninst/12.3.5\n\n  cray-fftw: cray-fftw/3.3.10.10\n\n...output trimmed...\n</code></pre> <p>A full discussion of the module system is available in the software environment section.</p> <p>A consistent set of modules is loaded on login to the machine (currently <code>PrgEnv-cray</code>, see below). Developing applications then means selecting and loading the appropriate set of modules before starting work.</p> <p>This section is aimed at code developers and will concentrate on the compilation environment, building libraries and executables, specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation.</p> <p>Tip</p> <p>If you want to get back to the login module state without having to logout and back in again, you can use:</p> <pre><code>module restore\n</code></pre> <p>This is also handy for build scripts to ensure you are starting from a known state.</p>"},{"location":"user-guide/development/#compiler-environments","title":"Compiler environments","text":"<p>There are four different compiler environments available on Cirrus:</p> <ul> <li>AMD Compiler Collection (AOCC)</li> <li>GNU Compiler Collection (GCC)</li> <li>Intel oneAPI (Intel)</li> <li>HPE Cray Compiler Collection (CCE) (current default compiler environment)</li> </ul> <p>The current compiler suite is selected via the <code>PrgEnv</code> module , while the specific compiler versions are determined by the relevant compiler module. A summary is:</p> Suite name Compiler Environment Module Compiler Version Module CCE <code>PrgEnv-cray</code> <code>cce</code> GCC <code>PrgEnv-gnu</code> <code>gcc-native</code> Intel <code>PrgEnv-intel</code> <code>intel</code> AOCC <code>PrgEnv-aocc</code> <code>aocc</code> <p>For example, at login, the default set of modules are:</p> <pre><code>[user@login03:~]$ module list\n\nCurrently Loaded Modules:\n  1) craype-x86-turin         5) xpmem/0.2.119-1.3_gef379be13330   9) cray-dsmml/0.3.1     13) epcc-setup-env\n  2) libfabric/1.22.0         6) cce/19.0.0                       10) cray-mpich/8.1.32    14) load-epcc-module\n  3) craype-network-ofi       7) cse_env/0.2                      11) cray-libsci/25.03.0\n  4) perftools-base/25.03.0   8) craype/2.7.34                    12) PrgEnv-cray/8.6.0\n</code></pre> <p>from which we see the default compiler environment is Cray (indicated by <code>PrgEnv-cray</code> (at 11 in the list above) and the default compiler module is <code>cce/19.0.0</code> (at 6 in the list above). The compiler environment will give access to a consistent set of compiler, MPI library via <code>cray-mpich</code> (at 9), and other libraries e.g., <code>cray-libsci</code> (at 10 in the list above).</p>"},{"location":"user-guide/development/#switching-between-compiler-environments","title":"Switching between compiler environments","text":"<p>Switching between different compiler environments is achieved using the <code>module load</code> command. For example, to switch from the default HPE Cray (CCE) compiler environment to the GCC environment, you would use:</p> <pre><code>[auser@ln03:~]$ module load PrgEnv-gnu\n\nLmod is automatically replacing \"cce/19.0.0\" with \"gcc-native/14.2\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.6.0\" with \"PrgEnv-gnu/8.6.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32\n</code></pre> <p>If you then use the <code>module list</code> command, you will see that your environment has been changed to the GCC environment:</p> <pre><code>[auser@ln03:~]$ module list\n\nCurrently Loaded Modules:\n  1) craype-x86-turin         5) xpmem/0.2.119-1.3_gef379be13330   9) gcc-native/14.2    13) cray-libsci/25.03.0\n  2) libfabric/1.22.0         6) cse_env/0.2                      10) craype/2.7.34      14) PrgEnv-gnu/8.6.0\n  3) craype-network-ofi       7) epcc-setup-env                   11) cray-dsmml/0.3.1\n  4) perftools-base/25.03.0   8) load-epcc-module                 12) cray-mpich/8.1.32\n</code></pre>"},{"location":"user-guide/development/#switching-between-compiler-versions","title":"Switching between compiler versions","text":"<p>Within a given compiler environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To switch to the GNU compiler environment from the default HPE Cray compiler environment and than swap the version of GCC from the 11.2.0 default to  the older 10.3.0 version, you would use</p> <pre><code>[auser@ln03:~]$ module load PrgEnv-gnu\n\nLmod is automatically replacing \"cce/19.0.0\" with \"gcc-native/14.2\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.6.0\" with \"PrgEnv-gnu/8.6.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32\n\nauser@ln03:~&gt; module load gcc-native/13.3\n\nThe following have been reloaded with a version change:\n  1) gcc-native/14.2 =&gt; gcc-native/13.3\n</code></pre> <p>The first swap command moves to the GNU compiler environment and the second swap command moves to the older version of GCC. As before, <code>module list</code> will show that your environment has been changed:</p> <pre><code>[auser@ln03:~]$ module list\n\nCurrently Loaded Modules:\n  1) craype-x86-turin         5) xpmem/0.2.119-1.3_gef379be13330   9) cray-dsmml/0.3.1  13) cray-mpich/8.1.32\n  2) libfabric/1.22.0         6) cse_env/0.2                      10) PrgEnv-gnu/8.6.0  14) cray-libsci/25.03.0\n  3) craype-network-ofi       7) epcc-setup-env                   11) craype/2.7.34\n  4) perftools-base/25.03.0   8) load-epcc-module                 12) gcc-native/13.3\n</code></pre>"},{"location":"user-guide/development/#compiler-wrapper-scripts-cc-cc-ftn","title":"Compiler wrapper scripts: <code>cc</code>, <code>CC</code>, <code>ftn</code>","text":"<p>To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: <code>cc</code>, <code>CC</code>, and <code>ftn</code>, respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the <code>-craype-verbose</code> flag to the compiler wrapper.</p> <p>The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries.</p> <p>Users should not, in general, invoke specific compilers at compile/link stages. In particular, <code>gcc</code>, which may default to <code>/usr/bin/gcc</code>, should not be used. The compiler wrappers <code>cc</code>, <code>CC</code>, and <code>ftn</code> should be used (with the underlying compiler type and version set by the module system). Other common MPI compiler wrappers e.g., <code>mpicc</code>, should also be replaced by the relevant wrapper, e.g. <code>cc</code> (commands such as <code>mpicc</code> are not available on Cirrus).</p> <p>Important</p> <p>Always use the compiler wrappers <code>cc</code>, <code>CC</code>, and/or <code>ftn</code> and not a specific compiler invocation. This will ensure consistent compile/link time behaviour.</p> <p>Tip</p> <p>If you are using a build system such as Make or CMake then you  will need to replace all occurrences of <code>mpicc</code> with <code>cc</code>, <code>mpicxx</code>/<code>mpic++</code> with <code>CC</code> and <code>mpif90</code> with <code>ftn</code>.</p>"},{"location":"user-guide/development/#compiler-man-pages-and-help","title":"Compiler man pages and help","text":"<p>Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard <code>man</code> pages. The <code>man</code> page for the compiler wrappers is common to all programming environments, while the <code>man</code> page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options:</p> Compiler suite C C++ Fortran Cray <code>man clang</code> <code>man clang++</code> <code>man crayftn</code> GNU <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> Intel <code>man icx</code> <code>man icpx</code> <code>man ifx</code> Wrappers <code>man cc</code> <code>man CC</code> <code>man ftn</code> <p>Tip</p> <p>You can also pass the <code>--help</code> option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses <code>ftn --craype-help</code> to access the help options.</p> <p>Tip</p> <p>There are no <code>man</code> pages for the AOCC compilers at the moment.</p> <p>Tip</p> <p>Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc. <code>clang --help</code> will produce a full summary of options with Cray-specific options marked \"Cray\". The <code>clang</code> man page on ARCHER2 concentrates on these Cray extensions to the <code>clang</code> front end and does not provide an exhaustive description of all <code>clang</code> options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran.</p>"},{"location":"user-guide/development/#which-compiler-environment","title":"Which compiler environment?","text":"<p>If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, <code>PrgEnv-gnu</code>); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three compiler environments.</p> <p>For users requiring specific compiler features, such as coarray Fortran, the recommended starting point would be Cray. The following sections provide further details of the different compiler environments.</p>"},{"location":"user-guide/development/#gnu-compiler-collection-gcc","title":"GNU compiler collection (GCC)","text":"<p>The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers.</p> <p>Switch to the GCC compiler environment via:</p> <pre><code>[auser@ln03:~]$ module load PrgEnv-gnu\n\nLmod is automatically replacing \"cce/19.0.0\" with \"gcc-native/14.2\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.6.0\" with \"PrgEnv-gnu/8.6.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32\n</code></pre> <p>Warning</p> <p>If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. <code>use mpi</code> or <code>INCLUDE 'mpif.h'</code>) you must add the <code>-fallow-argument-mismatch</code> option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of <code>gfortran</code> have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, <code>MPI_Send()</code>. This will now generate an error as not standard conforming. The <code>-fallow-argument-mismatch</code> option is used to reduce the error to a warning. The same effect may be achieved via <code>-std=legacy</code>.</p> <p>If you use the Fortran 2008 MPI interface (i.e. <code>use mpi_f08</code>) then you should not need to add this option.</p> <p>Fortran language MPI bindings are described in more detail at in the MPI Standard documentation.</p>"},{"location":"user-guide/development/#useful-gnu-fortran-options","title":"Useful Gnu Fortran options","text":"Option Comment <code>-O&lt;level&gt;</code> Optimisation levels: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code>, <code>-Ofast</code>. <code>-Ofast</code> is not recommended without careful regression testing on numerical output. <code>-std=&lt;standard&gt;</code> Default is gnu <code>-fallow-argument-mismatch</code> Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) <code>-fbounds-check</code> Use runtime checking of array indices <code>-fopenmp</code> Compile OpenMP (default is no OpenMP) <code>-v</code> Display verbose output from compiler stages <p>Tip</p> <p>The <code>standard</code> in <code>-std</code> may be one of <code>f95</code> <code>f2003</code>, <code>f2008</code> or <code>f2018</code>. The default option <code>-std=gnu</code> is the latest Fortran standard plus gnu extensions.</p> <p>Warning</p> <p>Past versions of <code>gfortran</code> have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to <code>MPI_Send()</code> and so on. This will now generate an error as not standard conforming. Use <code>-fallow-argument-mismatch</code> to reduce the error to a warning. The same effect may be achieved via <code>-std=legacy</code>.</p>"},{"location":"user-guide/development/#reference-material","title":"Reference material","text":"<ul> <li>C/C++ documentation</li> <li>Fortran documentation</li> </ul>"},{"location":"user-guide/development/#cray-compiling-environment-cce","title":"Cray Compiling Environment (CCE)","text":"<p>The Cray Compiling Environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions).</p> <p>The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically <code>CFLAGS</code> and <code>FFLAGS</code>) if compiling mixed C/Fortran applications.</p> <p>As CCE is the default compiler environment on Cirrus, you do not usually need to issue any commands to enable CCE.</p>"},{"location":"user-guide/development/#useful-cce-cc-options","title":"Useful CCE C/C++ options","text":"<p>When using the compiler wrappers <code>cc</code> or <code>CC</code>, some of the following options may be useful:</p> <p>Language, warning, Debugging options:</p> Option Comment <code>-std=&lt;standard&gt;</code> Default is <code>-std=gnu11</code> (<code>gnu++14</code> for C++) [1] <p>Performance options:</p> Option Comment <code>-Ofast</code> Optimisation levels: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code>, <code>-Ofast</code>. <code>-Ofast</code> is not recommended without careful regression testing on numerical output. <code>-ffp=level</code> Floating point maths optimisations levels 0-4 [2] <code>-flto</code> Link time optimisation <p>Miscellaneous options:</p> Option Comment <code>-fopenmp</code> Compile OpenMP (default is off) <code>-v</code> Display verbose output from compiler stages <p>Notes</p> <ol> <li>Option <code>-std=gnu11</code> gives <code>c11</code> plus GNU extensions (likewise     <code>c++14</code> plus GNU extensions). See     https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html</li> <li>Option <code>-ffp=3</code> is implied by <code>-Ofast</code> or <code>-ffast-math</code></li> </ol>"},{"location":"user-guide/development/#useful-cce-fortran-options","title":"Useful CCE Fortran options","text":"<p>Language, Warning, Debugging options:</p> Option Comment <code>-m &lt;level&gt;</code> Message level (default <code>-m 3</code> errors and warnings) <p>Performance options:</p> Option Comment <code>-O &lt;level&gt;</code> Optimisation levels: -O0 to -O3 (default -O2) <code>-h fp&lt;level&gt;</code> Floating point maths optimisations levels 0-3 <code>-h ipa</code> Inter-procedural analysis <p>Miscellaneous options:</p> Option Comment <code>-h omp</code> Compile OpenMP (default is <code>-hnoomp</code>) <code>-v</code> Display verbose output from compiler stages"},{"location":"user-guide/development/#cce-reference-documentation","title":"CCE Reference Documentation","text":"<ul> <li>Clang/Clang++ documentation, CCE-specific    details are available via <code>man clang</code> once the CCE compiler environment is loaded.</li> <li>Cray Fortran documentation</li> </ul>"},{"location":"user-guide/development/#intel-compilers-oneapi","title":"Intel compilers (oneAPI)","text":"<p>Intel oneAPI provides C/C++ compiers, Fortran compilers and other libraries and tools.</p> <p>Switch to the Intel compiler environment via:</p> <pre><code>[auser@ln03:~]$ module load PrgEnv-intel\n\nLmod is automatically replacing \"cce/19.0.0\" with \"intel/2023.2\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.6.0\" with \"PrgEnv-intel/8.6.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32\n</code></pre> <p>Warning</p> <p>The Intel compiler environment only provides the new LLVM based compilers (<code>icx</code>, <code>icpx</code> and <code>ifx</code>), the classic Intel compilers are not available.</p>"},{"location":"user-guide/development/#useful-intel-fortran-ifx-options","title":"Useful Intel Fortran (<code>ifx</code>) options","text":"Option Comment <code>-O&lt;level&gt;</code> Optimisation levels: <code>-O0</code>, <code>-O1</code>, <code>-O2</code>, <code>-O3</code>, <code>-Ofast</code>. <code>-Ofast</code> is not recommended without careful regression testing on numerical output. <code>-std=&lt;standard&gt;</code> Default is gnu <code>-fbounds-check</code> Use runtime checking of array indices <code>-fopenmp</code> Compile OpenMP (default is no OpenMP) <code>-v</code> Display verbose output from compiler stages"},{"location":"user-guide/development/#reference-material_1","title":"Reference material","text":"<ul> <li>oneAPI documentation library</li> </ul>"},{"location":"user-guide/development/#amd-optimizing-compiler-collection-aocc","title":"AMD Optimizing Compiler Collection (AOCC)","text":"<p>The AMD Optimizing Compiler Collection (AOCC) is a clang-based optimising compiler. AOCC also includes a flang-based Fortran compiler.</p> <p>Load the AOCC compiler environment from the default CCE (cray) compiler environment via:</p> <pre><code>[auser@ln03:~]$ module load PrgEnv-aocc\n\nLmod is automatically replacing \"cce/19.0.0\" with \"aocc/4.1\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.6.0\" with \"PrgEnv-gnu/8.6.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32\n</code></pre>"},{"location":"user-guide/development/#aocc-reference-material","title":"AOCC reference material","text":"<ul> <li>AOCC on AMD website</li> </ul>"},{"location":"user-guide/development/#message-passing-interface-mpi","title":"Message passing interface (MPI)","text":""},{"location":"user-guide/development/#hpe-cray-mpich","title":"HPE Cray MPICH","text":"<p>HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the Slingshot interconnect. The current implementation supports MPI standard version 3.4.</p> <p>The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: <code>cc</code>, <code>CC</code> and <code>ftn</code>.</p> <p>You do not need to do anything to make HPE Cray MPICH available when you log into Cirrus, it is available by default to all users.</p>"},{"location":"user-guide/development/#mpi-reference-material","title":"MPI reference material","text":"<ul> <li>MPI standard documents</li> </ul>"},{"location":"user-guide/development/#linking-and-libraries","title":"Linking and libraries","text":"<p>Linking to libraries is performed dynamically on Cirrus.</p> <p>Important</p> <p>Static linking is not supported on Cirrus. If you attempt to link statically, you will see errors similar to: <pre><code>/usr/bin/ld: cannot find -lpmi\n/usr/bin/ld: cannot find -lpmi2\ncollect2: error: ld returned 1 exit status\n</code></pre></p> <p>One can use the <code>-craype-verbose</code> flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded.</p> <p>Tip</p> <p>The RUNPATH setting in the executable only works for default versions of libraries. If you want to use non-default versions then you need to add some additional commands at compile time and in your job submission scripts. See the Using non-default versions of HPE Cray libraries on Cirrus.</p> <p>The library RUNPATH associated with an executable can be inspected via, e.g.,</p> <pre><code>$ readelf -d ./a.out\n</code></pre> <p>(swap <code>a.out</code> for the name of the executable you are querying).</p>"},{"location":"user-guide/development/#commonly-used-libraries","title":"Commonly used libraries","text":"<p>Modules with names prefixed by <code>cray-</code> are provided by HPE Cray, and work with any of the compiler environments and. These modules should be the first choice for access to software libraries if available.</p> <p>Tip</p> <p>More information on the different software libraries on Cirrus can be found in the Software libraries section of the user guide.</p>"},{"location":"user-guide/development/#hpe-cray-programming-environment-cpe-releases","title":"HPE Cray Programming Environment (CPE) releases","text":""},{"location":"user-guide/development/#available-hpe-cray-programming-environment-cpe-releases","title":"Available HPE Cray Programming Environment (CPE) releases","text":"<p>Cirrus currently has the following HPE Cray Programming Environment (CPE) releases available:</p> <ul> <li>25.09: Current default</li> </ul>"},{"location":"user-guide/development/#using-non-default-versions-of-hpe-cray-libraries","title":"Using non-default versions of HPE Cray libraries","text":"<p>If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the <code>LD_LIBRARY_PATH</code> environment variable.</p> <p>At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of <code>$CRAY_LD_LIBRARY_PATH</code> as the first entry. For example, to use the, non-default, newer 9.0.0 version of HPE Cray MPICH in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with:</p> <pre><code>module load cray-mpich/9.0.0\nexport LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n</code></pre> <p>The order is important here: every time you change a module, you will need to reset the value of <code>LD_LIBRARY_PATH</code> for the process to work (it will not be updated automatically).</p> <p>Now you can compile your code. You can check that the executable is using the correct version  of LibSci with the <code>ldd</code> command and look for the line beginning <code>libmpi*</code>, you should see the version in the path to the library file.</p> <p>Tip</p> <p>If any of the libraries point to versions in the <code>/opt/cray/pe/lib64</code> directory then these are using the default versions of the libraries rather than the  specific versions. This happens at compile time if you have forgotton to load  the right module and set <code>$LD_LIBRARY_PATH</code> afterwards.</p> <p>At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the <code>ldd</code> command in your job submission script to  check the library is pointing to the correct version). For example, a job submission script to run an executable with the non-default version of Cray MPICH could look like:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --time=0:20:0\n#SBATCH --exclusive\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=288\n#SBATCH --cpus-per-task=1\n\n# Replace the account code, partition and QoS with those you wish to use\n#SBATCH --account=t01        \n#SBATCH --partition=standard\n#SBATCH --qos=short\n\n# Setup up the environment to use the non-default version of LibSci\nmodule load cray-mpich/9.0.0\nexport LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH\n\n# Check which library versions the executable is pointing too\nldd /path/to/myapp.x\n\nexport OMP_NUM_THREADS=1\n\nsrun --hint=nomultithread --distribution=block:block /path/to/myapp.x\n</code></pre> <p>Tip</p> <p>As when compiling, the order of commands matters. Setting the value of <code>LD_LIBRARY_PATH</code> must happen after you have finished all your <code>module</code> commands for it to have the correct effect.</p> <p>Important</p> <p>You must setup the environment at both compile and run time otherwise you will end up using the default version of the library.</p>"},{"location":"user-guide/development/#compiling-on-compute-nodes","title":"Compiling on compute nodes","text":"<p>Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the <code>/home</code> file system is not available.</p> <p>An example job submission script for a compile job using <code>make</code> (assuming the Makefile is in the same directory as the job submission script) would be:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=compile\n#SBATCH --time=00:20:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=1\n\n# Replace the account code, partition and QoS with those you wish to use\n#SBATCH --account=t01        \n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n\nmake clean\n\nmake\n</code></pre> <p>Note</p> <p>If you want to use a compiler environment other than the default then you will need to add the <code>module load</code> command before the <code>make</code> command. e.g. to use the GCC compiler environemnt:</p> <pre><code>module load PrgEnv-gnu\n</code></pre> <p>You can also use a compute node in an interactive way using <code>salloc</code>. Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code.</p>"},{"location":"user-guide/development/#using-the-compiler-wrappers-for-serial-compilations","title":"Using the compiler wrappers for serial compilations","text":"<p>The compiler wrappers link with a number of HPE-provided libraries automatically.  It is possible to compile codes in serial with the compiler wrappers to take  advantage of the HPE libraries.</p> <p>To set up your environment for serial compilation, you will need to run:</p> <pre><code>  module load craype-network-none\n  module remove cray-mpich\n</code></pre> <p>Once this is done, you can use the compiler wrappers (<code>cc</code> for C, <code>CC</code> for  C++, and <code>ftn</code> for Fortran) to compile your code in serial.</p>"},{"location":"user-guide/development/#managing-development","title":"Managing development","text":"<p>Cirrus supports common revision control software such as <code>git</code>.</p> <p>Standard GNU autoconf tools are available, along with <code>make</code> (which is GNU Make). Versions of <code>cmake</code> are available.</p> <p>Tip</p> <p>Some of these tools are part of the system software, and typically reside in <code>/usr/bin</code>, while others are provided as part of the module system. Some tools may be available in different versions via both <code>/usr/bin</code> and via the module system. If you find the default version is too old, then look in the module system for a more recent version.</p>"},{"location":"user-guide/development/#build-instructions-for-software-on-cirrus","title":"Build instructions for software on Cirrus","text":"<p>The Cirrus CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including ARCHER2) in a public Github repository. See:</p> <ul> <li>Build instructions repository</li> </ul> <p>The repository always welcomes contributions from the Cirrus user community.</p>"},{"location":"user-guide/development/#support-for-building-software-on-cirrus","title":"Support for building software on Cirrus","text":"<p>If you run into issues building software on Cirrus or the software you require is not available then please contact the Cirrus Service Desk with any questions you have.</p>"},{"location":"user-guide/globus/","title":"Using Globus to transfer data to/from Cirrus /work filesystem","text":""},{"location":"user-guide/globus/#setting-up-cirrus-work","title":"Setting up Cirrus /work","text":"<p>Navigate to https://app.globus.org</p> <p>Log in with your Globus identity (this could be a globusid.org or other identity)</p> <p></p> <p>In File Manager, use the search tool to search for \u201ce1000-fs1 directories\u201d. Select it.</p> <p></p> <p>In the transfer pane, you are told that Authentication/Consent is required. Click Continue.</p> <p></p> <p></p> <p>Click on the EIDF safe (safe.epcc.ed.ac.uk) link</p> <p></p> <p>Select the correct User account (if you have more than one)</p> <p>Click Accept</p> <p></p> <p>Now confirm your Globus credentials \u2013 click Continue</p> <p></p> <p>Click on the SAFE id you selected previously</p> <p></p> <p>Make sure the correct User account is selected and Accept again</p> <p>Your Cirrus /work directory will be shown </p> <p></p> <p>Warning</p> <p>Your Cirrus <code>/work</code> directory will be listed as <code>/home/projectID/projectID/username</code>  The file system which is used for <code>/work</code> on Cirrus is mounted as <code>/home</code> on the e1000</p>"},{"location":"user-guide/globus/#setting-up-the-other-end-of-the-transfer","title":"Setting up the other end of the transfer","text":"<p>Make sure you select two-panel view mode</p>"},{"location":"user-guide/globus/#laptop","title":"Laptop","text":"<p>If you wish to transfer data to/from your personal laptop or other device, click on the Collection Search in the right-hand panel</p> <p></p> <p>Use the link to \u201cGet Globus Connect Personal\u201d to create a Collection for your local drive.</p> <p></p>"},{"location":"user-guide/globus/#other-server-eg-jasmin","title":"Other server e.g. JASMIN","text":"<p>If you wish to connect to another server, you will need to search for the Collection e.g. JASMIN Default Collection and authenticate</p> <p>Please see the JASMIN Globus page for more information</p> <p></p>"},{"location":"user-guide/globus/#setting-up-and-initiating-the-transfer","title":"Setting up and initiating the transfer","text":"<p>Once you are connected to both the Source and Destination Collections, you can use the File Manager to select the files to be transferred, and then click the Start button to initiate the transfer</p> <p></p> <p>A pop-up will appear once the Transfer request has been submitted successfully</p> <p>Clicking on the \u201cView Details\u201d will show the progress and final status of the transfer</p> <p></p> <p></p>"},{"location":"user-guide/globus/#using-a-different-cirrus-account","title":"Using a different Cirrus account","text":"<p>If you want to use Globus with a different account on Cirrus, you will have to go to Settings</p> <p></p> <p>Manage Identities</p> <p></p> <p>And Unlink the current EIDF safe identity, then repeat the link process with the other Cirrus account</p>"},{"location":"user-guide/hardware/","title":"Cirrus hardware","text":""},{"location":"user-guide/hardware/#system-overview","title":"System overview","text":"<p>Cirrus is a HPE EX4000 supercomputing system which has a total of 256 compute nodes. Each compute node has 288 cores (dual AMD EPYC 9825 144-core 2.2 GHz processors) giving a total of 73,228 cores. Compute nodes are connected together by a HPE Slingshot 11 interconnect. </p> <p>There are additional User Access Nodes (UAN, also called login nodes), which provide access to the system.</p> <p>Compute nodes are only accessible via the Slurm job scheduling system.</p> <p>There are two storage types: home and work. Home is available on login nodes. Work is available on login and compute nodes.</p> <p>The home file system is provided by Ceph with a capacity of 1 PB.</p> <p>The work file system consists of an HPE ClusterStor E1000 Lustre storage system with a capacity of 1 PB.</p>"},{"location":"user-guide/hardware/#compute-node-overview","title":"Compute node overview","text":"<p>The compute nodes each have 288 cores. They are dual socket nodes with two 144-core AMD EPYC 9825 processors.  There are 192 standard memory nodes and 64 high memory nodes.</p> <p>Note</p> <p>Note due to Simultaneous Multi-Threading (SMT) each core has 2 threads, therefore a node has 288 cores / 576 threads. Most users will not want to use SMT.</p> Component Details Processor 2x AMD Zen5 (Turin) EPYC 9825, 144-core, 2.2 GHz Cores per node 288 NUMA structure 8 NUMA regions per node (36 cores per NUMA region) Memory per node 768 GB (standard), 1,536 GB (high memory) DDR5 Memory per core 2.66 GB (standard), 5.33 GB (high memory) DDR5 L1 cache 80 kB/core L2 cache 1 MB/core L3 cache 32 MB/CCD Vector support AVX512 Network connection 2x 100 Gb/s injection ports per node <p>Each socket contains 12 Core Complex Dies (CCDs) and one I/O die (IOD). Each CCD contains 12 cores and 32 MB of L3 cache. Thus, there are 144 cores per socket and 288 cores per node.</p> <p>More information on the architecture of the AMD EPYC Zen2 processors:</p> <ul> <li>5th Gen AMD EPYC Processor Architecture White Paper</li> <li>HPC Tuning Guide for AMD EPYC 9005 Processors</li> </ul>"},{"location":"user-guide/hardware/#amd-zen5-microarchitecture","title":"AMD Zen5 microarchitecture","text":"<p>The AMD EPYC 9825 (Turin) processor has a base CPU clock of 2.2 GHz and a maximum boost clock of 3.7 GHz. There are twelve processor dies (CCDs) with a total of 144 cores per socket.</p> <p>Hybrid multi-die design:</p> <p>Within each socket, the twelve processor dies are fabricated on a 3 nanometer (nm) process, while the I/O die is fabricated on a 6 nm process. This design decision was made because the processor dies need the leading edge (and more expensive) 3 nm technology in order to reduce the amount of power and space needed to double the number of cores, and to add more cache, compared to previous generation EPYC processors. The I/O die retains the less expensive, older 6 nm technology.</p> <p>Infinity Fabric technology:</p> <p>Infinity Fabric technology is used for communication among different components throughout the node: within cores, between cores in a core complex die (CCD), among CCDs in a socket, to the main memory and PCIe, and between the two sockets.</p>"},{"location":"user-guide/hardware/#processor-hierarchy","title":"Processor hierarchy","text":"<p>The Zen5 processor hierarchy is as follows:</p> <ul> <li>Core: A CPU core has private L1I, L1D, and L2 caches, which are shared by two hyperthreads on the core.</li> <li>CCD: A core complex die includes 12 cores, 32 MB shared L3 cache and an Infinity Link to the I/O die (IOD). The CCDs connect to memory, I/O, and each other through the IOD.</li> <li>Socket: A socket includes twelve CCDs (total of 144 cores), a common centralized I/O die (includes twelve unified memory controllers and eight IO x16 PCIe 5.0 lanes - total of 128 lanes), and, for the first socket, a link to the network interface controller (NIC).</li> <li>Node: A node includes two sockets and a network interface controllers (NIC).</li> </ul> <p>CPU core</p> <p>AMD 9825 is a 64-bit x86 server microprocessor. A partial list of instructions and features supported in Rome includes SSE, SSE2, SSE3, SSSE3, SSE4a, SSE4.1, SSE4.2, AES, FMA, AVX, AVX2 (256 bit), AVX512, Integrated x87 FPU (FPU), Multi-Precision Add-Carry (ADX), 16-bit Floating Point Conversion (F16C), and No-eXecute (NX). For a complete list, run <code>cat /proc/cpuinfo</code> on the Cirrus login nodes.</p> <p>Each core:</p> <ul> <li>Can sustain execution of four x86 instructions per cycle, using features such as the micro-op cache, advanced branch prediction, and prefetching. The prefetcher works on streaming data and on variable strides, allowing it to accelerate many different data structures.</li> <li>Has two 256-bit Fused Multiply-Add (FMA) units and can deliver up to 16 double-precision floating point operations (flops) per cycle. Thus, the peak double-precision flops per node (at base frequency) is: 128 cores x 2.25 GHz x 16 = 4.6 teraflops.</li> <li>Can support Simultaneous Multi-threading (SMT), allowing two threads to execute simultaneously per core. SMT is available on Cirrus compute nodes but example submission scripts all use physical cores only as SMT is not usually beneficial for HPC applications.</li> </ul>"},{"location":"user-guide/hardware/#nps-setting","title":"NPS setting","text":"<p>The Cirrus compute nodes use NPS4.</p>"},{"location":"user-guide/hardware/#interconnect-details","title":"Interconnect details","text":"<p>Cirrus has a HPE Slingshot 11 interconnect with 200 Gb/s signalling per node. It uses a dragonfly topology:</p> <ul> <li> <p>Nodes are organized into groups.</p> <ul> <li>128 Nodes in a group.</li> <li>Electrical links between Network Interface Card (NIC) and switch.</li> <li>16 switches per group.</li> <li>2x NIC per node.</li> <li>All-to-all connection amongst switches in a group using electrical links.</li> </ul> </li> <li> <p>All-to-all connection between groups using optical links.</p> <ul> <li>2 groups per Cirrus Cabinet.</li> </ul> </li> </ul>"},{"location":"user-guide/introduction/","title":"Introduction","text":"<p>The Cirrus EX4000 system was installed in Q4 2025. The underlying technology is supplied by HPE Cray and is based on AMD 9005 series processors. The system runs a version of Red Hat Enterprise Linux (RHEL).</p>"},{"location":"user-guide/introduction/#overview-of-the-cirrus-system","title":"Overview of the Cirrus system","text":"A schematic of the Cirrus system, where users login into    the front end nodes, and work on the back end is managed by SLURM.     <p>There are four front end or login nodes which are intended for interactive access, and lightweight pre-processing and post-processing work. The front end nodes use AMD EPYC 9745 processors (two 128-core processors per node) each with a total of 1.5 TB of memory.</p> <p>The SLURM workload manager provides access to a total of 256 back end or compute nodes. All compute nodes have two 144-core AMD 9825 processors (a total of 288 physical codes per node). There are 192 standard compute nodes with 768 GB DDR5 RAM per node, and 64 \"high memory\" nodes with 1,536 GB per node. All the back end compute nodes are connected with Slingshot 11 interconnect.</p> <p>The SLURM scheduler is also informally known as \"the queue system\", although SLURM itself does not have the exact concept of queues. Work is submitted to partitions with a given quality of service (QoS).</p> <p>For further details of the compute node hardware and network, see the hardware description.</p>"},{"location":"user-guide/introduction/#storage","title":"Storage","text":"<p>Storage is provided by two file systems:</p> <ul> <li> <p>A 1.0 PB HPE E1000 ClusterStor Lustre parallel file system mounted on both   the front end and compute nodes. It is also referred to as the work file   system (<code>/work</code>). The work file system must be  used for work submitted to   SLURM.</p> </li> <li> <p>A 1.5 PB Ceph distributed file system mounted on the login nodes but   not the compute nodes. It is the location of users' home directories   (<code>/home</code>).   It follows that work submitted to SLURM must not reference users'   home directories.</p> </li> </ul> <p>For further storage details see Data Management and Transfer.</p>"},{"location":"user-guide/introduction/#charging","title":"Charging","text":"<p>Cirrus is a CPU-only system and usage of the queue system (SLURM) is accounted for in core hours (referred to as \"coreh\" in SAFE). For example, a job requesting one node exclusively for one hour will be charged 288 core hours from the relevant budget if it completes in exactly 60 minutes. Jobs requesting less than a full node will be charged pro-rata according to the number of cores requested and time taken. Accounting takes place at job completion.</p> <p>Applications for access to the service should make estimates of computational resource requirement in these units. Requirements for disk usage should be made in GB or TB.</p>"},{"location":"user-guide/introduction/#acknowledging-cirrus","title":"Acknowledging Cirrus","text":"<p>Please use the following phrase to acknowledge Cirrus in all research outputs that have used the facility:</p> <p>This work used the Cirrus UK National Tier-2 HPC Service at EPCC (http://www.cirrus.ac.uk) funded by The University of Edinburgh, the Edinburgh and South East Scotland City Region Deal, and UKRI via EPSRC.</p>"},{"location":"user-guide/python/","title":"Using Python","text":"<p>Python is supported on Cirrus both for running intensive parallel jobs and also as an analysis tool. This section describes how to use Python in either of these scenarios.</p> <p>The Python installations on Cirrus contain some of the most commonly used packages. If you wish to install additional Python packages, we recommend that you use the <code>pip</code> command, see the section entitled Installing your own Python packages (with pip).</p> <p>Important</p> <p>Python 2 is not supported on Cirrus as it has been deprecated since the start of 2020. </p> <p>Note</p> <p>When you log onto Cirrus, no Python module is loaded by default. You will generally need to load the <code>cray-python</code> module to access the functionality described below.</p>"},{"location":"user-guide/python/#hpe-cray-python-distribution","title":"HPE Cray Python distribution","text":"<p>The recommended way to use Python on Cirrus is to use the HPE Cray Python distribution.</p> <p>The HPE Cray distribution provides Python 3 along with some of the most common packages used for scientific computation and data analysis. These include:</p> <ul> <li>numpy and scipy - built using GCC against HPE Cray LibSci</li> <li>mpi4py - built using GCC against HPE Cray MPICH</li> <li>dask</li> </ul> <p>The HPE Cray Python distribution can be loaded (either on the front-end or in a submission script) using:</p> <pre><code>module load cray-python\n</code></pre> <p>Tip</p> <p>The HPE Cray Python distribution is built using GCC compilers. If you wish to compile your own Python, C/C++ or Fortran code to use with HPE Cray Python, you should ensure that you compile using <code>PrgEnv-gnu</code> to make sure they are compatible.</p>"},{"location":"user-guide/python/#installing-your-own-python-packages-with-pip","title":"Installing your own Python packages (with pip)","text":"<p>Sometimes, you may need to setup a local custom Python environment such that it extends a centrally-installed <code>cray-python</code> module. By extend, we mean being able to install packages locally that are not provided by <code>cray-python</code>. This is necessary because some Python packages such as <code>mpi4py</code> must be built specifically for the Cirrus system and so are best provided centrally.</p> <p>You can do this by creating a lightweight virtual environment where the local packages can be installed. This environment is created on top of an existing Python installation, known as the environment's base Python.</p> <p>First, load the <code>PrgEnv-gnu</code> environment.</p> <pre><code>[auser@login01:~]$ module load PrgEnv-gnu\n</code></pre> <p>This first step is necessary because subsequent <code>pip</code> installs may involve source code compilation and it is better that this be done using the GCC compilers to maintain consistency with how some base Python packages have been built.</p> <p>Second, select the base Python by loading the <code>cray-python</code> module that you wish to extend.</p> <pre><code>[auser@login01:~]$ module load cray-python\n</code></pre> <p>Next, create the virtual environment within a designated folder.</p> <pre><code>python -m venv --system-site-packages /work/t01/t01/auser/myvenv\n</code></pre> <p>In our example, the environment is created within a <code>myvenv</code> folder located on <code>/work</code>, which means the environment will be accessible from the compute nodes. The <code>--system-site-packages</code> option ensures this environment is based on the currently loaded <code>cray-python</code> module. See https://docs.python.org/3/library/venv.html for more details.</p> <p>You're now ready to activate your environment.</p> <pre><code>source /work/t01/t01/auser/myvenv/bin/activate\n</code></pre> <p>Tip</p> <p>The <code>myvenv</code> path uses a fictitious project code, <code>t01</code>, and username, <code>auser</code>. Please remember to replace those values with your actual project code and username. Alternatively, you could enter <code>${HOME/home/work}</code> in place of <code>/work/t01/t01/auser</code>. That command fragment expands <code>${HOME}</code> and then replaces the <code>home</code> part with <code>work</code>.</p> <p>Installing packages to your local environment can now be done as follows.</p> <pre><code>(myvenv) [auser@login01:~]$ python -m pip install &lt;package name&gt;\n</code></pre> <p>Running <code>pip</code> directly as in <code>pip install &lt;package name&gt;</code> will also work, but we show the <code>python -m</code> approach as this is consistent with the way the virtual environment was created. Further, if the package installation will require code compilation, you should amend the command to ensure use of the Cirrus compiler wrappers.</p> <pre><code>(myvenv) [auser@login01:~]$ CC=cc CXX=CC FC=ftn python -m pip install &lt;package name&gt;\n</code></pre> <p>And when you have finished installing packages, you can deactivate the environment by running the <code>deactivate</code> command.</p> <pre><code>(myvenv) [auser@login01:~]$ deactivate\n[auser@login01:~]$\n</code></pre> <p>The packages you have installed will only be available once the local environment has been activated. So, when running code that requires these packages, you must first activate the environment, by adding the activation command to the submission script, as shown below.</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=myvenv\n#SBATCH --exclusive\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=288\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:10:00\n#SBATCH --account=[budget code]\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\nsource /work/t01/t01/auser/myvenv/bin/activate\n\nexport SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\n\nsrun --distribution=block:block --hint=nomultithread python myvenv-script.py\n</code></pre> <p>Tip</p> <p>If you find that a module you've installed to a virtual environment on <code>/work</code> isn't found when running a job, it may be that it was previously installed to the default location of <code>$HOME/.local</code> which is not mounted on the compute nodes. This can be an issue as <code>pip</code> will reuse any modules found at this default location rather than reinstall them into a virtual environment. Thus, even if the virtual environment is on <code>/work</code>, a module you've asked for may actually be located on <code>/home</code>.</p> <p>You can check a module's install location and its dependencies with <code>pip show</code>, for example <code>pip show matplotlib</code>. You may then run <code>pip uninstall matplotlib</code> while no virtual environment is active to uninstall it from <code>$HOME/.local</code>, and then re-run <code>pip install matplotlib</code> while your virtual environment on <code>/work</code> is active to reinstall it there. You will need to do this for any modules installed on <code>/home</code> that will use either directly or indirectly. Remember you can check all your installed modules with <code>pip list</code>.</p>"},{"location":"user-guide/python/#conda-on-archer2","title":"Conda on ARCHER2","text":"<p>Conda-based Python distributions (e.g. Anaconda, Mamba, Miniconda) are an extremely popular way of installing and accessing software on many systems, including Cirrus. Although conda-based distributions can be used on Cirrus,  care is needed in how they are installed and configured so that the installation does not adversely effect your use of Cirrus. In particular, you should be careful of:</p> <ul> <li>Where you install conda on Cirrus</li> <li>Conda additions to shell configuration files such as <code>.bashrc</code></li> </ul> <p>We cover each of these points in more detail below.</p>"},{"location":"user-guide/python/#conda-install-location","title":"Conda install location","text":"<p>If you only need to use the files and executables from your conda installation on the login and data analysis nodes (via the <code>serial</code> QoS) then the best place to install conda is in your home directory structure - this will usually be the default install location provided by the installation script.</p> <p>If you need to access the files and executables from conda on the compute nodes then you will need to install to a  different location as the home file systems are not available on the compute nodes. There are two main options for using conda from Cirrus compute nodes:</p> <ol> <li>Use a conda container image </li> <li>Install conda on the work file system</li> </ol>"},{"location":"user-guide/python/#use-a-conda-container-image","title":"Use a conda container image","text":"<p>You can pull official conda-based container images from Dockerhub that you can use if you want just the standard set of Python modules that come with the distribution. For example, to get the latest Anaconda distribution as an  Apptainer container image on the Cirrus work file system, you would use (on an Cirrus login node, from the directory on the work file system where you want to store the container image):</p> <pre><code>apptainer build anaconda3.sif docker://continuumio/anaconda3\n</code></pre> <p>Once you have the container image, you can run scripts in it with a command like:</p> <pre><code>apptainer exec -B $PWD anaconda3.sif python my_script.py\n</code></pre> <p>As the container image is a single large file, you end up doing a single large read from the work file system rather than lots of small reads of individual Python files, this improves the performance of Python and reduces the  detrimental impact on the wider file system performance for all users.</p> <p>We have pre-built a Apptainer container with the Anaconda distribution in on  ARCHER2. Users can access it at <code>$EPCC_CONTAINER_DIR/anaconda3.sif</code>. To run a Python script with the centrally-installed image, you can use:</p> <pre><code>apptainer exec -B $PWD $EPCC_CONTAINER_DIR/anaconda3.sif python my_script.py\n</code></pre> <p>If you want additional packages that are not available in the standard container images then you will need to build your own container images. If you need help to do this, then please contact the Cirrus Service Desk</p>"},{"location":"user-guide/python/#install-conda-on-the-work-file-system","title":"Install conda on the work file system","text":"<p>To do this, specify an install location  in your directories on the work file system when prompted in the conda installation process.</p>"},{"location":"user-guide/python/#conda-additions-to-shell-configuration-files","title":"Conda additions to shell configuration files","text":"<p>During the install process most conda-based distributions will ask a question like:</p> <p>Do you wish the installer to initialize Miniconda3 by running conda init?</p> <p>If you are installing to the Cirrus work directories, you  should answer \"no\" to this question.</p> <p>Adding the initialisation to shell startup scripts (typically <code>.bashrc</code>) means that every time you login to Cirrus, the conda environment will try to initialise by reading lots of files within the conda installation. This approach was designed for the case where a user has installed conda on their personal device and so is the only user of the file system. For shared file systems such as those on Cirrus, this places a large load on the file system and will lead to you seeing  slow login times and slow response from your command line on Cirrus. It will also lead to degraded read/write performance from the work file systems for you and other users so should be avoided at  all costs.</p> <p>If you have previously installed a conda distribution and answered \"yes\" to the question about adding the initialisation to shell configuration files, you should edit your <code>~/.bashrc</code> file to remove the conda initialisation entries. This means deleting the lines that look something like:</p> <pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('/work/t01/t01/auser/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\neval \"$__conda_setup\"\nelse\nif [ -f \"/work/t01/t01/auser/miniconda3/etc/profile.d/conda.sh\" ]; then\n. \"/work/t01/t01/auser/miniconda3/etc/profile.d/conda.sh\"\nelse\nexport PATH=\"/work/t01/t01/auser/miniconda3/bin:$PATH\"\nfi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n</code></pre>"},{"location":"user-guide/python/#running-python","title":"Running Python","text":""},{"location":"user-guide/python/#example-serial-python-submission-script","title":"Example serial Python submission script","text":"<pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=python_test\n#SBATCH --ntasks=1\n#SBATCH --time=00:10:00\n\n# Replace [budget code] below with your project code (e.g. t01)\n#SBATCH --account=[budget code]\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n# Load the Python module, ...\nmodule load cray-python\n\n# ..., or, if using local virtual environment\nsource &lt;&lt;path to virtual environment&gt;&gt;/bin/activate\n\n# Run your Python program\npython python_test.py\n</code></pre>"},{"location":"user-guide/python/#example-mpi4py-job-submission-script","title":"Example mpi4py job submission script","text":"<p>Programs that have been parallelised with mpi4py can be run on the Cirrus compute nodes. Unlike the serial Python submission script however, we must launch the Python interpreter using <code>srun</code>. Failing to do so will result in Python running a single MPI rank only. </p> <pre><code>#!/bin/bash --login\n# Slurm job options (job-name, compute nodes, job time)\n#SBATCH --job-name=mpi4py_test\n#SBATCH --nodes=2\n#SBATCH --exclusive\n#SBATCH --ntasks-per-node=288\n#SBATCH --cpus-per-task=1\n#SBATCH --time=0:10:0\n\n# Replace [budget code] below with your budget code (e.g. t01)\n#SBATCH --account=[budget code]\n#SBATCH --partition=standard\n#SBATCH --qos=standard\n\n# Load the Python module, ...\nmodule load cray-python\n\n# ..., or, if using local virtual environment\nsource &lt;&lt;path to virtual environment&gt;&gt;/bin/activate\n\n# Pass cpus-per-task setting to srun\nexport SRUN_CPUS_PER_TASK=${SLURM_CPUS_PER_TASK}\n\n# Run your Python program\n#   Note that srun MUST be used to wrap the call to python,\n#   otherwise your code will run serially\nsrun --distribution=block:block --hint=nomultithread python mpi4py_test.py\n</code></pre> <p>Tip</p> <p>If you have installed your own packages you will need to activate your local Python environment within your job submission script as shown at the end of Installing your own Python packages (with pip).</p>"},{"location":"user-guide/python/#using-jupyterlab-on-archer2","title":"Using JupyterLab on ARCHER2","text":"<p>It is possible to view and run Jupyter notebooks from both login nodes  and compute nodes on Cirrus.</p> <p>Note</p> <p>You can test such notebooks on the login nodes, but please do not attempt to run any computationally intensive work. Jobs may get killed once they hit a CPU limit on login nodes.</p> <p>Please follow these steps.</p> <ol> <li> <p>Install JupyterLab in your work directory.    <pre><code>module load cray-python\nexport PYTHONUSERBASE=/work/t01/t01/auser/.local\nexport PATH=$PYTHONUSERBASE/bin:$PATH\n# source &lt;&lt;path to virtual environment&gt;&gt;/bin/activate  # If using a virtualenvironment uncomment this line and remove the --user flag from the next\n\npip install --user jupyterlab\n</code></pre></p> </li> <li> <p>If you want to test JupyterLab on the login node please go straight to step 3.    To run your Jupyter notebook on a compute node, you first need to run an interactive session.    <pre><code>srun --nodes=1 --exclusive --time=00:20:00 --account=&lt;your_budget&gt; \\\n     --partition=standard --qos=short \\\n     --pty /bin/bash\n</code></pre>    Your prompt will change to something like below.    <pre><code>[auser@cs-n0034:/tmp]$\n</code></pre>    In this case, the node id is <code>cs-n0034</code>. Now execute the following on the compute node.    <pre><code>cd /work/t01/t01/auser # Update the path to your work directory\nexport PYTHONUSERBASE=$(pwd)/.local\nexport PATH=$PYTHONUSERBASE/bin:$PATH\nexport HOME=$(pwd)\nmodule load cray-python\n# source &lt;&lt;path to virtual environment&gt;&gt;/bin/activate  # If using a virtualenvironment uncomment this line\n</code></pre></p> </li> <li> <p>Run the JupyterLab server.    <pre><code>export JUPYTER_RUNTIME_DIR=$(pwd)\njupyter lab --ip=0.0.0.0 --no-browser\n</code></pre>    Once it's started, you will see a URL printed in the terminal window of     the form <code>http://127.0.0.1:&lt;port_number&gt;/lab?token=&lt;string&gt;</code>; we'll need this URL for    step 6.</p> </li> <li> <p>Please skip this step if you are connecting from a machine running Windows.    Open a new terminal window on your laptop and run the following command.    <pre><code>ssh &lt;username&gt;@login.cirrus.ac.uk -L&lt;port_number&gt;:&lt;node_id&gt;:&lt;port_number&gt;\n</code></pre>    where <code>&lt;username&gt;</code> is your username, and <code>&lt;node_id&gt;</code> is the id of the node you're     currently on (for a login node, this will be <code>login01</code>, or similar; on a compute     node, it will be a mix of numbers and letters). In our example, <code>&lt;node_id&gt;</code>    is <code>cs-n0034</code>. Note, please use the same port number as that shown in the URL of    step 3. This number may vary, likely values are 8888 or 8889.</p> </li> <li> <p>Please skip this step if you are connecting from Linux or macOS. If you are connecting from Windows, you should use MobaXterm to configure an SSH tunnel as follows.</p> <ul> <li>Click on the <code>Tunnelling</code> button above the MobaXterm terminal. Create a new tunnel by clicking on <code>New SSH tunnel</code> in the window that opens.</li> <li>In the new window that opens, make sure the <code>Local port forwarding</code> radio button is selected.</li> <li>In the <code>forwarded port</code> text box on the left under <code>My computer with MobaXterm</code>, enter the port number indicated in the JupyterLab server output (e.g., 8888 or 8890).</li> <li>In the three text boxes on the bottom right under <code>SSH server</code> enter <code>login.archer2.ac.uk</code>, your ARCHER2 username and then <code>22</code>.</li> <li>At the top right, under <code>Remote server</code>, enter the id of the login or compute node running the JupyterLab server and the associated port number.</li> <li>Click on the <code>Save</code> button.</li> <li>In the tunnelling window, you will now see a new row for the settings you just entered. If you like, you can give a name to the tunnel in the leftmost column to identify it.</li> <li>Click on the small key icon close to the right for the new connection to tell MobaXterm which SSH private key to use when connecting to ARCHER2. You should tell it to use the same <code>.ppk</code> private key that you normally use when connecting to ARCHER2.</li> <li>The tunnel should now be configured. Click on the small start button (like a play '&gt;' icon) for the new tunnel to open it. You'll be asked to enter your ARCHER2 account password -- please do so.</li> </ul> </li> <li> <p>Now, if you open a browser window locally, you should be able to navigate to the URL    from step 3, and this should display the JupyterLab server. If JupyterLab is running    on a compute node, the notebook will be available for the length of the interactive    session you have requested.</p> </li> </ol> <p>Warning</p> <p>Please do not use the other http address given by the JupyterLab output, the one formatted <code>http://&lt;node_id&gt;:&lt;port_number&gt;/lab?token=&lt;string&gt;</code>. Your local browser will not recognise the <code>&lt;node_id&gt;</code> part of the address.</p>"},{"location":"user-guide/resource_management/","title":"File and Resource Management","text":"<p>This section covers some of the tools and technical knowledge that will be key to maximising the usage of the Cirrus system, such as the online administration tool SAFE and checking resources available.</p> <p>Default file permissions are then outlined, along with a description of changing these permissions to the desired setting. This leads on to the sharing of data between users and systems often a vital tool for project groups and collaboration.</p> <p>Finally we cover some guidelines for I/O and data archiving on Cirrus.</p>"},{"location":"user-guide/resource_management/#the-cirrus-administration-web-site-safe","title":"The Cirrus Administration Web Site (SAFE)","text":"<p>All users have a login and password on the Cirrus Administration Web Site (also know as the 'SAFE'): SAFE. Once logged into this web site, users can find out much about their usage of the Cirrus system, including:</p> <ul> <li>Account details - password reset, change contact details</li> <li>Project details - project code, start and end dates</li> <li>Resource balance - how much time is left in each project you are a member   of</li> <li>File system details - current usage and quotas</li> <li>Reports - generate reports on your usage over a specified period,   including individual job records</li> <li>Service desk - raise queries and track progress of open queries</li> </ul>"},{"location":"user-guide/resource_management/#checking-your-resource-allocation","title":"Checking your resource allocation","text":"<p>You can view these details by logging into the SAFE.</p> <p>Use the Login accounts menu to select the user account that you wish to query. The page for the login account will summarise the resources available to account.</p> <p>You can also generate reports on your usage over a particular period and examine the details of how much resource individual jobs on the system cost. To do this use the Service information menu and select Report generator.</p>"},{"location":"user-guide/resource_management/#storage-quotas","title":"Storage quotas","text":"<p>Storage quotas on Cirrus are managed via SAFE.</p> <p>For information on how to query quotas and use live on the system see the descriptions in the Data Management and Transfer section</p>"},{"location":"user-guide/resource_management/#backup-policies","title":"Backup policies","text":"<ul> <li>The <code>/home</code> file system is not backed up.</li> <li>The <code>/work</code> file system is not backed up.</li> </ul> <p>We strongly advise that you keep copies of any critical data on on an alternative system that is fully backed up.</p>"},{"location":"user-guide/resource_management/#sharing-data-with-other-cirrus-users","title":"Sharing data with other Cirrus users","text":"<p>How you share data with other Cirrus users depends on whether or not they belong to the same project as you. Each project has two shared folders that can be used for sharing data.</p>"},{"location":"user-guide/resource_management/#sharing-data-with-cirrus-users-in-your-project","title":"Sharing data with Cirrus users in your project","text":"<p>Each project has an inner shared folder on the <code>/home</code> and <code>/work</code> filesystems:</p> <pre><code>/home/[project code]/[project code]/shared\n\n/work/[project code]/[project code]/shared\n</code></pre> <p>This folder has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is <code>x01</code> the inner shared folder on the <code>/work</code> file system would be located at <code>/work/x01/x01/shared</code>.</p>"},{"location":"user-guide/resource_management/#sharing-data-with-all-cirrus-users","title":"Sharing data with all Cirrus users","text":"<p>Each project also has an outer shared folder on the <code>/home</code> and <code>/work</code> filesystems:</p> <pre><code>/home/[project code]/shared\n\n/work/[project code]/shared\n</code></pre> <p>It is writable by all project members and readable by any user on the system. You can place any data you wish to share with other Cirrus users who are not members of your project in this directory. For example, if your project code is <code>x01</code> the outer shared folder on the <code>/work</code> file system would be located at <code>/work/x01/shared</code>.</p>"},{"location":"user-guide/resource_management/#file-permissions-and-security","title":"File permissions and security","text":"<p>You should check the permissions of any files that you place in the shared area, especially if those files were created in your own Cirrus account. Files of the latter type are likely to be readable by you only.</p> <p>The chmod command below shows how to make sure that a file placed in the outer shared folder is also readable by all Cirrus users.</p> <pre><code>chmod a+r /work/x01/shared/your-shared-file.txt\n</code></pre> <p>Similarly, for the inner shared folder, chmod can be called such that read permission is granted to all users within the x01 project.</p> <pre><code>chmod g+r /work/x01/x01/shared/your-shared-file.txt\n</code></pre> <p>If you're sharing a set of files stored within a folder hierarchy the chmod is slightly more complicated.</p> <pre><code>chmod -R a+Xr /work/x01/shared/my-shared-folder\nchmod -R g+Xr /work/x01/x01/shared/my-shared-folder\n</code></pre> <p>The <code>-R</code> option ensures that the read permission is enabled recursively and the <code>+X</code> guarantees that the user(s) you're sharing the folder with can access the subdirectories below my-shared-folder.</p> <p>Default Unix file permissions can be specified by the <code>umask</code> command. The default umask value on Cirrus is 22, which provides \"group\" and \"other\" read permissions for all files created, and \"group\" and \"other\" read and execute permissions for all directories created. This is highly undesirable, as it allows everyone else on the system to access (but at least not modify or delete) every file you create. Thus it is strongly recommended that users change this default umask behaviour, by adding the command <code>umask 077</code> to their <code>$HOME/.profile</code> file. This umask setting only allows the user access to any file or directory created. The user can then selectively enable \"group\" and/or \"other\" access to particular files or directories if required.</p>"},{"location":"user-guide/resource_management/#file-types","title":"File types","text":""},{"location":"user-guide/resource_management/#ascii-or-formatted-files","title":"ASCII (or formatted) files","text":"<p>These are the most portable, but can be extremely inefficient to read and write. There is also the problem that if the formatting is not done correctly, the data may not be output to full precision (or to the subsequently required precision), resulting in inaccurate results when the data is used. Another common problem with formatted files is FORMAT statements that fail to provide an adequate range to accommodate future requirements, e.g. if we wish to output the total number of processors, NPROC, used by the application, the statement:</p> <pre><code>WRITE (*,'I3') NPROC\n</code></pre> <p>will not work correctly if NPROC is greater than 999.</p>"},{"location":"user-guide/resource_management/#binary-or-unformatted-files","title":"Binary (or unformatted) files","text":"<p>These are much faster to read and write, especially if an entire array is read or written with a single READ or WRITE statement. However the files produced may not be readable on other systems.</p> <p>GNU compiler <code>-fconvert=swap</code> compiler option. This compiler option often needs to be used together with a second option <code>-frecord-marker</code>, which specifies the length of record marker (extra bytes inserted before or after the actual data in the binary file) for unformatted files generated on a particular system. To read a binary file generated by a big-endian system on Cirrus, use <code>-fconvert=swap -frecord-marker=4</code>. Please note that due to the same 'length of record marker' reason, the unformatted files generated by GNU and other compilers on Cirrus are not compatible. In fact, the same WRITE statements would result in slightly larger files with GNU compiler. Therefore it is recommended to use the same compiler for your simulations and related pre- and post-processing jobs.</p>"},{"location":"user-guide/resource_management/#other-file-formats","title":"Other file formats","text":""},{"location":"user-guide/resource_management/#direct-access-files","title":"Direct access files","text":"<p>Fortran unformatted files with specified record lengths. These may be more portable between different systems than ordinary (i.e. sequential IO) unformatted files, with significantly better performance than formatted (or ASCII) files. The \"endian\" issue will, however, still be a potential problem.</p>"},{"location":"user-guide/resource_management/#portable-data-formats","title":"Portable data formats","text":"<p>These machine-independent formats for representing scientific data are specifically designed to enable the same data files to be used on a wide variety of different hardware and operating systems. The most common formats are:</p> <ul> <li>netCDF: http://www.unidata.ucar.edu/software/netcdf/</li> <li>HDF: http://www.hdfgroup.org/HDF5/</li> </ul> <p>It is important to note that these portable data formats are evolving standards, so make sure you are aware of which version of the standard/software you are using, and keep up-to-date with any backward-compatibility implications of each new release.</p>"},{"location":"user-guide/resource_management/#file-io-performance-guidelines","title":"File IO Performance Guidelines","text":"<p>Here are some general guidelines</p> <ul> <li>Whichever data formats you choose, it is vital that you test that you   can access your data correctly on all the different systems where it   is required. This testing should be done as early as possible in the   software development or porting process (i.e. before you generate lots   of data from expensive production runs), and should be repeated with   every major software upgrade.</li> <li>Document the file formats and metadata of your important data files   very carefully. The best documentation will include a copy of the   relevant I/O subroutines from your code. Of course, this documentation   must be kept up-to-date with any code modifications.</li> <li>Use binary (or unformatted) format for files that will only be used on   the Intel system, e.g. for checkpointing files. This will give the   best performance. Binary files may also be suitable for larger output   data files, if they can be read correctly on other systems.</li> <li>Most codes will produce some human-readable (i.e. ASCII) files to   provide some information on the progress and correctness of the   calculation. Plan ahead when choosing format statements to allow for   future code usage, e.g. larger problem sizes and processor counts.</li> <li>If the data you generate is widely shared within a large community, or   if it must be archived for future reference, invest the time and   effort to standardise on a suitable portable data format, such as   netCDF or HDF.</li> </ul>"},{"location":"user-guide/resource_management/#common-io-patterns","title":"Common I/O patterns","text":"<p>There is a number of I/O patterns that are frequently used in applications:</p>"},{"location":"user-guide/resource_management/#single-file-single-writer-serial-io","title":"Single file, single writer (Serial I/O)","text":"<p>A common approach is to funnel all the I/O through a single master process. Although this has the advantage of producing a single file, the fact that only a single client is doing all the I/O means that it gains little benefit from the parallel file system.</p>"},{"location":"user-guide/resource_management/#file-per-process-fpp","title":"File-per-process (FPP)","text":"<p>One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand but has the disadvantage that, at the end of the calculation, the data is spread across many different files and may therefore be difficult to use for further analysis without a data reconstruction stage.</p>"},{"location":"user-guide/resource_management/#single-file-multiple-writers-without-collective-operations","title":"Single file, multiple writers without collective operations","text":"<p>There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset; parallel I/O libraries such as MPI-IO, HDF5 and NetCDF also enable this.</p> <p>Shared-file I/O has the advantage that all the data is organised correctly in a single file making analysis or restart more straightforward.</p> <p>The problem is that, with many clients all accessing the same file, there can be a lot of contention for file system resources.</p>"},{"location":"user-guide/resource_management/#single-shared-file-with-collective-writes-ssf","title":"Single Shared File with collective writes (SSF)","text":"<p>The problem with having many clients performing I/O at the same time is that, to prevent them clashing with each other, the I/O library may have to take a conservative approach. For example, a file may be locked while each client is accessing it which means that I/O is effectively serialised and performance may be poor.</p> <p>However, if I/O is done collectively where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file.</p>"},{"location":"user-guide/resource_management/#achieving-efficient-io","title":"Achieving efficient I/O","text":"<p>This section provides information on getting the best performance out of the <code>/work</code> parallel file system on Cirrus when writing data, particularly using parallel I/O patterns.</p> <p>You may find that using the <code>/user-guide/solidstate</code> gives better performance than <code>/work</code> for some applications and IO patterns.</p>"},{"location":"user-guide/resource_management/#lustre","title":"Lustre","text":"<p>The Cirrus <code>/work</code> file system use Lustre as a parallel file system technology. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns.</p>"},{"location":"user-guide/resource_management/#striping","title":"Striping","text":"<p>One of the main factors leading to the high performance of <code>/work</code> Lustre file systems is the ability to stripe data across multiple Object Storage Targets (OSTs) in a round-robin fashion. Files are striped when the data is split up in chunks that will then be stored on different OSTs across the <code>/work</code> file system. Striping might improve the I/O performance because it increases the available bandwidth since multiple processes can read and write the same files simultaneously. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results.</p> <p>Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on directory where your output files will be written so that all output files inherit the settings.</p>"},{"location":"user-guide/resource_management/#default-configuration","title":"Default configuration","text":"<p>The work (Lustre) file system on Cirrus has the following default stripe settings:</p> <ul> <li>A default stripe count of 1</li> <li>A default stripe size of 1 MiB (1048576 bytes)</li> </ul> <p>These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is <code>lfs getstripe</code>. For example, to query the stripe settings of an already created directory <code>res_dir</code>:</p> <pre><code>[auser@login01:auser]$ lfs getstripe res_dir/\nres_dir\nstripe_count:   1 stripe_size:    1048576 stripe_offset:  -1\n</code></pre>"},{"location":"user-guide/resource_management/#setting-custom-striping-configurations","title":"Setting Custom Striping Configurations","text":"<p>Users can set stripe settings for a directory (or file) using the <code>lfs setstripe</code> command. The options for <code>lfs setstripe</code> are:</p> <ul> <li><code>[--stripe-count|-c]</code> to set the stripe count; 0 means use the system   default (usually 1) and -1 means stripe over all available OSTs.</li> <li><code>[--stripe-size|-s]</code> to set the stripe size; 0 means use the system   default (usually 1 MB) otherwise use k, m or g for KB, MB or GB   respectively</li> <li><code>[--stripe-index|-i]</code> to set the OST index (starting at 0) on which to   start striping for this file. An index of -1 allows the MDS to choose   the starting index and it is strongly recommended, as this allows   space and load balancing to be done by the MDS as needed.</li> </ul> <p>For example, to set a stripe size of 4 MiB for the existing directory <code>res_dir</code>, along with maximum striping count you would use:</p> <pre><code>[auser@login01:auser]$ lfs setstripe -s 4m -c -1 res_dir/\n</code></pre>"},{"location":"user-guide/sw-environment/","title":"Software environment","text":"<p>The software environment on Cirrus is managed using the  Lmod software. Selecting which software is available in your environment is primarily controlled through the <code>module</code> command. By loading and switching software modules you control which software and versions are available to you.</p> <p>Information</p> <p>A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages.</p> <p>All users on Cirrus start with the default software environment loaded.</p> <p>Software modules on Cirrus are provided by both HPE (usually known as the HPE Cray Programming Environment, CPE) and by EPCC, who provide the Service Provision, and Computational Science and Engineering services.</p> <p>In this section, we provide:</p> <ul> <li>A brief overview of the <code>module</code> command</li> <li>A brief description of how the <code>module</code> command manipulates your      environment</li> </ul>"},{"location":"user-guide/sw-environment/#using-the-module-command","title":"Using the <code>module</code> command","text":"<p>We only cover basic usage of the Lmod <code>module</code> command here. For full documentation please see the Lmod documentation</p> <p>The <code>module</code> command takes a subcommand to indicate what operation you wish to perform. Common subcommands are:</p> <ul> <li><code>module restore</code> - Restore the default module setup (i.e. as if you had logged      out and back in again)</li> <li><code>module list [name]</code> - List modules currently loaded in your      environment, optionally filtered by <code>[name]</code></li> <li><code>module avail [name]</code> - List modules available, optionally      filtered by <code>[name]</code></li> <li><code>module spider [name][/version]</code> - Search available modules (including hidden       modules) and provide information on modules</li> <li><code>module load name</code> - Load the module called <code>name</code> into your      environment</li> <li><code>module remove name</code> - Remove the module called <code>name</code> from your      environment</li> <li><code>module help name</code> - Show help information on module <code>name</code></li> <li><code>module show name</code> - List what module <code>name</code> actually does to your      environment</li> </ul> <p>These are described in more detail below.</p> <p>Tip</p> <p>Lmod allows you to use the <code>ml</code> shortcut command. Without any arguments, <code>ml</code> behaves like <code>module list</code>; when a module name is specified to <code>ml</code>, <code>ml</code> behaves like <code>module load</code>.</p> <p>Note</p> <p>You will often have to include <code>module</code> commands in any job submission scripts to setup the software to use in your jobs. Generally, if you load modules in interactive sessions, these loaded modules do not carry over into any job submission scripts.</p> <p>Important</p> <p>You should not use the <code>module purge</code> command on Cirrus as this will cause issues for the HPE Cray programming environment. If you wish to  reset your modules, you should use the <code>module restore</code> command instead.</p>"},{"location":"user-guide/sw-environment/#information-on-the-available-modules","title":"Information on the available modules","text":"<p>The key commands for getting information on modules are covered in more detail below. They are:</p> <ul> <li><code>module list</code></li> <li><code>module avail</code></li> <li><code>module spider</code></li> <li><code>module help</code></li> <li><code>module show</code></li> </ul>"},{"location":"user-guide/sw-environment/#module-list","title":"<code>module list</code>","text":"<p>The <code>module list</code> command will give the names of the modules and their versions you have presently loaded in your environment:</p> <pre><code>auser@login03:~&gt; module list\n\nCurrently Loaded Modules:\n  1) craype-x86-turin                  6) cce/19.0.0         11) cray-libsci/25.03.0\n  2) libfabric/1.22.0                  7) cse_env/0.2        12) PrgEnv-cray/8.6.0\n  3) craype-network-ofi                8) craype/2.7.34      13) epcc-setup-env\n  4) perftools-base/25.03.0            9) cray-dsmml/0.3.1   14) load-epcc-module\n  5) xpmem/0.2.119-1.3_gef379be13330  10) cray-mpich/8.1.32\n</code></pre> <p>All users start with a default set of modules loaded corresponding to:</p> <ul> <li>The HPE Cray Compiling Environment (CCE): includes the HPE Cray clang and Fortran compilers</li> <li>HPE Cray MPICH: The HPE Cray MPI library</li> <li>HPE Cray LibSci: The HPE Cray numerical libraries (including BLAS/LAPACK and ScaLAPACK)</li> </ul>"},{"location":"user-guide/sw-environment/#module-avail","title":"<code>module avail</code>","text":"<p>Finding out which software modules are currently available to load on the system is performed using the <code>module avail</code> command. To list all software modules currently available to load, use:</p> <pre><code>auser@login01:~&gt; module avail\n\n---- /work/y07/shared/cirrus-ex/cirrus-ex-lmod/apps/mpi/crayclang/16.0/ofi/1.0/x86-turin/1.0/cray-mpich/8.0 ----\n   xthi/1.0\n\n---- /work/y07/shared/cirrus-ex/cirrus-ex-software/spack-cirrus-ex/0.2/cirrus-ex-cse/modules/cce/19.0.0 ----\n   eigen/3.4.0    lammps/20250612    metis/5.1.0    parmetis/4.0.3    petsc/3.23.4\n\n---------------- /opt/cray/pe/lmod/modulefiles/mpi/crayclang/16.0/ofi/1.0/cray-mpich/8.0 ----------------\n   cray-hdf5-parallel/1.14.3.5    cray-mpixlate/1.0.7    cray-parallel-netcdf/1.12.3.17\n\n----- /work/y07/shared/cirrus-ex/cirrus-ex-software/spack-cirrus-ex/0.2/cirrus-ex-cse/modules/Core ------\n   castep/24.1    likwid/5.4.1       openfoam/2412     python-venv/1.0\n   cp2k/2025.1    openfoam-org/12    py-torch/2.7.1    python/3.11.7\n\n-------------------------- /work/y07/shared/cirrus-ex/cirrus-ex-lmod/apps/core --------------------------\n   vasp/6/6.5.1\n\n------------------------- /work/y07/shared/cirrus-ex/cirrus-ex-lmod/utils/core --------------------------\n   cmake/4.1.2    cse_env/0.2      (L,D)    epcc-setup-env (L)    spack/1.0.2/0.1\n   cse_env/0.1    epcc-reframe/0.5          reframe/4.8.4         spack/1.0.2/0.2 (D)\n\n---------------------- /opt/cray/pe/lmod/modulefiles/comnet/crayclang/16.0/ofi/1.0 ----------------------\n   cray-mpich-abi/8.1.32    cray-mpich-abi/9.0.0 (D)    cray-mpich/8.1.32 (L)    cray-mpich/9.0.0 (D)\n\n------------------------- /opt/cray/pe/lmod/modulefiles/compiler/crayclang/16.0 -------------------------\n   cray-hdf5/1.14.3.5    cray-libsci/25.03.0 (L)\n\n------------------------------ /opt/cray/pe/lmod/modulefiles/mix_compilers ------------------------------\n   aocc-mixed/5.0.0    gcc-native-mixed/12.2    gcc-native-mixed/14.2 (D)    intel-oneapi-mixed/2025.0\n   cce-mixed/19.0.0    gcc-native-mixed/13.3    intel-mixed/2025.0\n\n---------------------------- /opt/cray/pe/lmod/modulefiles/perftools/25.03.0 ----------------------------\n   perftools-lite-events    perftools-lite-hbm      perftools-lite       perftools\n   perftools-lite-gpu       perftools-lite-loops    perftools-preload\n\n------------------------------- /opt/cray/pe/lmod/modulefiles/net/ofi/1.0 -------------------------------\n   cray-openshmemx/11.7.4\n\n---------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-turin/1.0 ----------------------------\n   cray-fftw/3.3.10.10\n\n\n\n...output trimmed...\n</code></pre> <p>This will list all the names and versions of the modules that you can currently load. Note that other modules may be defined but not available to you as they depend on modules you do not have loaded. Lmod only shows modules that you can currently load, not all those that are defined. You can search for modules that are not currently visble to you using the <code>module spider</code> command - we  cover this in more detail below.</p> <p>Note also, that not all modules may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted.</p> <p>You can list all the modules of a particular type by providing an argument to the <code>module avail</code> command. For example, to list all available versions of the HPE Cray FFTW library, use:</p> <pre><code>auser@login03:~&gt;  module avail cray-fftw\n\n------------------------------- /opt/cray/pe/lmod/modulefiles/cpu/x86-turin/1.0 --------------------------------\n   cray-fftw/3.3.10.10\n\nModule defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree.\nSee https://lmod.readthedocs.io/en/latest/060_locating.html for details.\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre>"},{"location":"user-guide/sw-environment/#module-spider","title":"<code>module spider</code>","text":"<p>The <code>module spider</code> command is used to find out which modules are defined on the system. Unlike <code>module avail</code>, this includes modules that are not currently able to be loaded due to the fact you have not yet loaded dependencies to make them directly available.</p> <p><code>module spider</code> takes 3 forms:</p> <ul> <li><code>module spider</code> without any arguments lists all modules defined on the system</li> <li><code>module spider &lt;module&gt;</code> shows information on which versions of <code>&lt;module&gt;</code> are    defined on the system</li> <li><code>module spider &lt;module&gt;/&lt;version&gt;</code> shows information on the specific version of     the module defined on the system, including dependencies that must be loaded     before this module can be loaded (if any)</li> </ul> <p>If you cannot find a module that you expect to be on the system using <code>module avail</code> then you can use <code>module spider</code> to find out which dependencies you need to load to make the module available.</p> <p>For example, the module <code>cray-netcdf-hdf5parallel</code> is installed on Cirrus but it will not be found by <code>module avail</code>:</p> <pre><code>auser@login03:~&gt; module avail cray-netcdf-hdf5parallel\nNo module(s) or extension(s) found!\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>We can use <code>module spider</code> without any arguments to verify it exists and list the versions available:</p> <pre><code>auser@login03:~&gt; module spider\n\n-------------------------------------------------------------------------------------------------------------\nThe following is a list of the modules and extensions currently available:\n-------------------------------------------------------------------------------------------------------------\n  PrgEnv-aocc: PrgEnv-aocc/8.6.0\n\n  PrgEnv-cray: PrgEnv-cray/8.6.0\n\n  PrgEnv-gnu: PrgEnv-gnu/8.6.0\n\n  PrgEnv-intel: PrgEnv-intel/8.6.0\n\n  aocc: aocc/5.0.0\n\n  aocc-mixed: aocc-mixed/5.0.0\n\n  atp: atp/3.15.6\n\n  castep: castep/24.1\n\n  cce: cce/19.0.0\n\n  cce-mixed: cce-mixed/19.0.0\n\n  cmake: cmake/4.1.2\n\n\n...output trimmed...\n</code></pre> <p>Now we know which versions are available, we can use <code>module spider cray-netcdf-hdf5parallel/4.9.0.1</code> to find out how we can make it available:</p> <pre><code>auser@login03:~&gt; module spider libxc/7.0.0\n\n-------------------------------------------------------------------------------------------------------------\n  libxc: libxc/7.0.0\n-------------------------------------------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"libxc/7.0.0\" module is available to load.\n\n      gcc-native/12.2\n      gcc-native/13.3\n      gcc-native/14.2\n      load-epcc-module  cse_env/0.2  gcc-spack/14.2-db4a2iu\n\n    Help:\n      Libxc is a library of exchange-correlation functionals for density-\n      functional theory.\n</code></pre> <p>There is a lot of information here, but what the output is essentailly telling us is that in order to have <code>libxc/7.0.0</code> available to  load we need to have loaded a compiler (any version of GCC) and some utility modules that are already loaded for all users. We can satisfy all of the dependencies by loading <code>PrgEnv-gnu</code> (to load a <code>gcc-native</code> module), and then we can use <code>module avail libxc</code> again to show that the module is now available to load:</p> <pre><code>[auser@login03:~] module load PrgEnv-gnu\n\nLmod is automatically replacing \"cce/19.0.0\" with \"gcc-native/14.2\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.6.0\" with \"PrgEnv-gnu/8.6.0\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-libsci/25.03.0     2) cray-mpich/8.1.32\n\n[auser@login03 ~]$ module avail libxc\n\n------- /work/y07/shared/cirrus-ex/cirrus-ex-software/spack-cirrus-ex/0.2/cirrus-ex-cse/modules/gcc/14.2 -------\n   libxc/7.0.0\n\nModule defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree.\nSee https://lmod.readthedocs.io/en/latest/060_locating.html for details.\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\"\n</code></pre>"},{"location":"user-guide/sw-environment/#module-help","title":"<code>module help</code>","text":"<p>If you want more info on any of the modules, you can use the <code>module help</code> command:</p> <pre><code>[auser@login03:~] module help gromacs\n</code></pre>"},{"location":"user-guide/sw-environment/#module-show","title":"<code>module show</code>","text":"<p>The <code>module show</code> command reveals what operations the module actually performs to change your environment when it is loaded. For example, for the default FFTW module:</p> <pre><code>[auser@login03:~] module show gromacs\n\n  [...]\n</code></pre>"},{"location":"user-guide/sw-environment/#loading-removing-and-swapping-modules","title":"Loading, removing and swapping modules","text":"<p>To change your environment and make different software available you use the following commands which we cover in more detail below.</p> <ul> <li><code>module load</code></li> <li><code>module remove</code></li> </ul>"},{"location":"user-guide/sw-environment/#module-load","title":"<code>module load</code>","text":"<p>To load a module to use the <code>module load</code> command. For example, to load the default version of GROMACS into your environment, use:</p> <pre><code>[auser@login03:~] module load gromacs\n</code></pre> <p>Once you have done this, your environment will be setup to use GROMACS. The above command will load the default version of GROMACS. If you need a specific version of the software, you can add more information:</p> <pre><code>[auser@login01:~] module load gromacs/2022.4 \n</code></pre> <p>will load GROMACS version 2022.4 into your environment, regardless of the default.</p> <p>In Lmod, the <code>module load</code> command will swap a current loaded module for the one specified in the command if there is a conflict.</p>"},{"location":"user-guide/sw-environment/#module-remove","title":"<code>module remove</code>","text":"<p>If you want to remove software from your environment, <code>module remove</code> will remove a loaded module:</p> <pre><code>[auser@uan01:~] module remove gromacs\n</code></pre> <p>will unload what ever version of <code>gromacs</code> you might have loaded (even if it is not the default).</p>"},{"location":"user-guide/sw-environment/#shell-environment-overview","title":"Shell environment overview","text":"<p>When you log in to Cirrus, you are using the bash shell by default. As with any software, the bash shell has loaded a set of environment variables that can be listed by executing <code>printenv</code> or <code>export</code>.</p> <p>The environment variables listed before are useful to define the behaviour of the software you run. For instance, <code>OMP_NUM_THREADS</code> define the number of threads.</p> <p>To define an environment variable, you need to execute:</p> <pre><code>export OMP_NUM_THREADS=4\n</code></pre> <p>Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks.</p> <p>You can show the value of a specific environment variable if you print it:</p> <pre><code>echo $OMP_NUM_THREADS\n</code></pre> <p>Do not forget the dollar symbol. To remove an environment variable, just execute:</p> <pre><code>unset OMP_NUM_THREADS\n</code></pre> <p>Note that the dollar symbol is not included when you use the <code>unset</code> command.</p>"},{"location":"user-guide/sw-environment/#cgroup-control-of-login-resources","title":"cgroup control of login resources","text":"<p>Note that it not possible for a single user to  monopolise the resources on a login node as this is controlled by cgroups. This means that a user cannot usually slow down the response time for other users.</p>"}]}