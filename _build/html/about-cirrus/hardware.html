<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Hardware &#8212; Cirrus 1 documentation</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Cirrus 1 documentation" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="hardware">
<h1>Hardware<a class="headerlink" href="#hardware" title="Permalink to this headline">¶</a></h1>
<p>The Cirrus facility is based around a Intel ICE XA 59 node cluster that
provides the central computational resource.</p>
<div class="section" id="intel-ice-xa-cluster">
<h2>Intel ICE XA Cluster<a class="headerlink" href="#intel-ice-xa-cluster" title="Permalink to this headline">¶</a></h2>
<p>The Cirrus architecture consists of compute nodes connected together by
a single Infiniband fabric, with 56 compute nodes and 3 login nodes.</p>
<div class="section" id="compute-nodes">
<h3>Compute Nodes<a class="headerlink" href="#compute-nodes" title="Permalink to this headline">¶</a></h3>
<p>Cirrus compute nodes contain two 2.1 GHz, 18-core Intel Xeon E5-2695
(Broadwell) series processors. Each of the cores in these processors
support 2 hardware threads (Hyperthreads), which are enabled by default.</p>
<p>The compute nodes on Cirrus have 256 GB of memory shared between the two
processors. The memory is arranged in a non-uniform access (NUMA) form:
each 18-core processor is a single NUMA region with local memory of 128
GB. Access to the local memory by cores within a NUMA region has a lower
latency than accessing memory on the other NUMA region.</p>
<p>There are three levels of cache, configured as follows:</p>
<ul class="simple">
<li>L1 Cache 32 KB Instr., 32 KB Data (per core)</li>
<li>L2 Cache 256 KB (per core)</li>
<li>L3 Cache 45 MB (shared)</li>
</ul>
<p>There are 56 compute nodes on Cirrus giving a total of 2,016 cores).
When employing hyperthreads, the core count doubles to 4,032. NB
hyperthreads are currently switched on by default</p>
</div>
</div>
<div class="section" id="infiniband-fabric">
<h2>Infiniband fabric<a class="headerlink" href="#infiniband-fabric" title="Permalink to this headline">¶</a></h2>
<p>The system has a single infiniband (IB) fabric and every compute node
and login node has a single ib0 interface on it. The IB interface is
FDR, rated at 54.5Gbps. The Lustre servers have two connections on the
IB fabric and all Lustre file system IO traverses the IB fabric.</p>
</div>
<div class="section" id="filesystems-and-data-infrastructure">
<h2>Filesystems and Data Infrastructure<a class="headerlink" href="#filesystems-and-data-infrastructure" title="Permalink to this headline">¶</a></h2>
<p>There is currently a single filesystem available on Cirrus: the /lustre
filesystem. This filesystem is a collection of three high-performance,
parallel Lustre filesystems.</p>
<p>There is currently a total of 200 TB available in <code class="docutils literal"><span class="pre">/lustre</span></code> on Cirrus.
The cluster login and compute nodes mount the storage as /lustre, and
all home directories are available on all nodes.</p>
<p>The compute nodes are diskless. Each node boots from a cluster
management noded called the Rack Leader and NFS mounts the root file
system from this management node.</p>
<p><strong>NB Currently, the /lustre filesystem are not backed-up in any way.</strong></p>
</div>
<div class="section" id="parallel-i-o">
<h2>Parallel I/O<a class="headerlink" href="#parallel-i-o" title="Permalink to this headline">¶</a></h2>
<p>For a description of the terms associated with Lustre file systems
please see the description on Wikipedia:</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Lustre_(file_system)">Lustre File Systems
Description</a></li>
</ul>
<p>The default striping on the Lustre filesystem is 1 stripe, and the
default stripe size is 1 MiB. There are 4 OSTs.</p>
<p>The parallel I/O performance has been measured by writing to a single
shared file using MPI I/O. All cores per node were writing
simultaneously. Best performance was withh 4 nodes (144 cores) writing
tof 4 stripes of 1 MiB and was found to be around 1.5 GB/s.</p>
<p>Performance was evaluated using the EPCC &#8220;benchio&#8221; application, found
at: <a class="reference external" href="https://github.com/ARCHER-CSE/parallel-io">https://github.com/ARCHER-CSE/parallel-io</a></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Hardware</a><ul>
<li><a class="reference internal" href="#intel-ice-xa-cluster">Intel ICE XA Cluster</a><ul>
<li><a class="reference internal" href="#compute-nodes">Compute Nodes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#infiniband-fabric">Infiniband fabric</a></li>
<li><a class="reference internal" href="#filesystems-and-data-infrastructure">Filesystems and Data Infrastructure</a></li>
<li><a class="reference internal" href="#parallel-i-o">Parallel I/O</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/about-cirrus/hardware.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, G. Pringle, A. R. Turner.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="../_sources/about-cirrus/hardware.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>