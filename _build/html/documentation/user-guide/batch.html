<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Running Jobs on Cirrus &#8212; Cirrus 1 documentation</title>
    
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Cirrus 1 documentation" href="../../index.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="running-jobs-on-cirrus">
<h1>Running Jobs on Cirrus<a class="headerlink" href="#running-jobs-on-cirrus" title="Permalink to this headline">¶</a></h1>
<p>The Cirrus facility uses PBS (Portable Batch System) to schedule jobs.
Writing a submission script is typically the most convenient way to
submit your job to the job submission system. Example submission scripts
(with explanations) for the most common job types are provided below.</p>
<p>Interactive jobs are also available and can be particularly useful for
developing and debugging applications. More details are available below.</p>
<p>If you have any questions on how to run jobs on Cirrus do not hesitate
to contact the EPCC Helpdesk.</p>
<div class="section" id="using-pbs-pro">
<h2>Using PBS Pro<a class="headerlink" href="#using-pbs-pro" title="Permalink to this headline">¶</a></h2>
<p>You typically interact with PBS by (1) specifying PBS directives in job
submission scripts (see examples below) and (2) issuing PBS commands
from the login nodes.</p>
<p>There are three key commands used to interact with the PBS on the
command line:</p>
<ul class="simple">
<li>``             qsub         ``</li>
<li>``             qstat         ``</li>
<li>``             qdel         ``</li>
</ul>
<p>Check the PBS <code class="docutils literal"><span class="pre">man</span></code> page for more advanced commands:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">man</span> <span class="n">pbs</span>
</pre></div>
</div>
<div class="section" id="the-qsub-command">
<h3>The qsub command<a class="headerlink" href="#the-qsub-command" title="Permalink to this headline">¶</a></h3>
<p>The qsub command submits a job to PBS:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qsub</span> <span class="n">job_script</span><span class="o">.</span><span class="n">pbs</span>
</pre></div>
</div>
<p>This will submit your job script &#8220;job_script.pbs&#8221; to the job-queues.
See the sections below for details on how to write job scripts.</p>
<p>Note: To ensure the minimum wait time for your job, you should specify a
walltime as short as possible for your job (i.e. if your job is going to
run for 3 hours, do not specify 12 hours). On average, the longer the
walltime you specify, the longer you will queue for.</p>
</div>
<div class="section" id="the-qstat-command">
<h3>The qstat command<a class="headerlink" href="#the-qstat-command" title="Permalink to this headline">¶</a></h3>
<p>Use the command qstat to view the job queue. For example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">qstat</span> <span class="o">-</span><span class="n">q</span>
</pre></div>
</div>
<p>will list all available queues on the Cirrus facility.</p>
<p>You can view just your jobs by using:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qstat -u $USER
</pre></div>
</div>
<p>The &#8221; <code class="docutils literal"><span class="pre">-a</span></code> &#8221; option to qstat provides the output in a more useful
format.</p>
<p>To see more information about a queued job, use:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qstat -f $JOBID
</pre></div>
</div>
<p>This option may be useful when your job fails to enter a running state.
The output contains a PBS comment field which may explain why the job
failed to run.</p>
<p>If the batch system has calculated an estimated start time for a job, it
is possible to view this by adding the <code class="docutils literal"><span class="pre">-T</span></code> flag as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qstat -T $JOBID
</pre></div>
</div>
</div>
<div class="section" id="the-qdel-command">
<h3>The qdel command<a class="headerlink" href="#the-qdel-command" title="Permalink to this headline">¶</a></h3>
<p>Use this command to delete a job from Cirrus&#8217;s job queue. For example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>qdel $JOBID
</pre></div>
</div>
<p>will remove the job with ID <code class="docutils literal"><span class="pre">$JOBID</span></code> from the queue.</p>
</div>
</div>
<div class="section" id="output-from-pbs-jobs">
<h2>Output from PBS jobs<a class="headerlink" href="#output-from-pbs-jobs" title="Permalink to this headline">¶</a></h2>
<p>PBS produces standard output and standard error for each batch job can
be found in files <code class="docutils literal"><span class="pre">&lt;jobname&gt;.o&lt;Job</span> <span class="pre">ID&gt;</span></code> and <code class="docutils literal"><span class="pre">&lt;jobname&gt;.e&lt;Job</span> <span class="pre">ID&gt;</span></code>
respectively. These files appear in the job&#8217;s working directory once
your job has completed or its maximum allocated time to run (i.e. wall
time, see later sections) has ran out.</p>
</div>
<div class="section" id="running-parallel-jobs">
<h2>Running Parallel Jobs<a class="headerlink" href="#running-parallel-jobs" title="Permalink to this headline">¶</a></h2>
<p>This section describes how to write job submission scripts specifically
for different kinds of parallel jobs on Cirrus.</p>
<p>All parallel job submission scripts require (as a minimum) you to
specify three things:</p>
<ul class="simple">
<li>The number of virtual cores you require via the
&#8220;<code class="docutils literal"><span class="pre">-l</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">select=[cores]</span></code>&#8221; option. Each node has 36 physical
cores (2x 18-core sockets) but hyperthreads are enabled (2 per core).
Thus, to request 2 nodes, for instance, you must select 144 &#8220;cores&#8221;
(36 cores * 2 hyperthreads * 2 nodes.</li>
<li>The maximum length of time (i.e. walltime) you want the job to run
for via the &#8220;<code class="docutils literal"><span class="pre">-l</span> <span class="pre">walltime=[hh:mm:ss]</span></code>&#8221; option. To ensure the
minimum wait time for your job, you should specify a walltime as
short as possible for your job (i.e. if your job is going to run for
3 hours, do not specify 12 hours). On average, the longer the
walltime you specify, the longer you will queue for.</li>
<li>The project code that you want to charge the job to via the
&#8220;<code class="docutils literal"><span class="pre">-A</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[project</span> <span class="pre">code]</span></code>&#8221; option</li>
<li>By default compute nodes are shared, meaning other jobs may be placed
alongside yours if your resource request (with -l select) leaves some
cores free. To guarantee exclusive node usage, use the option
<code class="docutils literal"><span class="pre">-l</span> <span class="pre">place=excl</span></code>.</li>
<li>The project code that you want to charge the job to via the
&#8220;<code class="docutils literal"><span class="pre">-A</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">[project</span> <span class="pre">code]</span></code>&#8221; option</li>
<li><code class="docutils literal"><span class="pre">-N</span> <span class="pre">My_job</span></code>
Name for your job is set using <code class="docutils literal"><span class="pre">-N</span> <span class="pre">My_job</span></code>. In the examples below
the name will be &#8220;My_job&#8221;, but uou can replace &#8220;My_job&#8221; with any
name you want. The name will be used in various places. In particular
it will be used in the queue listing and to generate the name of your
output and/or error file(s). Note there is a limit on the size of the
name.</li>
</ul>
<p>In addition to these mandatory specifications, there are many other
options you can provide to PBS.</p>
<div class="section" id="pbs-submission-options">
<h3>PBS Submission Options<a class="headerlink" href="#pbs-submission-options" title="Permalink to this headline">¶</a></h3>
<p>This section provides more information on various options used when
submitting jobs to PBS on Cirrus. We also list a number of options that
<strong>should not</strong> be used on the system.</p>
<p>When specified in a job submission script, all PBS options start with a
&#8220;#PBS&#8221;-string. All options can also be specified directly on the command
line.</p>
</div>
<div class="section" id="parallel-job-launcher-mpiexec-mpt">
<h3>Parallel job launcher <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code><a class="headerlink" href="#parallel-job-launcher-mpiexec-mpt" title="Permalink to this headline">¶</a></h3>
<p>The job launcher for parallel jobs on Cirrus is <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> .</p>
<p>A sample MPI job launch line using <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> looks like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpiexec_mpt</span> <span class="o">-</span><span class="n">n</span> <span class="mi">72</span> <span class="o">-</span><span class="n">ppn</span> <span class="mi">36</span> <span class="o">./</span><span class="n">my_mpi_executable</span><span class="o">.</span><span class="n">x</span> <span class="n">arg1</span> <span class="n">arg2</span>
</pre></div>
</div>
<p>This will start the parallel executable &#8220;my_mpi_executable.x&#8221; with
arguments &#8220;arg1&#8221; and &#8220;arg2&#8221;. The job will be started using 36 MPI
processes, by default 36 MPI processes are placed on each compute node
using all of the virtual cores available.</p>
<p>The most important <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code> flags are:</p>
<blockquote>
<div><dl class="docutils">
<dt><code class="docutils literal"><span class="pre">-n</span> <span class="pre">[total</span> <span class="pre">number</span> <span class="pre">of</span> <span class="pre">MPI</span> <span class="pre">processes]</span></code></dt>
<dd>Specifies the total number of distributed memory parallel processes
(not including shared-memory threads). For jobs that use all
physical cores this will usually be a multiple of 18. The default on
Cirrus is 1.</dd>
<dt><code class="docutils literal"><span class="pre">-ppn</span> <span class="pre">[parallel</span> <span class="pre">processes</span> <span class="pre">per</span> <span class="pre">node]</span></code></dt>
<dd>Specifies the number of distributed memory parallel processes per
node. There is a choice of 1-18 for physical cores on Cirrus compute
nodes (1-36 if you are using HyperThreading) If you are running with
exclusive node usage, the most economic choice is always to run with
&#8220;fully-packed&#8221; nodes on all physical cores if possible, i.e.
<code class="docutils literal"><span class="pre">-N</span> <span class="pre">18</span></code> . Running &#8220;unpacked&#8221; or &#8220;underpopulated&#8221; (i.e. not using
all the physical cores on a node) is useful if you need large
amounts of memory per parallel process or you are using more than
one shared-memory thread per parallel process.</dd>
<dt><code class="docutils literal"><span class="pre">-nt</span> <span class="pre">[threads</span> <span class="pre">per</span> <span class="pre">parallel</span> <span class="pre">process]</span></code></dt>
<dd>Specifies the number of cores for each parallel process to use for
shared-memory threading. (This is in addition to the
<code class="docutils literal"><span class="pre">OMP_NUM_THREADS</span></code> environment variable if you are using OpenMP for
your shared memory programming.) The default on Cirrus is 1.</dd>
<dt><code class="docutils literal"><span class="pre">-j</span> <span class="pre">[hyperthreads]</span></code></dt>
<dd>Specifies the number of Intel HyperThreads to use for each physical
core. Valid values for this are 0, 1 or 2. 0 indicates that all
available HyperThreads should be used and hence is equivalent to 2
on Cirrus. The default is currently 2, but experience suggests -j 1
should give the best performance for most codes on Cirrus.</dd>
</dl>
</div></blockquote>
<p>Please use ``         man         mpiexec_mpt`` and
``         mpiexec_mpt`` -h to query further options.</p>
</div>
<div class="section" id="example-job-submission-script-for-mpi-parallel-job">
<h3>Example: job submission script for MPI parallel job<a class="headerlink" href="#example-job-submission-script-for-mpi-parallel-job" title="Permalink to this headline">¶</a></h3>
<p>A simple MPI job submission script to submit a job using 64 compute
nodes (maximum of 1536 physical cores) for 20 minutes would look like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login

# PBS job options (name, compute nodes, job time)
#PBS -N Example_MPI_Job
#PBS -l select=144
#PBS -l walltime=00:20:00

#To get exclusive node usage
#PBS -l excl

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A [budget code]

# Change to the directory that the job was submitted from
cd $PBS_O_WORKDIR

# Set the number of threads to 1
#   This prevents any system libraries from automatically
#   using threading.
export OMP_NUM_THREADS=1

# Launch the parallel job
#   Using 1536 MPI processes and 24 MPI processes per node
mpiexec_mpt -n 36 -ppn 18 -j 1 ./my_mpi_executable.x arg1 arg2 &gt; my_stdout.txt 2&gt; my_stderr.txt
</pre></div>
</div>
<p>This will run your executable &#8220;my_mpi_executable.x&#8221; in parallel on 36
MPI processes using 2 nodes with hyperthreading switched off. PBS will
allocate 2 nodes to your job and place 18 MPI processes on each node
(one per physical core).</p>
<p>See above for a detailed discussion of the different PBS options</p>
</div>
<div class="section" id="example-job-submission-script-for-mpi-openmp-mixed-mode-parallel-job">
<h3>Example: job submission script for MPI+OpenMP (mixed mode) parallel job<a class="headerlink" href="#example-job-submission-script-for-mpi-openmp-mixed-mode-parallel-job" title="Permalink to this headline">¶</a></h3>
<p>Mixed mode codes that use both MPI (or another distributed memory
parallel model) and OpenMP should take care to ensure that the shared
memory portion of the process/thread placement does not span more than
one node. This means that the number of shared memory threads should be
a factor of 18.</p>
<p>In the example below, we are using 2 nodes for 6 hours. There are 4 MPI
processes in total and 16 OpenMP threads per MPI process. NB this
example employs hyperthreads</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>#!/bin/bash --login

# PBS job options (name, compute nodes, job time)
#PBS -N Example_MixedMode_Job
#PBS -l select=144
#PBS -l walltime=6:0:0

# Replace [budget code] below with your project code (e.g. t01)
#PBS -A [budget code]

# Change to the direcotry that the job was submitted from
cd $PBS_O_WORKDIR

# Set the number of threads to 4
#   There are 4 OpenMP threads per MPI process
export OMP_NUM_THREADS=16

# Launch the parallel job
#   Using 128*6 = 768 MPI processes
#   6 MPI processes per node
#   3 MPI processes per NUMA region
#   4 OpenMP threads per MPI process
mpiexec_mpt -n 4 -ppn 2 omplace -nt 16 ./my_mixed_executable.x arg1 arg2 &gt; my_stdout.txt 2&gt; my_stderr.txt
</pre></div>
</div>
</div>
<div class="section" id="interactive-jobs">
<h3>Interactive jobs<a class="headerlink" href="#interactive-jobs" title="Permalink to this headline">¶</a></h3>
<p>The nature of the job submission system on Cirrus does not lend itself
to developing or debugging code as the queues are primarily set up for
production jobs.</p>
<p>When you are developing or debugging code you often want to run many
short jobs with a small amount of editing the code between runs. One of
the best ways to achieve this on Cirrus is to use the login nodes. An
interactive job allows you to issue the &#8216; <code class="docutils literal"><span class="pre">mpirun</span></code> &#8216; commands directly
from the command line witout using a job submission script.</p>
<p>For instance, to run a short 4-way MPI job on the login node, issue the
following command <code class="docutils literal"><span class="pre">mpirun</span> <span class="pre">-n</span> <span class="pre">4</span> <span class="pre">./hello_mpi.x</span></code></p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Running Jobs on Cirrus</a><ul>
<li><a class="reference internal" href="#using-pbs-pro">Using PBS Pro</a><ul>
<li><a class="reference internal" href="#the-qsub-command">The qsub command</a></li>
<li><a class="reference internal" href="#the-qstat-command">The qstat command</a></li>
<li><a class="reference internal" href="#the-qdel-command">The qdel command</a></li>
</ul>
</li>
<li><a class="reference internal" href="#output-from-pbs-jobs">Output from PBS jobs</a></li>
<li><a class="reference internal" href="#running-parallel-jobs">Running Parallel Jobs</a><ul>
<li><a class="reference internal" href="#pbs-submission-options">PBS Submission Options</a></li>
<li><a class="reference internal" href="#parallel-job-launcher-mpiexec-mpt">Parallel job launcher <code class="docutils literal"><span class="pre">mpiexec_mpt</span></code></a></li>
<li><a class="reference internal" href="#example-job-submission-script-for-mpi-parallel-job">Example: job submission script for MPI parallel job</a></li>
<li><a class="reference internal" href="#example-job-submission-script-for-mpi-openmp-mixed-mode-parallel-job">Example: job submission script for MPI+OpenMP (mixed mode) parallel job</a></li>
<li><a class="reference internal" href="#interactive-jobs">Interactive jobs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/documentation/user-guide/batch.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, G. Pringle, A. R. Turner.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="../../_sources/documentation/user-guide/batch.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>